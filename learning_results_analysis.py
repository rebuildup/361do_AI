#!/usr/bin/env python3
"""
Multi-Agent Learning Results Analysis
8æ™‚é–“ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’çµæœã®è©³ç´°åˆ†æ
"""

import json
import sys
from datetime import datetime, timedelta
from pathlib import Path
from collections import Counter

def analyze_learning_session(json_file):
    """å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã®è©³ç´°åˆ†æ"""
    
    print("=" * 80)
    print("ğŸ‰ 8æ™‚é–“ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ  çµæœåˆ†æ")
    print("=" * 80)
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # åŸºæœ¬æƒ…å ±åˆ†æ
    session_info = data.get('session_info', {})
    learning_stats = data.get('learning_stats', {})
    agent_roles = data.get('agent_roles', {})
    conversation_history = data.get('conversation_history', [])
    
    print(f"ğŸ“… å®Ÿè¡ŒæœŸé–“:")
    print(f"  é–‹å§‹æ™‚åˆ»: {session_info.get('start_time', 'Unknown')}")
    print(f"  çµ‚äº†æ™‚åˆ»: {session_info.get('end_time', 'Unknown')}")
    print(f"  ç·å®Ÿè¡Œæ™‚é–“: {session_info.get('total_runtime_hours', 0):.2f}æ™‚é–“")
    print(f"  åˆ¶é™æ™‚é–“: {session_info.get('time_limit_hours', 0)}æ™‚é–“")
    
    # å­¦ç¿’çµ±è¨ˆåˆ†æ
    print(f"\nğŸ“Š å­¦ç¿’çµ±è¨ˆ:")
    print(f"  ç·ä¼šè©±æ•°: {learning_stats.get('total_conversations', 0)}")
    print(f"  å­¦ç¿’ã‚µã‚¤ã‚¯ãƒ«æ•°: {learning_stats.get('total_learning_cycles', 0)}")
    print(f"  çŸ¥è­˜å…±æœ‰æ•°: {learning_stats.get('knowledge_shared', 0)}")
    print(f"  æ”¹å–„å®Ÿæ–½æ•°: {learning_stats.get('improvements_made', 0)}")
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
    print(f"\nğŸ¤– ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
    agent_interactions = learning_stats.get('agent_interactions', {})
    
    for agent_id, stats in agent_interactions.items():
        agent_name = agent_roles.get(agent_id, {}).get('name', agent_id)
        focus = agent_roles.get(agent_id, {}).get('focus', 'Unknown')
        
        print(f"  {agent_name} ({agent_id}):")
        print(f"    å°‚é–€åˆ†é‡: {focus}")
        print(f"    ç™ºè¨€æ•°: {stats.get('messages_sent', 0)}")
        print(f"    å­¦ç¿’ã‚µã‚¤ã‚¯ãƒ«: {stats.get('learning_cycles', 0)}")
        print(f"    çŸ¥è­˜è²¢çŒ®: {stats.get('knowledge_contributions', 0)}")
    
    # ä¼šè©±å†…å®¹åˆ†æ
    print(f"\nğŸ’¬ ä¼šè©±å†…å®¹åˆ†æ:")
    if conversation_history:
        print(f"  è¨˜éŒ²ã•ã‚ŒãŸä¼šè©±æ•°: {len(conversation_history)}")
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ç™ºè¨€æ•°
        agent_message_counts = Counter()
        total_execution_time = 0
        successful_conversations = 0
        
        for conv in conversation_history:
            agent_name = conv.get('agent_name', 'Unknown')
            agent_message_counts[agent_name] += 1
            total_execution_time += conv.get('execution_time', 0)
            if conv.get('success', False):
                successful_conversations += 1
        
        print(f"  æˆåŠŸã—ãŸä¼šè©±: {successful_conversations}/{len(conversation_history)} ({successful_conversations/len(conversation_history)*100:.1f}%)")
        print(f"  å¹³å‡å¿œç­”æ™‚é–“: {total_execution_time/len(conversation_history):.2f}ç§’")
        
        print(f"\n  ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ç™ºè¨€æ•°:")
        for agent_name, count in agent_message_counts.most_common():
            print(f"    {agent_name}: {count}å›")
        
        # ä½¿ç”¨ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«åˆ†æ
        all_tools = []
        intent_analysis = Counter()
        
        for conv in conversation_history:
            tools_used = conv.get('tools_used', [])
            all_tools.extend(tools_used)
            
            intent = conv.get('intent', {})
            primary_intent = intent.get('primary_intent', 'unknown')
            intent_analysis[primary_intent] += 1
        
        if all_tools:
            print(f"\n  ä½¿ç”¨ãƒ„ãƒ¼ãƒ«çµ±è¨ˆ:")
            tool_counts = Counter(all_tools)
            for tool, count in tool_counts.most_common():
                print(f"    {tool}: {count}å›")
        
        print(f"\n  ä¼šè©±æ„å›³åˆ†æ:")
        for intent, count in intent_analysis.most_common():
            print(f"    {intent}: {count}å› ({count/len(conversation_history)*100:.1f}%)")
    
    # å­¦ç¿’åŠ¹æœåˆ†æ
    print(f"\nğŸ“ˆ å­¦ç¿’åŠ¹æœåˆ†æ:")
    
    # æ™‚é–“ã‚ãŸã‚Šã®åŠ¹ç‡æ€§
    runtime_hours = session_info.get('total_runtime_hours', 1)
    conversations_per_hour = learning_stats.get('total_conversations', 0) / runtime_hours
    learning_cycles_per_hour = learning_stats.get('total_learning_cycles', 0) / runtime_hours
    knowledge_per_hour = learning_stats.get('knowledge_shared', 0) / runtime_hours
    
    print(f"  æ™‚é–“ã‚ãŸã‚Šä¼šè©±æ•°: {conversations_per_hour:.1f}å›/æ™‚é–“")
    print(f"  æ™‚é–“ã‚ãŸã‚Šå­¦ç¿’ã‚µã‚¤ã‚¯ãƒ«: {learning_cycles_per_hour:.1f}å›/æ™‚é–“")
    print(f"  æ™‚é–“ã‚ãŸã‚ŠçŸ¥è­˜å…±æœ‰: {knowledge_per_hour:.1f}ä»¶/æ™‚é–“")
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®å”èª¿æ€§åˆ†æ
    total_messages = sum(stats.get('messages_sent', 0) for stats in agent_interactions.values())
    total_contributions = sum(stats.get('knowledge_contributions', 0) for stats in agent_interactions.values())
    
    if total_messages > 0:
        print(f"\nğŸ¤ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“å”èª¿æ€§:")
        print(f"  ç·ç™ºè¨€æ•°: {total_messages}")
        print(f"  ç·çŸ¥è­˜è²¢çŒ®: {total_contributions}")
        print(f"  ç™ºè¨€ã‚ãŸã‚ŠçŸ¥è­˜è²¢çŒ®ç‡: {total_contributions/total_messages:.2f}")
        
        # ãƒãƒ©ãƒ³ã‚¹åˆ†æ
        message_distribution = [stats.get('messages_sent', 0) for stats in agent_interactions.values()]
        max_messages = max(message_distribution)
        min_messages = min(message_distribution)
        balance_ratio = min_messages / max_messages if max_messages > 0 else 0
        
        print(f"  ç™ºè¨€ãƒãƒ©ãƒ³ã‚¹æ¯”: {balance_ratio:.2f} (1.0ãŒå®Œå…¨ãƒãƒ©ãƒ³ã‚¹)")
    
    # ä¼šè©±ãƒˆãƒ”ãƒƒã‚¯åˆ†æ
    if conversation_history:
        print(f"\nğŸ“ ä¼šè©±ãƒˆãƒ”ãƒƒã‚¯åˆ†æ:")
        
        # ä¼šè©±ã®é•·ã•åˆ†æ
        response_lengths = []
        for conv in conversation_history:
            content = conv.get('content', '')
            response_lengths.append(len(content))
        
        if response_lengths:
            avg_length = sum(response_lengths) / len(response_lengths)
            max_length = max(response_lengths)
            min_length = min(response_lengths)
            
            print(f"  å¹³å‡å¿œç­”é•·: {avg_length:.0f}æ–‡å­—")
            print(f"  æœ€é•·å¿œç­”: {max_length}æ–‡å­—")
            print(f"  æœ€çŸ­å¿œç­”: {min_length}æ–‡å­—")
    
    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æï¼ˆå¯èƒ½ãªå ´åˆï¼‰
    try:
        analyze_database_impact()
    except Exception as e:
        print(f"\nâš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
    
    # æ¨å¥¨äº‹é …
    print(f"\nğŸ’¡ å­¦ç¿’åŠ¹æœã¨æ¨å¥¨äº‹é …:")
    
    success_rate = successful_conversations / len(conversation_history) * 100 if conversation_history else 0
    
    if success_rate >= 95:
        print(f"  âœ… å„ªç§€: ä¼šè©±æˆåŠŸç‡ {success_rate:.1f}% - ã‚·ã‚¹ãƒ†ãƒ ã¯éå¸¸ã«å®‰å®šã—ã¦å‹•ä½œ")
    elif success_rate >= 80:
        print(f"  âœ… è‰¯å¥½: ä¼šè©±æˆåŠŸç‡ {success_rate:.1f}% - ã‚·ã‚¹ãƒ†ãƒ ã¯å®‰å®šã—ã¦å‹•ä½œ")
    else:
        print(f"  âš ï¸ æ”¹å–„å¿…è¦: ä¼šè©±æˆåŠŸç‡ {success_rate:.1f}% - ã‚·ã‚¹ãƒ†ãƒ ã®å®‰å®šæ€§å‘ä¸ŠãŒå¿…è¦")
    
    if balance_ratio >= 0.8:
        print(f"  âœ… ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯å¥½ (æ¯”ç‡: {balance_ratio:.2f})")
    else:
        print(f"  âš ï¸ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®ç™ºè¨€ãƒãƒ©ãƒ³ã‚¹ã«åã‚Šã‚ã‚Š (æ¯”ç‡: {balance_ratio:.2f})")
    
    if conversations_per_hour >= 1.0:
        print(f"  âœ… åŠ¹ç‡çš„ãªå­¦ç¿’ãƒšãƒ¼ã‚¹ ({conversations_per_hour:.1f}ä¼šè©±/æ™‚é–“)")
    else:
        print(f"  âš ï¸ å­¦ç¿’ãƒšãƒ¼ã‚¹ãŒä½ã„å¯èƒ½æ€§ ({conversations_per_hour:.1f}ä¼šè©±/æ™‚é–“)")
    
    # ç·åˆè©•ä¾¡
    print(f"\nğŸ† ç·åˆè©•ä¾¡:")
    
    score = 0
    max_score = 100
    
    # å®Ÿè¡Œæ™‚é–“é”æˆåº¦ (25ç‚¹)
    time_achievement = min(runtime_hours / session_info.get('time_limit_hours', 8), 1.0)
    time_score = time_achievement * 25
    score += time_score
    print(f"  å®Ÿè¡Œæ™‚é–“é”æˆåº¦: {time_achievement*100:.1f}% ({time_score:.1f}/25ç‚¹)")
    
    # ä¼šè©±æˆåŠŸç‡ (25ç‚¹)
    success_score = (success_rate / 100) * 25
    score += success_score
    print(f"  ä¼šè©±æˆåŠŸç‡: {success_rate:.1f}% ({success_score:.1f}/25ç‚¹)")
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå”èª¿æ€§ (25ç‚¹)
    balance_score = balance_ratio * 25
    score += balance_score
    print(f"  ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå”èª¿æ€§: {balance_ratio*100:.1f}% ({balance_score:.1f}/25ç‚¹)")
    
    # å­¦ç¿’åŠ¹ç‡æ€§ (25ç‚¹)
    efficiency_score = min(conversations_per_hour / 2.0, 1.0) * 25  # 2ä¼šè©±/æ™‚é–“ã‚’æº€ç‚¹ã¨ã™ã‚‹
    score += efficiency_score
    print(f"  å­¦ç¿’åŠ¹ç‡æ€§: {min(conversations_per_hour/2.0*100, 100):.1f}% ({efficiency_score:.1f}/25ç‚¹)")
    
    print(f"\n  ğŸ¯ ç·åˆã‚¹ã‚³ã‚¢: {score:.1f}/{max_score}ç‚¹")
    
    if score >= 90:
        grade = "S (å„ªç§€)"
    elif score >= 80:
        grade = "A (è‰¯å¥½)"
    elif score >= 70:
        grade = "B (æ™®é€š)"
    elif score >= 60:
        grade = "C (æ”¹å–„å¿…è¦)"
    else:
        grade = "D (å¤§å¹…æ”¹å–„å¿…è¦)"
    
    print(f"  ğŸ“Š è©•ä¾¡: {grade}")
    
    print("=" * 80)

def analyze_database_impact():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®å­¦ç¿’åŠ¹æœåˆ†æ"""
    print(f"\nğŸ—„ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å­¦ç¿’åŠ¹æœåˆ†æ:")
    
    try:
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆã®ç°¡æ˜“ç¢ºèª
        import sqlite3
        
        db_path = "data/agent.db"
        if Path(db_path).exists():
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ•°ç¢ºèª
            cursor.execute("SELECT COUNT(*) FROM learning_data")
            learning_data_count = cursor.fetchone()[0]
            
            # çŸ¥è­˜ã‚¢ã‚¤ãƒ†ãƒ æ•°ç¢ºèª
            cursor.execute("SELECT COUNT(*) FROM knowledge_items")
            knowledge_count = cursor.fetchone()[0]
            
            print(f"  å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç·æ•°: {learning_data_count}ä»¶")
            print(f"  çŸ¥è­˜ã‚¢ã‚¤ãƒ†ãƒ æ•°: {knowledge_count}ä»¶")
            
            # æœ€è¿‘è¿½åŠ ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
            cursor.execute("""
                SELECT COUNT(*) FROM learning_data 
                WHERE created_at > datetime('now', '-1 day')
            """)
            recent_data = cursor.fetchone()[0]
            
            print(f"  éå»24æ™‚é–“ã®æ–°è¦å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {recent_data}ä»¶")
            
            conn.close()
        else:
            print(f"  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {db_path}")
            
    except Exception as e:
        print(f"  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’çµæœåˆ†æ")
    parser.add_argument("--file", help="åˆ†æã™ã‚‹JSONãƒ•ã‚¡ã‚¤ãƒ«")
    
    args = parser.parse_args()
    
    # æœ€æ–°ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•æ¤œå‡º
    if not args.file:
        json_files = list(Path('.').glob('multi_agent_learning_session_*.json'))
        if json_files:
            latest_file = max(json_files, key=lambda x: x.stat().st_mtime)
            args.file = str(latest_file)
            print(f"æœ€æ–°ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {args.file}")
        else:
            print("çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
    
    # åˆ†æå®Ÿè¡Œ
    analyze_learning_session(args.file)

if __name__ == "__main__":
    main()