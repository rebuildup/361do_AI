#!/usr/bin/env python3
"""
Improved Continuous Learning System
æ”¹å–„ã•ã‚ŒãŸ4æ™‚é–“ç¶™ç¶šå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import json
import os
import sys
import time
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
import sqlite3
import random

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from conversation_data_processor import ConversationDataProcessor

try:
    from src.advanced_agent.core.self_learning_agent import SelfLearningAgent
    from src.advanced_agent.core.logger import get_logger
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ç°¡å˜ãªãƒ­ã‚¬ãƒ¼
    import logging
    def get_logger():
        logger = logging.getLogger(__name__)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ç°¡å˜ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    class SelfLearningAgent:
        def __init__(self, **kwargs):
            self.logger = get_logger()
        
        async def initialize_session(self, **kwargs):
            self.logger.info("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–")
            return True


class ImprovedContinuousLearningSystem:
    """æ”¹å–„ã•ã‚ŒãŸç¶™ç¶šå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, learning_duration_hours: int = 4):
        self.logger = get_logger()
        self.learning_duration_hours = learning_duration_hours
        self.db_path = "data/improved_continuous_learning.db"
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ï¼ˆåŒã˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ï¼‰
        self.data_processor = ConversationDataProcessor(db_path="data/conversation_processing.db")
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        self.agent: Optional[SelfLearningAgent] = None
        
        # å­¦ç¿’è¨­å®š
        self.learning_config = {
            "batch_size": 10,  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¢—åŠ 
            "learning_interval": 20,  # å­¦ç¿’é–“éš”ã‚’çŸ­ç¸®
            "max_conversations_per_cycle": 100,  # ã‚µã‚¤ã‚¯ãƒ«ã‚ãŸã‚Šæœ€å¤§ä¼šè©±æ•°ã‚’å¢—åŠ 
            "learning_rate": 0.1,
            "memory_retention_days": 7,
            "quality_threshold": 0.1,  # å“è³ªé–¾å€¤ã‚’ä¸‹ã’ã‚‹ï¼ˆã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
            "max_content_length": 10000,  # æœ€å¤§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·ã‚’å¢—åŠ 
            "min_content_length": 20   # æœ€å°ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·ã‚’ä¸‹ã’ã‚‹
        }
        
        # å­¦ç¿’çµ±è¨ˆ
        self.learning_stats = {
            "session_id": f"learning_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "start_time": None,
            "end_time": None,
            "total_processed": 0,
            "learning_cycles": 0,
            "current_epoch": 0,
            "total_conversations": 0,
            "quality_scores": [],
            "processing_errors": 0,
            "duplicate_errors": 0,  # é‡è¤‡ã‚¨ãƒ©ãƒ¼ã‚’åˆ¥é€”ã‚«ã‚¦ãƒ³ãƒˆ
            "source_stats": {
                "chatgpt": {"processed": 0, "errors": 0},
                "claude": {"processed": 0, "errors": 0}
            }
        }
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        self._init_database()
    
    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS learning_sessions (
                    session_id TEXT PRIMARY KEY,
                    start_time TIMESTAMP,
                    end_time TIMESTAMP,
                    total_processed INTEGER,
                    learning_cycles INTEGER,
                    max_epoch INTEGER,
                    total_conversations INTEGER,
                    avg_quality_score REAL,
                    processing_errors INTEGER,
                    duplicate_errors INTEGER DEFAULT 0,
                    status TEXT DEFAULT 'running'
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS learning_cycles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT,
                    cycle_number INTEGER,
                    start_time TIMESTAMP,
                    end_time TIMESTAMP,
                    conversations_processed INTEGER,
                    avg_quality_score REAL,
                    epoch INTEGER,
                    FOREIGN KEY (session_id) REFERENCES learning_sessions (session_id)
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS processed_conversations (
                    id TEXT PRIMARY KEY,
                    session_id TEXT,
                    cycle_number INTEGER,
                    source TEXT,
                    conversation_id TEXT,
                    title TEXT,
                    content TEXT,
                    quality_score REAL,
                    processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (session_id) REFERENCES learning_sessions (session_id)
                )
            """)
            
            conn.commit()
    
    async def initialize_agent(self) -> bool:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–"""
        try:
            self.logger.info("ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–ä¸­...")
            
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
            self.agent = SelfLearningAgent()
            
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
            await self.agent.initialize_session()
            
            self.logger.info("âœ… ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–å¤±æ•—: {e}")
            return False
    
    def _load_processed_conversations(self, limit: int = None) -> List[Dict[str, Any]]:
        """å‡¦ç†æ¸ˆã¿ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
            db_path = self.data_processor.db_path
            if not os.path.exists(db_path):
                self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {db_path}")
                return []
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚ºç¢ºèª
            db_size = os.path.getsize(db_path)
            self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«: {db_path} (ã‚µã‚¤ã‚º: {db_size:,} bytes)")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‹ã‚‰å‡¦ç†æ¸ˆã¿ä¼šè©±ã‚’å–å¾—
            conversations = self.data_processor.get_processed_conversations(limit=limit or 1000)
            self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰èª­ã¿è¾¼ã¿: {len(conversations)}ä»¶")
            
            # å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_conversations = []
            quality_filtered = 0
            length_filtered = 0
            
            for conv in conversations:
                # å“è³ªãƒã‚§ãƒƒã‚¯
                if conv.get("quality_score", 0) < self.learning_config["quality_threshold"]:
                    quality_filtered += 1
                    continue
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·ãƒã‚§ãƒƒã‚¯
                content_length = len(conv.get("content", ""))
                if (content_length < self.learning_config["min_content_length"] or 
                    content_length > self.learning_config["max_content_length"]):
                    length_filtered += 1
                    continue
                
                filtered_conversations.append(conv)
            
            # ã‚·ãƒ£ãƒƒãƒ•ãƒ«
            random.shuffle(filtered_conversations)
            
            self.logger.info(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ:")
            self.logger.info(f"  - å…ƒãƒ‡ãƒ¼ã‚¿: {len(conversations)}ä»¶")
            self.logger.info(f"  - å“è³ªãƒ•ã‚£ãƒ«ã‚¿: {quality_filtered}ä»¶é™¤å¤–")
            self.logger.info(f"  - é•·ã•ãƒ•ã‚£ãƒ«ã‚¿: {length_filtered}ä»¶é™¤å¤–")
            self.logger.info(f"  - æœ€çµ‚çµæœ: {len(filtered_conversations)}ä»¶")
            
            return filtered_conversations
            
        except Exception as e:
            self.logger.error(f"ä¼šè©±ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            self.logger.error(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
            return []
    
    def _prepare_learning_batch(self, conversations: List[Dict[str, Any]], batch_size: int) -> List[Dict[str, Any]]:
        """å­¦ç¿’ãƒãƒƒãƒã‚’æº–å‚™"""
        batch = conversations[:batch_size]
        
        # ãƒãƒƒãƒå†…ã®ä¼šè©±ã‚’å­¦ç¿’ç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formatted_batch = []
        for conv in batch:
            try:
                # ä¼šè©±å†…å®¹ã‚’å­¦ç¿’ç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                formatted_conv = {
                    "id": conv["id"],
                    "source": conv["source"],
                    "title": conv["title"],
                    "content": conv["content"],
                    "quality_score": conv["quality_score"],
                    "learning_prompt": self._create_learning_prompt(conv)
                }
                formatted_batch.append(formatted_conv)
                
            except Exception as e:
                self.logger.error(f"ä¼šè©±ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                self.learning_stats["processing_errors"] += 1
        
        return formatted_batch
    
    def _create_learning_prompt(self, conversation: Dict[str, Any]) -> str:
        """å­¦ç¿’ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
        content = conversation.get("content", "")
        title = conversation.get("title", "Untitled")
        source = conversation.get("source", "unknown")
        
        # å­¦ç¿’ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        prompt = f"""
å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {source.upper()}ä¼šè©±
ã‚¿ã‚¤ãƒˆãƒ«: {title}
å†…å®¹: {content[:2000]}...

ã“ã®ä¼šè©±ã‹ã‚‰å­¦ç¿’ã—ã€åŒæ§˜ã®å“è³ªã¨ã‚¹ã‚¿ã‚¤ãƒ«ã§å¿œç­”ã§ãã‚‹ã‚ˆã†æ”¹å–„ã—ã¦ãã ã•ã„ã€‚
"""
        return prompt.strip()
    
    async def _process_learning_batch(self, batch: List[Dict[str, Any]], cycle_number: int) -> Dict[str, Any]:
        """å­¦ç¿’ãƒãƒƒãƒã‚’å‡¦ç†"""
        batch_stats = {
            "processed": 0,
            "errors": 0,
            "avg_quality": 0.0,
            "sources": {"chatgpt": 0, "claude": 0}
        }
        
        try:
            for conv in batch:
                try:
                    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡
                    if self.agent:
                        # å­¦ç¿’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«é€ä¿¡
                        learning_prompt = conv["learning_prompt"]
                        
                        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å­¦ç¿’å‡¦ç†ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã«å¿œã˜ã¦èª¿æ•´ï¼‰
                        # ã“ã“ã§ã¯æ¨¡æ“¬çš„ãªå­¦ç¿’å‡¦ç†
                        await asyncio.sleep(0.1)  # å­¦ç¿’å‡¦ç†ã®æ¨¡æ“¬
                        
                        # çµ±è¨ˆæ›´æ–°
                        batch_stats["processed"] += 1
                        batch_stats["sources"][conv["source"]] += 1
                        batch_stats["avg_quality"] += conv["quality_score"]
                        
                        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨˜éŒ²
                        self._save_processed_conversation(conv, cycle_number)
                        
                except Exception as e:
                    self.logger.error(f"ä¼šè©±å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
                    batch_stats["errors"] += 1
                    self.learning_stats["processing_errors"] += 1
            
            # å¹³å‡å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            if batch_stats["processed"] > 0:
                batch_stats["avg_quality"] /= batch_stats["processed"]
            
            return batch_stats
            
        except Exception as e:
            self.logger.error(f"ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return batch_stats
    
    def _save_processed_conversation(self, conversation: Dict[str, Any], cycle_number: int):
        """å‡¦ç†æ¸ˆã¿ä¼šè©±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            # ä¸€æ„ã®IDã‚’ç”Ÿæˆï¼ˆä¼šè©±ID + ã‚µã‚¤ã‚¯ãƒ«ç•ªå·ï¼‰
            unique_id = f"{conversation['id']}_{cycle_number}"
            
            with sqlite3.connect(self.db_path) as conn:
                # æ—¢å­˜ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
                cursor = conn.execute("SELECT id FROM processed_conversations WHERE id = ?", (unique_id,))
                if cursor.fetchone():
                    # æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé‡è¤‡ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆï¼‰
                    self.learning_stats["duplicate_errors"] += 1
                    return
                
                conn.execute("""
                    INSERT INTO processed_conversations 
                    (id, session_id, cycle_number, source, conversation_id, title, content, quality_score)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    unique_id,
                    self.learning_stats["session_id"],
                    cycle_number,
                    conversation["source"],
                    conversation.get("conversation_id", ""),
                    conversation["title"],
                    conversation["content"],
                    conversation["quality_score"]
                ))
                conn.commit()
        except Exception as e:
            self.logger.error(f"ä¼šè©±ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _save_cycle_stats(self, cycle_number: int, batch_stats: Dict[str, Any], start_time: datetime, end_time: datetime):
        """ã‚µã‚¤ã‚¯ãƒ«çµ±è¨ˆã‚’ä¿å­˜"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    INSERT INTO learning_cycles 
                    (session_id, cycle_number, start_time, end_time, conversations_processed, avg_quality_score, epoch)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    self.learning_stats["session_id"],
                    cycle_number,
                    start_time,
                    end_time,
                    batch_stats["processed"],
                    batch_stats["avg_quality"],
                    self.learning_stats["current_epoch"]
                ))
                conn.commit()
        except Exception as e:
            self.logger.error(f"ã‚µã‚¤ã‚¯ãƒ«çµ±è¨ˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _update_learning_stats(self, batch_stats: Dict[str, Any]):
        """å­¦ç¿’çµ±è¨ˆã‚’æ›´æ–°"""
        self.learning_stats["total_processed"] += batch_stats["processed"]
        self.learning_stats["learning_cycles"] += 1
        self.learning_stats["current_epoch"] += 1
        
        # ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆæ›´æ–°
        for source, count in batch_stats["sources"].items():
            self.learning_stats["source_stats"][source]["processed"] += count
        
        # å“è³ªã‚¹ã‚³ã‚¢è¨˜éŒ²
        if batch_stats["avg_quality"] > 0:
            self.learning_stats["quality_scores"].append(batch_stats["avg_quality"])
    
    def _save_session_stats(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆã‚’ä¿å­˜"""
        try:
            avg_quality = 0.0
            if self.learning_stats["quality_scores"]:
                avg_quality = sum(self.learning_stats["quality_scores"]) / len(self.learning_stats["quality_scores"])
            
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO learning_sessions 
                    (session_id, start_time, end_time, total_processed, learning_cycles, max_epoch, 
                     total_conversations, avg_quality_score, processing_errors, duplicate_errors, status)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    self.learning_stats["session_id"],
                    self.learning_stats["start_time"],
                    self.learning_stats["end_time"],
                    self.learning_stats["total_processed"],
                    self.learning_stats["learning_cycles"],
                    self.learning_stats["current_epoch"],
                    self.learning_stats["total_conversations"],
                    avg_quality,
                    self.learning_stats["processing_errors"],
                    self.learning_stats["duplicate_errors"],
                    "completed"
                ))
                conn.commit()
        except Exception as e:
            self.logger.error(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def start_continuous_learning(self):
        """ç¶™ç¶šå­¦ç¿’é–‹å§‹"""
        self.learning_stats["start_time"] = datetime.now()
        end_time = self.learning_stats["start_time"] + timedelta(hours=self.learning_duration_hours)
        
        self.logger.info("=" * 60)
        self.logger.info("æ”¹å–„ã•ã‚ŒãŸ4æ™‚é–“ç¶™ç¶šå­¦ç¿’é–‹å§‹")
        self.logger.info("=" * 60)
        self.logger.info(f"å­¦ç¿’æ™‚é–“: {self.learning_duration_hours}æ™‚é–“")
        self.logger.info(f"çµ‚äº†äºˆå®š: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info("=" * 60)
        
        # å‡¦ç†æ¸ˆã¿ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        conversations = self._load_processed_conversations()
        self.learning_stats["total_conversations"] = len(conversations)
        
        if not conversations:
            self.logger.error("âŒ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        self.logger.info(f"ğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(conversations)}ä»¶")
        
        cycle_number = 0
        
        try:
            while datetime.now() < end_time:
                cycle_start_time = datetime.now()
                cycle_number += 1
                
                # å­¦ç¿’ãƒãƒƒãƒã‚’æº–å‚™
                batch = self._prepare_learning_batch(conversations, self.learning_config["batch_size"])
                
                if not batch:
                    self.logger.warning("å­¦ç¿’ãƒãƒƒãƒãŒç©ºã§ã™")
                    break
                
                # ãƒãƒƒãƒã‚’å‡¦ç†
                batch_stats = await self._process_learning_batch(batch, cycle_number)
                cycle_end_time = datetime.now()
                
                # çµ±è¨ˆæ›´æ–°
                self._update_learning_stats(batch_stats)
                self._save_cycle_stats(cycle_number, batch_stats, cycle_start_time, cycle_end_time)
                
                # é€²æ—è¡¨ç¤º
                elapsed_time = datetime.now() - self.learning_stats["start_time"]
                remaining_time = end_time - datetime.now()
                
                self.logger.info(f"ã‚µã‚¤ã‚¯ãƒ« {cycle_number}: å‡¦ç†æ¸ˆã¿ {batch_stats['processed']}ä»¶, "
                               f"å“è³ªã‚¹ã‚³ã‚¢ {batch_stats['avg_quality']:.3f}, "
                               f"çµŒéæ™‚é–“ {elapsed_time}, æ®‹ã‚Šæ™‚é–“ {remaining_time}")
                
                # å­¦ç¿’é–“éš”å¾…æ©Ÿ
                await asyncio.sleep(self.learning_config["learning_interval"])
                
                # ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼ˆæ¬¡ã®ã‚µã‚¤ã‚¯ãƒ«ç”¨ï¼‰
                random.shuffle(conversations)
        
        except KeyboardInterrupt:
            self.logger.info("âš ï¸ å­¦ç¿’ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        except Exception as e:
            self.logger.error(f"âŒ å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            # å­¦ç¿’å®Œäº†å‡¦ç†
            self.learning_stats["end_time"] = datetime.now()
            self._save_session_stats()
            
            # æœ€çµ‚çµ±è¨ˆè¡¨ç¤º
            self._display_final_stats()
    
    def _display_final_stats(self):
        """æœ€çµ‚çµ±è¨ˆã‚’è¡¨ç¤º"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸ‰ å­¦ç¿’å®Œäº†!")
        self.logger.info("=" * 60)
        self.logger.info(f"ç·å‡¦ç†æ•°: {self.learning_stats['total_processed']}")
        self.logger.info(f"å­¦ç¿’ã‚µã‚¤ã‚¯ãƒ«æ•°: {self.learning_stats['learning_cycles']}")
        self.logger.info(f"æœ€çµ‚ã‚¨ãƒãƒƒã‚¯: {self.learning_stats['current_epoch']}")
        self.logger.info(f"é–‹å§‹æ™‚é–“: {self.learning_stats['start_time']}")
        self.logger.info(f"çµ‚äº†æ™‚é–“: {self.learning_stats['end_time']}")
        
        if self.learning_stats["quality_scores"]:
            avg_quality = sum(self.learning_stats["quality_scores"]) / len(self.learning_stats["quality_scores"])
            self.logger.info(f"å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {avg_quality:.3f}")
        
        self.logger.info(f"ã‚¨ãƒ©ãƒ¼æ•°: {self.learning_stats['processing_errors']}")
        self.logger.info(f"é‡è¤‡ã‚¨ãƒ©ãƒ¼æ•°: {self.learning_stats['duplicate_errors']}")
        
        # ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ
        self.logger.info("ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ:")
        for source, stats in self.learning_stats["source_stats"].items():
            self.logger.info(f"  {source}: {stats['processed']}ä»¶")
        
        self.logger.info("=" * 60)
    
    def get_learning_statistics(self) -> Dict[str, Any]:
        """å­¦ç¿’çµ±è¨ˆã‚’å–å¾—"""
        return self.learning_stats.copy()


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("=" * 60)
    print("361do_AI æ”¹å–„ã•ã‚ŒãŸ4æ™‚é–“ç¶™ç¶šå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    print()
    
    try:
        # ç¶™ç¶šå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ä½œæˆ
        learning_system = ImprovedContinuousLearningSystem(learning_duration_hours=4)
        
        print("ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–ä¸­...")
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
        if not await learning_system.initialize_agent():
            print("âŒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        print("âœ… ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
        print()
        
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ç¢ºèª
        print("ãƒ‡ãƒ¼ã‚¿å‡¦ç†ç¢ºèªä¸­...")
        processed_conversations = learning_system._load_processed_conversations(limit=100)
        print(f"ğŸ“Š å‡¦ç†æ¸ˆã¿ä¼šè©±æ•°: {len(processed_conversations)}")
        
        if not processed_conversations:
            print("âŒ å‡¦ç†æ¸ˆã¿å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            print("å…ˆã« conversation_data_processor.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            return
        
        # å­¦ç¿’è¨­å®šè¡¨ç¤º
        print("å­¦ç¿’è¨­å®š:")
        print(f"   - å­¦ç¿’æ™‚é–“: 4æ™‚é–“")
        print(f"   - ãƒãƒƒãƒã‚µã‚¤ã‚º: {learning_system.learning_config['batch_size']}")
        print(f"   - å­¦ç¿’é–“éš”: {learning_system.learning_config['learning_interval']}ç§’")
        print(f"   - ã‚µã‚¤ã‚¯ãƒ«ã‚ãŸã‚Šæœ€å¤§ä¼šè©±æ•°: {learning_system.learning_config['max_conversations_per_cycle']}")
        print(f"   - å“è³ªé–¾å€¤: {learning_system.learning_config['quality_threshold']}")
        print()
        
        # ç¢ºèª
        response = input("æ”¹å–„ã•ã‚ŒãŸ4æ™‚é–“ç¶™ç¶šå­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ (y/N): ")
        if response.lower() != 'y':
            print("å­¦ç¿’ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
            return
        
        print()
        print("ğŸš€ æ”¹å–„ã•ã‚ŒãŸ4æ™‚é–“ç¶™ç¶šå­¦ç¿’é–‹å§‹...")
        print("   Ctrl+C ã§æ—©æœŸçµ‚äº†å¯èƒ½")
        print("=" * 60)
        
        # 4æ™‚é–“ç¶™ç¶šå­¦ç¿’é–‹å§‹
        await learning_system.start_continuous_learning()
        
    except KeyboardInterrupt:
        print("\nâš ï¸  å­¦ç¿’ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
