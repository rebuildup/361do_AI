"""
Streamlit ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  Web UI

Streamlit ã®æ—¢å­˜å¿œç­”æ€§æ©Ÿèƒ½ã«ã‚ˆã‚‹ ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚’çµ±åˆã—ã€
é€²æ—ãƒ»VRAM è¡¨ç¤ºã‚’å®Ÿè£…ã€å±¥æ­´ç®¡ç†ãƒ»ç¶™ç¶šã‚’çµ±åˆ
"""

import asyncio
import logging
import time
import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import uuid

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import requests
import aiohttp

# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’é¿ã‘ã‚‹ãŸã‚ã€try-except ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from ..monitoring.system_monitor import SystemMonitor
    from ..memory.persistent_memory import PersistentMemoryManager
    from ..reasoning.basic_engine import BasicReasoningEngine
    from ..core.error_handler import get_error_handler, ErrorType, ErrorSeverity, handle_error
except ImportError as e:
    logging.warning(f"ä¸€éƒ¨ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
    # ãƒ‡ãƒ¢ç”¨ã®ãƒ¢ãƒƒã‚¯ã‚¯ãƒ©ã‚¹
    class SystemMonitor:
        async def get_system_stats(self):
            return {"cpu_percent": 50.0, "memory_percent": 60.0}
        
        async def get_gpu_stats(self):
            return {"memory_percent": 70.0, "utilization_percent": 80.0, "temperature": 75.0}
    
    class PersistentMemoryManager:
        async def search_memories(self, query: str, **kwargs):
            return {"results": [], "total_found": 0}
    
    class BasicReasoningEngine:
        async def reasoning_inference(self, prompt: str, **kwargs):
            try:
                import ollama
                response = ollama.chat(
                    model="deepseek-r1:7b",
                    messages=[{"role": "user", "content": prompt}]
                )
                return {"response": response["message"]["content"]}
            except Exception:
                return {"response": "Ollamaæ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"}

logger = logging.getLogger(__name__)


class StreamlitUI:
    """Streamlit Web UI ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, api_base_url: str = "http://localhost:8000"):
        self.api_base_url = api_base_url
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
        self._initialize_session_state()
        
        # è¨­å®šç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self._initialize_settings_manager()
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.system_monitor = SystemMonitor()
        self.memory_manager = PersistentMemoryManager()
        self.reasoning_engine = BasicReasoningEngine()
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆæœŸåŒ–
        self.error_handler = get_error_handler()
        
        logger.info("Streamlit UI åˆæœŸåŒ–å®Œäº†")
    
    def _initialize_settings_manager(self):
        """è¨­å®šç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        
        try:
            from .settings_manager import get_settings_manager
            self.settings_manager = get_settings_manager()
            
            # è¨­å®šã‚’èª­ã¿è¾¼ã¿ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«åæ˜ 
            settings = self.settings_manager.get_settings()
            self._apply_settings_to_session(settings)
            
            logger.info("è¨­å®šç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            
        except ImportError as e:
            logger.warning(f"è¨­å®šç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
            self.settings_manager = None
    
    def _apply_settings_to_session(self, settings):
        """è¨­å®šã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«é©ç”¨ - Pydantic Settings ã«ã‚ˆã‚‹ å‹•çš„è¨­å®šå¤‰æ›´ãƒ»åæ˜ """
        
        try:
            # ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’é©ç”¨
            current_model_config = settings.get_current_model_config()
            if current_model_config:
                st.session_state.settings.update({
                    "model": settings.current_model,
                    "temperature": current_model_config.temperature,
                    "max_tokens": current_model_config.max_tokens,
                    "top_p": current_model_config.top_p,
                    "top_k": current_model_config.top_k,
                    "repeat_penalty": current_model_config.repeat_penalty
                })
            
            # UIè¨­å®šã‚’é©ç”¨
            st.session_state.settings.update({
                "auto_refresh": settings.ui.auto_refresh,
                "refresh_interval": settings.ui.refresh_interval,
                "auto_save": settings.ui.auto_save,
                "save_interval": settings.ui.save_interval,
                "show_debug": settings.ui.show_debug,
                "max_chat_history": settings.ui.max_chat_history
            })
            
            # ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã‚’é©ç”¨
            self.api_base_url = settings.system.api_base_url
            
            logger.info("è¨­å®šã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«é©ç”¨ã—ã¾ã—ãŸ")
            
        except Exception as e:
            logger.error(f"è¨­å®šé©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _initialize_session_state(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–"""
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´
        if "messages" not in st.session_state:
            st.session_state.messages = []
        
        # ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆå±¥æ­´
        if "system_stats_history" not in st.session_state:
            st.session_state.system_stats_history = []
        
        # è¨­å®š
        if "settings" not in st.session_state:
            st.session_state.settings = {
                "model": "qwen2:7b-instruct",  # å®Ÿéš›ã«åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«
                "temperature": 0.7,
                "max_tokens": 500,
                "use_cot": True,
                "auto_refresh": True,
                "refresh_interval": 5,
                "auto_save": True,
                "save_interval": 10
            }
        
        # UI çŠ¶æ…‹
        if "current_session_id" not in st.session_state:
            st.session_state.current_session_id = str(uuid.uuid4())
        
        if "last_refresh" not in st.session_state:
            st.session_state.last_refresh = datetime.now()
        
        if "processing" not in st.session_state:
            st.session_state.processing = False
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
        if "session_start_time" not in st.session_state:
            st.session_state.session_start_time = datetime.now()
        
        if "saved_sessions" not in st.session_state:
            st.session_state.saved_sessions = {}
        
        if "api_calls" not in st.session_state:
            st.session_state.api_calls = 0
        
        if "error_count" not in st.session_state:
            st.session_state.error_count = 0
    
    def run(self):
        """ãƒ¡ã‚¤ãƒ³ UI å®Ÿè¡Œ"""
        
        # ãƒšãƒ¼ã‚¸è¨­å®š
        st.set_page_config(
            page_title="Advanced AI Agent",
            page_icon="ğŸ¤–",
            layout="wide",
            initial_sidebar_state="expanded"
        )
        
        # ã‚«ã‚¹ã‚¿ãƒ  CSS
        self._apply_custom_css()
        
        # ã‚µã‚¤ãƒ‰ãƒãƒ¼
        self._render_sidebar()
        
        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        self._render_main_content()
        
        # è‡ªå‹•ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥
        if st.session_state.settings["auto_refresh"]:
            self._auto_refresh()
    
    def _apply_custom_css(self):
        """ã‚«ã‚¹ã‚¿ãƒ  CSS é©ç”¨"""
        st.markdown("""
        <style>
        .main-header {
            font-size: 2.5rem;
            font-weight: bold;
            color: #1f77b4;
            text-align: center;
            margin-bottom: 2rem;
        }
        
        .metric-card {
            background-color: #f0f2f6;
            padding: 1rem;
            border-radius: 0.5rem;
            border-left: 4px solid #1f77b4;
            margin-bottom: 1rem;
        }
        
        .status-healthy {
            color: #28a745;
            font-weight: bold;
        }
        
        .status-warning {
            color: #ffc107;
            font-weight: bold;
        }
        
        .status-critical {
            color: #dc3545;
            font-weight: bold;
        }
        
        .chat-message {
            padding: 1rem;
            margin: 0.5rem 0;
            border-radius: 0.5rem;
        }
        
        .user-message {
            background-color: #e3f2fd;
            border-left: 4px solid #2196f3;
        }
        
        .assistant-message {
            background-color: #f3e5f5;
            border-left: 4px solid #9c27b0;
        }
        </style>
        """, unsafe_allow_html=True)
    
    def _render_sidebar(self):
        """ã‚µã‚¤ãƒ‰ãƒãƒ¼æç”»"""
        
        with st.sidebar:
            st.markdown("## âš™ï¸ è¨­å®š")
            
            # å‹•çš„ãƒ¢ãƒ‡ãƒ«é¸æŠ - Pydantic Settings ã«ã‚ˆã‚‹ ãƒ¢ãƒ‡ãƒ«é¸æŠãƒ»åˆ‡ã‚Šæ›¿ãˆ
            if self.settings_manager:
                settings = self.settings_manager.get_settings()
                model_names = list(settings.models.keys())
                
                # ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«é¸æŠ
                current_index = model_names.index(settings.current_model) if settings.current_model in model_names else 0
                
                selected_model = st.selectbox(
                    "ãƒ¢ãƒ‡ãƒ«",
                    model_names,
                    index=current_index,
                    help="ä½¿ç”¨ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ"
                )
                
                # ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆå‡¦ç†
                if selected_model != settings.current_model:
                    if settings.switch_model(selected_model):
                        self.settings_manager.save_settings(settings)
                        self._apply_settings_to_session(settings)
                        st.success(f"ãƒ¢ãƒ‡ãƒ«ã‚’ '{selected_model}' ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")
                        st.rerun()
                
                # ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’å–å¾—
                current_model_config = settings.get_current_model_config()
                if current_model_config:
                    # æ¨è«–è¨­å®šï¼ˆå‹•çš„ï¼‰
                    new_temperature = st.slider(
                        "Temperature", 
                        0.0, 2.0, 
                        current_model_config.temperature, 
                        0.1,
                        help="ç”Ÿæˆã®å‰µé€ æ€§ã‚’åˆ¶å¾¡"
                    )
                    
                    new_max_tokens = st.slider(
                        "Max Tokens", 
                        50, 4000, 
                        current_model_config.max_tokens, 
                        50,
                        help="ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°"
                    )
                    
                    # è¨­å®šå¤‰æ›´ã®æ¤œå‡ºã¨é©ç”¨
                    if (new_temperature != current_model_config.temperature or 
                        new_max_tokens != current_model_config.max_tokens):
                        
                        current_model_config.temperature = new_temperature
                        current_model_config.max_tokens = new_max_tokens
                        
                        # è‡ªå‹•ä¿å­˜
                        self.settings_manager.save_settings(settings)
                        self._apply_settings_to_session(settings)
                
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é™çš„è¨­å®š
                st.session_state.settings["model"] = st.selectbox(
                    "ãƒ¢ãƒ‡ãƒ«",
                    ["qwen2:7b-instruct", "deepseek-r1:7b", "qwen2.5:7b-instruct-q4_k_m", "qwen2:1.5b-instruct-q4_k_m"],
                    index=0
                )
                
                st.session_state.settings["temperature"] = st.slider(
                    "Temperature", 0.0, 2.0, 0.7, 0.1
                )
                
                st.session_state.settings["max_tokens"] = st.slider(
                    "Max Tokens", 50, 2000, 500, 50
                )
            
            st.session_state.settings["use_cot"] = st.checkbox(
                "Chain-of-Thought ã‚’ä½¿ç”¨", True
            )
            
            st.markdown("---")
            
            # è‡ªå‹•ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥è¨­å®š
            st.session_state.settings["auto_refresh"] = st.checkbox(
                "è‡ªå‹•ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥", True
            )
            
            if st.session_state.settings["auto_refresh"]:
                st.session_state.settings["refresh_interval"] = st.slider(
                    "ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥é–“éš”ï¼ˆç§’ï¼‰", 1, 30, 5
                )
            
            st.markdown("---")
            
            # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
            self._render_system_info()
            
            st.markdown("---")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
            self._render_session_management()
    
    def _render_system_info(self):
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤º"""
        
        st.markdown("### ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
        
        try:
            # ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆã‚’éåŒæœŸã§å–å¾—ï¼ˆç°¡ç•¥åŒ–ï¼‰
            system_stats = self._get_system_stats_sync()
            
            if system_stats:
                # CPU ä½¿ç”¨ç‡
                cpu_percent = system_stats.get("cpu_percent", 0)
                cpu_color = self._get_status_color(cpu_percent, 70, 90)
                st.markdown(f"**CPU:** <span class='{cpu_color}'>{cpu_percent:.1f}%</span>", 
                           unsafe_allow_html=True)
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
                memory_percent = system_stats.get("memory_percent", 0)
                memory_color = self._get_status_color(memory_percent, 70, 90)
                st.markdown(f"**ãƒ¡ãƒ¢ãƒª:** <span class='{memory_color}'>{memory_percent:.1f}%</span>", 
                           unsafe_allow_html=True)
                
                # GPU æƒ…å ±
                gpu_stats = self._get_gpu_stats_sync()
                if gpu_stats:
                    gpu_memory = gpu_stats.get("memory_percent", 0)
                    gpu_color = self._get_status_color(gpu_memory, 80, 95)
                    st.markdown(f"**GPU ãƒ¡ãƒ¢ãƒª:** <span class='{gpu_color}'>{gpu_memory:.1f}%</span>", 
                               unsafe_allow_html=True)
                    
                    gpu_temp = gpu_stats.get("temperature", 0)
                    temp_color = self._get_status_color(gpu_temp, 75, 85)
                    st.markdown(f"**GPU æ¸©åº¦:** <span class='{temp_color}'>{gpu_temp:.1f}Â°C</span>", 
                               unsafe_allow_html=True)
            
        except Exception as e:
            st.error(f"ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _get_status_color(self, value: float, warning_threshold: float, critical_threshold: float) -> str:
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è‰²å–å¾—"""
        if value >= critical_threshold:
            return "status-critical"
        elif value >= warning_threshold:
            return "status-warning"
        else:
            return "status-healthy"
    
    def _get_system_stats_sync(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆåŒæœŸå–å¾—ï¼ˆç°¡ç•¥åŒ–ï¼‰"""
        try:
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ asyncio.run() ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã§å®šæœŸå–å¾—
            import psutil
            return {
                "cpu_percent": psutil.cpu_percent(),
                "memory_percent": psutil.virtual_memory().percent
            }
        except Exception:
            return {"cpu_percent": 50.0, "memory_percent": 60.0}
    
    def _get_gpu_stats_sync(self) -> Dict[str, Any]:
        """GPU çµ±è¨ˆåŒæœŸå–å¾—ï¼ˆç°¡ç•¥åŒ–ï¼‰"""
        try:
            import pynvml
            pynvml.nvmlInit()
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            
            memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
            utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
            
            return {
                "memory_percent": (memory_info.used / memory_info.total) * 100,
                "temperature": temperature,
                "utilization_percent": utilization.gpu
            }
        except Exception:
            return {"memory_percent": 70.0, "temperature": 75.0, "utilization_percent": 80.0}
    
    def _render_session_management(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç† - Streamlit ã®æ—¢å­˜ã‚»ãƒƒã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã«ã‚ˆã‚‹ å±¥æ­´ç®¡ç†ãƒ»ç¶™ç¶š"""
        
        st.markdown("### ğŸ‘¤ ã‚»ãƒƒã‚·ãƒ§ãƒ³")
        
        st.text(f"ID: {st.session_state.current_session_id[:8]}...")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ
        session_stats = self._get_session_statistics()
        st.markdown(f"**ä¼šè©±æ•°:** {session_stats['message_count']}")
        st.markdown(f"**é–‹å§‹æ™‚åˆ»:** {session_stats['start_time']}")
        st.markdown(f"**ç¶™ç¶šæ™‚é–“:** {session_stats['duration']}")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸ”„ æ–°è¦ã‚»ãƒƒã‚·ãƒ§ãƒ³"):
                self._create_new_session()
        
        with col2:
            if st.button("ğŸ’¾ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜"):
                self._save_session()
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´ç®¡ç†
        st.markdown("#### ğŸ“‹ ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´")
        
        # ä¿å­˜æ¸ˆã¿ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§
        saved_sessions = self._get_saved_sessions()
        
        if saved_sessions:
            selected_session = st.selectbox(
                "ä¿å­˜æ¸ˆã¿ã‚»ãƒƒã‚·ãƒ§ãƒ³",
                options=list(saved_sessions.keys()),
                format_func=lambda x: f"{x[:8]}... ({saved_sessions[x]['timestamp']})"
            )
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("ğŸ“‚ ã‚»ãƒƒã‚·ãƒ§ãƒ³å¾©å…ƒ"):
                    self._restore_session(selected_session)
            
            with col2:
                if st.button("ğŸ“„ è©³ç´°è¡¨ç¤º"):
                    self._show_session_details(selected_session)
            
            with col3:
                if st.button("ğŸ—‘ï¸ ã‚»ãƒƒã‚·ãƒ§ãƒ³å‰Šé™¤"):
                    self._delete_session(selected_session)
        else:
            st.info("ä¿å­˜æ¸ˆã¿ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“")
        
        # è‡ªå‹•ä¿å­˜è¨­å®š
        st.markdown("#### âš™ï¸ è‡ªå‹•ä¿å­˜è¨­å®š")
        
        auto_save = st.checkbox("è‡ªå‹•ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜", value=True)
        
        if auto_save:
            save_interval = st.slider("ä¿å­˜é–“éš”ï¼ˆåˆ†ï¼‰", 1, 60, 10)
            st.session_state.settings["auto_save"] = True
            st.session_state.settings["save_interval"] = save_interval
        else:
            st.session_state.settings["auto_save"] = False
    
    def _get_session_statistics(self) -> Dict[str, Any]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆå–å¾—"""
        
        message_count = len(st.session_state.messages)
        start_time = st.session_state.get("session_start_time", datetime.now())
        duration = datetime.now() - start_time
        
        return {
            "message_count": message_count,
            "start_time": start_time.strftime("%H:%M:%S"),
            "duration": str(duration).split(".")[0]  # ç§’ä»¥ä¸‹ã‚’é™¤å»
        }
    
    def _create_new_session(self):
        """æ–°è¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        
        # ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è‡ªå‹•ä¿å­˜
        if st.session_state.settings.get("auto_save", True):
            self._save_session()
        
        # æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ ID ç”Ÿæˆ
        st.session_state.current_session_id = str(uuid.uuid4())
        st.session_state.messages = []
        st.session_state.session_start_time = datetime.now()
        
        st.success("æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¾ã—ãŸ")
        st.rerun()
    
    def _get_saved_sessions(self) -> Dict[str, Dict[str, Any]]:
        """ä¿å­˜æ¸ˆã¿ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§å–å¾—"""
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰å–å¾—ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯æ°¸ç¶šåŒ–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰ï¼‰
        saved_sessions = st.session_state.get("saved_sessions", {})
        
        return saved_sessions
    
    def _restore_session(self, session_id: str):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³å¾©å…ƒ"""
        
        try:
            saved_sessions = self._get_saved_sessions()
            
            if session_id in saved_sessions:
                session_data = saved_sessions[session_id]
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒ
                st.session_state.current_session_id = session_id
                st.session_state.messages = session_data.get("messages", [])
                st.session_state.settings.update(session_data.get("settings", {}))
                
                st.success(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id[:8]}... ã‚’å¾©å…ƒã—ã¾ã—ãŸ")
                st.rerun()
            else:
                st.error("ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                
        except Exception as e:
            st.error(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³å¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
    
    def _show_session_details(self, session_id: str):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°è¡¨ç¤º"""
        
        try:
            saved_sessions = self._get_saved_sessions()
            
            if session_id in saved_sessions:
                session_data = saved_sessions[session_id]
                
                with st.expander(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°: {session_id[:8]}..."):
                    st.json(session_data)
            else:
                st.error("ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                
        except Exception as e:
            st.error(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³è©³ç´°è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
    
    def _delete_session(self, session_id: str):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³å‰Šé™¤"""
        
        try:
            if "saved_sessions" not in st.session_state:
                st.session_state.saved_sessions = {}
            
            if session_id in st.session_state.saved_sessions:
                del st.session_state.saved_sessions[session_id]
                st.success(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id[:8]}... ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                st.rerun()
            else:
                st.error("ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                
        except Exception as e:
            st.error(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _render_main_content(self):
        """ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æç”»"""
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        st.markdown('<h1 class="main-header">ğŸ¤– Advanced AI Agent</h1>', 
                   unsafe_allow_html=True)
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ãƒ»VRAMè¡¨ç¤º
        self._render_realtime_progress_indicator()
        
        st.markdown("---")
        
        # ã‚¿ãƒ–æ§‹æˆ
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ’¬ ãƒãƒ£ãƒƒãƒˆ", "ğŸ“Š ç›£è¦–", "ğŸ” è¨˜æ†¶æ¤œç´¢", "âš™ï¸ ç®¡ç†"])
        
        with tab1:
            self._render_chat_interface()
        
        with tab2:
            self._render_monitoring_dashboard()
        
        with tab3:
            self._render_memory_search()
        
        with tab4:
            self._render_admin_panel()
    
    def _render_chat_interface(self):
        """ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¿œç­”æ€§æ©Ÿèƒ½çµ±åˆ"""
        
        st.markdown("### ğŸ’¬ AI ãƒãƒ£ãƒƒãƒˆ")
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒãƒ£ãƒƒãƒˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        self._render_realtime_chat_status()
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
        chat_container = st.container()
        
        with chat_container:
            for message in st.session_state.messages:
                self._render_message(message)
        
        # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
        with st.form("chat_form", clear_on_submit=True):
            col1, col2 = st.columns([4, 1])
            
            with col1:
                user_input = st.text_area(
                    "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...",
                    height=100,
                    key="chat_input",
                    placeholder="è³ªå•ã‚„æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚Enterã§é€ä¿¡ã€Shift+Enterã§æ”¹è¡Œ"
                )
            
            with col2:
                st.markdown("<br>", unsafe_allow_html=True)
                submit_button = st.form_submit_button("é€ä¿¡ ğŸ“¤", use_container_width=True)
                
                if st.form_submit_button("ğŸ”„ ã‚¯ãƒªã‚¢", use_container_width=True):
                    st.session_state.messages = []
                    st.rerun()
        
        # ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆæƒ…å ±
        st.markdown("""
        <div style="font-size: 0.8rem; color: #6c757d; margin-top: 10px;">
            ğŸ’¡ ãƒ’ãƒ³ãƒˆ: Ctrl+Enter ã§é€ä¿¡ã€Shift+Enter ã§æ”¹è¡Œ
        </div>
        """, unsafe_allow_html=True)
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†
        if submit_button and user_input.strip():
            self._process_chat_message(user_input.strip())
    
    def _render_message(self, message: Dict[str, Any]):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æç”»"""
        
        role = message["role"]
        content = message["content"]
        timestamp = message.get("timestamp", datetime.now())
        
        if role == "user":
            st.markdown(f"""
            <div class="chat-message user-message">
                <strong>ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼</strong> <small>({timestamp.strftime('%H:%M:%S')})</small><br>
                {content}
            </div>
            """, unsafe_allow_html=True)
        
        elif role == "assistant":
            processing_time = message.get("processing_time", 0)
            confidence = message.get("confidence_score")
            
            confidence_text = f" (ä¿¡é ¼åº¦: {confidence:.2f})" if confidence else ""
            
            st.markdown(f"""
            <div class="chat-message assistant-message">
                <strong>ğŸ¤– AI ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ</strong> 
                <small>({timestamp.strftime('%H:%M:%S')} - {processing_time:.2f}ç§’{confidence_text})</small><br>
                {content}
            </div>
            """, unsafe_allow_html=True)
            
            # æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—è¡¨ç¤º - æ”¹è‰¯ç‰ˆ
            if message.get("reasoning_steps"):
                reasoning_steps = message["reasoning_steps"]
                step_count = len(reasoning_steps)
                
                with st.expander(f"ğŸ§  æ¨è«–éç¨‹ã‚’è¡¨ç¤º ({step_count}ã‚¹ãƒ†ãƒƒãƒ—)", expanded=False):
                    # æ¨è«–å“è³ªã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼
                    quality_score = message.get("quality_score", 0.0)
                    confidence = message.get("confidence_score", 0.0)
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°", step_count)
                    with col2:
                        st.metric("ä¿¡é ¼åº¦", f"{confidence:.2f}")
                    with col3:
                        if quality_score > 0:
                            st.metric("å“è³ªã‚¹ã‚³ã‚¢", f"{quality_score:.2f}")
                    
                    st.markdown("---")
                    
                    # å„æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¡¨ç¤º
                    for i, step in enumerate(reasoning_steps, 1):
                        # ã‚¹ãƒ†ãƒƒãƒ—ã‚¿ã‚¤ãƒ—ã‚’æ¨å®š
                        step_type = self._detect_step_type(step)
                        step_icon = self._get_step_icon(step_type)
                        
                        # ã‚¹ãƒ†ãƒƒãƒ—è¡¨ç¤º
                        st.markdown(f"""
                        <div style="margin: 10px 0; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #007bff; border-radius: 5px;">
                            <strong>{step_icon} ã‚¹ãƒ†ãƒƒãƒ— {i} ({step_type})</strong><br>
                            <span style="color: #495057;">{step}</span>
                        </div>
                        """, unsafe_allow_html=True)
                    
                    # æ¨è«–çµ±è¨ˆ
                    if step_count > 0:
                        avg_step_length = sum(len(step) for step in reasoning_steps) / step_count
                        st.markdown(f"""
                        <div style="margin-top: 15px; padding: 8px; background-color: #e9ecef; border-radius: 5px; font-size: 0.9rem;">
                            ğŸ“Š <strong>æ¨è«–çµ±è¨ˆ:</strong> å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—é•·: {avg_step_length:.0f}æ–‡å­—
                        </div>
                        """, unsafe_allow_html=True)
    
    def _detect_step_type(self, step: str) -> str:
        """æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¿ã‚¤ãƒ—ã‚’æ¤œå‡º"""
        
        step_lower = step.lower()
        
        if any(keyword in step_lower for keyword in ['æ€è€ƒ', 'thought', 'è€ƒãˆ', 'æ¤œè¨']):
            return "æ€è€ƒ"
        elif any(keyword in step_lower for keyword in ['è¡Œå‹•', 'action', 'å®Ÿè¡Œ', 'è¨ˆç®—']):
            return "è¡Œå‹•"
        elif any(keyword in step_lower for keyword in ['è¦³å¯Ÿ', 'observation', 'çµæœ', 'ç¢ºèª']):
            return "è¦³å¯Ÿ"
        elif any(keyword in step_lower for keyword in ['çµè«–', 'conclusion', 'å›ç­”', 'ç­”ãˆ']):
            return "çµè«–"
        elif any(keyword in step_lower for keyword in ['åˆ†æ', 'ç†è§£', 'æ•´ç†']):
            return "åˆ†æ"
        else:
            return "æ¨è«–"
    
    def _get_step_icon(self, step_type: str) -> str:
        """ã‚¹ãƒ†ãƒƒãƒ—ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸã‚¢ã‚¤ã‚³ãƒ³ã‚’å–å¾—"""
        
        icons = {
            "æ€è€ƒ": "ğŸ¤”",
            "è¡Œå‹•": "âš¡",
            "è¦³å¯Ÿ": "ğŸ‘ï¸",
            "çµè«–": "âœ…",
            "åˆ†æ": "ğŸ”",
            "æ¨è«–": "ğŸ’­"
        }
        
        return icons.get(step_type, "ğŸ“")
    
    def _calculate_reasoning_quality(self, reasoning_steps: List[str], processing_time: float) -> float:
        """æ¨è«–å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""
        
        try:
            quality_factors = []
            
            # ã‚¹ãƒ†ãƒƒãƒ—æ•°ã«ã‚ˆã‚‹å“è³ªè©•ä¾¡
            step_count = len(reasoning_steps)
            if step_count >= 3:
                step_score = min(step_count / 6.0, 1.0)  # 6ã‚¹ãƒ†ãƒƒãƒ—ã§æœ€å¤§
            else:
                step_score = step_count / 3.0
            quality_factors.append(step_score)
            
            # ã‚¹ãƒ†ãƒƒãƒ—ã®å¤šæ§˜æ€§è©•ä¾¡
            step_types = set(self._detect_step_type(step) for step in reasoning_steps)
            diversity_score = len(step_types) / 6.0  # 6ç¨®é¡ã®ã‚¿ã‚¤ãƒ—ã§æœ€å¤§
            quality_factors.append(diversity_score)
            
            # ã‚¹ãƒ†ãƒƒãƒ—ã®è©³ç´°åº¦è©•ä¾¡
            avg_step_length = sum(len(step) for step in reasoning_steps) / step_count if step_count > 0 else 0
            detail_score = min(avg_step_length / 100.0, 1.0)  # 100æ–‡å­—ã§æœ€å¤§
            quality_factors.append(detail_score)
            
            # å‡¦ç†æ™‚é–“åŠ¹ç‡è©•ä¾¡
            time_efficiency = 1.0 - min(processing_time / 20.0, 1.0)  # 20ç§’ã§æœ€ä½
            quality_factors.append(time_efficiency)
            
            return sum(quality_factors) / len(quality_factors) if quality_factors else 0.0
            
        except Exception as e:
            logger.error(f"å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.5
    
    def _calculate_confidence_score(self, reasoning_steps: List[str], response: str) -> float:
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        
        try:
            confidence_factors = []
            
            # æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°ã«ã‚ˆã‚‹ä¿¡é ¼åº¦
            step_count = len(reasoning_steps)
            step_confidence = min(step_count / 5.0, 1.0)  # 5ã‚¹ãƒ†ãƒƒãƒ—ã§æœ€å¤§
            confidence_factors.append(step_confidence)
            
            # è«–ç†çš„æ¥ç¶šè©ã®å­˜åœ¨
            logical_connectors = ['ãªãœãªã‚‰', 'ã—ãŸãŒã£ã¦', 'ãã®ãŸã‚', 'ã¤ã¾ã‚Š', 'ã¾ãŸ', 'ã•ã‚‰ã«', 'because', 'therefore', 'thus']
            connector_count = sum(1 for connector in logical_connectors if connector in response.lower())
            connector_confidence = min(connector_count / 3.0, 1.0)  # 3å€‹ã§æœ€å¤§
            confidence_factors.append(connector_confidence)
            
            # å…·ä½“çš„ãªæ•°å€¤ã‚„äº‹å®Ÿã®å­˜åœ¨
            has_numbers = bool(re.search(r'\d+', response))
            has_specifics = any(keyword in response for keyword in ['å…·ä½“çš„', 'ä¾‹ãˆã°', 'å®Ÿéš›ã«', 'è©³ç´°'])
            specificity_confidence = (0.5 if has_numbers else 0.0) + (0.5 if has_specifics else 0.0)
            confidence_factors.append(specificity_confidence)
            
            # çµè«–ã®æ˜ç¢ºæ€§
            has_clear_conclusion = any(keyword in response for keyword in ['çµè«–', 'ç­”ãˆ', 'å›ç­”', 'æœ€çµ‚çš„', 'ã¾ã¨ã‚'])
            conclusion_confidence = 1.0 if has_clear_conclusion else 0.6
            confidence_factors.append(conclusion_confidence)
            
            return sum(confidence_factors) / len(confidence_factors) if confidence_factors else 0.5
            
        except Exception as e:
            logger.error(f"ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.5
    
    def _process_chat_message(self, user_input: str):
        """ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†"""
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
        user_message = {
            "role": "user",
            "content": user_input,
            "timestamp": datetime.now()
        }
        st.session_state.messages.append(user_message)
        
        # å‡¦ç†ä¸­è¡¨ç¤º
        with st.spinner("AI ãŒè€ƒãˆã¦ã„ã¾ã™..."):
            try:
                # API å‘¼ã³å‡ºã—ï¼ˆç°¡ç•¥åŒ–ï¼‰
                response = self._call_chat_api(user_input)
                
                # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
                assistant_message = {
                    "role": "assistant",
                    "content": response.get("response", "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"),
                    "timestamp": datetime.now(),
                    "processing_time": response.get("processing_time", 0),
                    "confidence_score": response.get("confidence_score"),
                    "reasoning_steps": response.get("reasoning_steps")
                }
                st.session_state.messages.append(assistant_message)
                
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                
                error_message = {
                    "role": "assistant",
                    "content": f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                    "timestamp": datetime.now()
                }
                st.session_state.messages.append(error_message)
        
        # UI ã‚’æ›´æ–°
        st.rerun()
    
    def _call_chat_api(self, user_input: str) -> Dict[str, Any]:
        """ãƒãƒ£ãƒƒãƒˆ API å‘¼ã³å‡ºã— - å®Ÿéš›ã®Ollamaæ¥ç¶š"""
        
        start_time = time.time()
        
        try:
            # 1. ç›´æ¥Ollama APIå‘¼ã³å‡ºã—ã‚’è©¦è¡Œ
            ollama_response = self._call_ollama_direct(user_input)
            if ollama_response:
                processing_time = time.time() - start_time
                # æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æŠ½å‡ºã¨å“è³ªè©•ä¾¡
                reasoning_steps = None
                quality_score = 0.0
                confidence_score = 0.85
                
                if st.session_state.settings.get("use_cot"):
                    reasoning_steps = self._extract_reasoning_steps(ollama_response)
                    if reasoning_steps:
                        quality_score = self._calculate_reasoning_quality(reasoning_steps, processing_time)
                        confidence_score = self._calculate_confidence_score(reasoning_steps, ollama_response)
                
                return {
                    "response": ollama_response,
                    "processing_time": processing_time,
                    "confidence_score": confidence_score,
                    "quality_score": quality_score,
                    "reasoning_steps": reasoning_steps
                }
            
            # 2. FastAPI ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå‘¼ã³å‡ºã—ã‚’è©¦è¡Œ
            fastapi_response = self._call_fastapi_endpoint(user_input)
            if fastapi_response:
                processing_time = time.time() - start_time
                return fastapi_response
            
            # 3. è»½é‡ãƒ¢ãƒ‡ãƒ«ã§ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            fallback_response = self._call_ollama_fallback(user_input)
            if fallback_response:
                processing_time = time.time() - start_time
                return {
                    "response": fallback_response,
                    "processing_time": processing_time,
                    "confidence_score": 0.65,
                    "model_used": "fallback"
                }
            
            # 4. æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return {
                "response": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ç¾åœ¨AIãƒ¢ãƒ‡ãƒ«ã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
                "processing_time": time.time() - start_time,
                "confidence_score": 0.0,
                "error": "connection_failed"
            }
                
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"ãƒãƒ£ãƒƒãƒˆAPIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°çµ±åˆ
            try:
                error_info = asyncio.run(self.error_handler.handle_error(e, {
                    "user_input": user_input,
                    "model": st.session_state.settings.get("model", "unknown"),
                    "processing_time": processing_time
                }))
                
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                user_message = self._generate_user_friendly_error_message(error_info)
                
                return {
                    "response": user_message,
                    "processing_time": processing_time,
                    "confidence_score": 0.0,
                    "error": str(e),
                    "error_info": {
                        "type": error_info.error_type.value,
                        "severity": error_info.severity.value,
                        "suggestions": error_info.recovery_suggestions
                    }
                }
                
            except Exception as handler_error:
                logger.error(f"ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {handler_error}")
                return {
                    "response": f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                    "processing_time": processing_time,
                    "confidence_score": 0.0,
                    "error": str(e)
                }
    
    def _call_ollama_direct(self, user_input: str) -> Optional[str]:
        """ç›´æ¥Ollama APIå‘¼ã³å‡ºã— - Chain-of-Thoughtæ¨è«–çµ±åˆ"""
        
        try:
            import ollama
            
            model = st.session_state.settings["model"]
            temperature = st.session_state.settings["temperature"]
            
            # Chain-of-Thought æ¨è«–ãŒæœ‰åŠ¹ãªå ´åˆ
            if st.session_state.settings.get("use_cot", True):
                return self._execute_cot_reasoning(user_input, model, temperature)
            else:
                # é€šå¸¸ã®æ¨è«–
                response = ollama.chat(
                    model=model,
                    messages=[
                        {"role": "user", "content": user_input}
                    ],
                    options={
                        "temperature": temperature,
                        "num_predict": st.session_state.settings.get("max_tokens", 500)
                    }
                )
                
                return response["message"]["content"]
            
        except ImportError:
            logger.warning("ollama ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None
        except Exception as e:
            logger.error(f"Ollamaç›´æ¥å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _execute_cot_reasoning(self, user_input: str, model: str, temperature: float) -> Optional[str]:
        """Chain-of-Thought æ¨è«–å®Ÿè¡Œ"""
        
        try:
            import ollama
            
            # ReAct Agent ã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            cot_prompt = f"""ã‚ãªãŸã¯æ®µéšçš„ã«è€ƒãˆã‚‹å„ªç§€ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€æ®µéšçš„ã«æ¨è«–ã—ã¦ãã ã•ã„ã€‚

åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:
- calculator: æ•°å¼è¨ˆç®— (ä¾‹: 2+3*4, 10/2)
- analyzer: ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
- knowledge: çŸ¥è­˜æ¤œç´¢

æ¨è«–å½¢å¼:
Question: è§£æ±ºã™ã¹ãå•é¡Œ
Thought: ä½•ã‚’è€ƒãˆã€ã©ã®ã‚ˆã†ãªè¡Œå‹•ã‚’å–ã‚‹ã¹ãã‹
Action: å®Ÿè¡Œã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (calculator/analyzer/knowledge/none)
Action Input: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¸ã®å…¥åŠ›
Observation: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®çµæœ
... (å¿…è¦ã«å¿œã˜ã¦Thought/Action/Action Input/Observationã‚’ç¹°ã‚Šè¿”ã—)
Thought: æœ€çµ‚çš„ãªç­”ãˆãŒã‚ã‹ã‚Šã¾ã—ãŸ
Final Answer: æœ€çµ‚å›ç­”

é‡è¦ãªæŒ‡ç¤º:
1. å„ã‚¹ãƒ†ãƒƒãƒ—ã§æ˜ç¢ºã«æ€è€ƒéç¨‹ã‚’ç¤ºã—ã¦ãã ã•ã„
2. è¤‡é›‘ãªå•é¡Œã¯å°ã•ãªéƒ¨åˆ†ã«åˆ†è§£ã—ã¦ãã ã•ã„
3. è¨ˆç®—ãŒå¿…è¦ãªå ´åˆã¯calculatorãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
4. æœ€çµ‚å›ç­”ã§ã¯ã€æ¨è«–éç¨‹ã‚’è¦ç´„ã—ã¦ãã ã•ã„

Question: {user_input}
Thought:"""

            # æ®µéšçš„æ¨è«–å®Ÿè¡Œ
            reasoning_steps = []
            current_prompt = cot_prompt
            max_iterations = 5
            
            for iteration in range(max_iterations):
                # Ollama APIå‘¼ã³å‡ºã—
                response = ollama.chat(
                    model=model,
                    messages=[
                        {"role": "user", "content": current_prompt}
                    ],
                    options={
                        "temperature": temperature,
                        "num_predict": 200  # å„ã‚¹ãƒ†ãƒƒãƒ—ã¯çŸ­ã‚ã«
                    }
                )
                
                step_response = response["message"]["content"]
                reasoning_steps.append(step_response)
                
                # Final AnswerãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯çµ‚äº†
                if "Final Answer:" in step_response:
                    break
                
                # ActionãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                if "Action:" in step_response and "Action Input:" in step_response:
                    action_result = self._simulate_tool_execution(step_response)
                    current_prompt += f"\n{step_response}\nObservation: {action_result}\nThought:"
                else:
                    current_prompt += f"\n{step_response}\nThought:"
            
            # æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã‚’çµ±åˆã—ã¦æœ€çµ‚å›ç­”ã‚’æ§‹ç¯‰
            final_response = self._build_cot_response(reasoning_steps, user_input)
            return final_response
            
        except Exception as e:
            logger.error(f"CoTæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _simulate_tool_execution(self, step_response: str) -> str:
        """ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        
        try:
            # Action ã¨ Action Input ã‚’æŠ½å‡º
            action_match = re.search(r"Action:\s*(\w+)", step_response)
            input_match = re.search(r"Action Input:\s*(.+?)(?=\n|$)", step_response)
            
            if not action_match or not input_match:
                return "ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ"
            
            action = action_match.group(1).lower()
            action_input = input_match.group(1).strip()
            
            # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
            if action == "calculator":
                return self._execute_calculator(action_input)
            elif action == "analyzer":
                return self._execute_analyzer(action_input)
            elif action == "knowledge":
                return self._execute_knowledge_search(action_input)
            else:
                return f"ä¸æ˜ãªãƒ„ãƒ¼ãƒ«: {action}"
                
        except Exception as e:
            return f"ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def _execute_calculator(self, expression: str) -> str:
        """è¨ˆç®—ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ"""
        
        try:
            # å®‰å…¨ãªè¨ˆç®—ã®ãŸã‚ã€åŸºæœ¬çš„ãªæ¼”ç®—ã®ã¿è¨±å¯
            import ast
            import operator
            
            # ã‚µãƒãƒ¼ãƒˆã™ã‚‹æ¼”ç®—å­
            ops = {
                ast.Add: operator.add,
                ast.Sub: operator.sub,
                ast.Mult: operator.mul,
                ast.Div: operator.truediv,
                ast.Pow: operator.pow,
                ast.USub: operator.neg,
            }
            
            def eval_expr(node):
                if isinstance(node, ast.Num):
                    return node.n
                elif isinstance(node, ast.Constant):  # Python 3.8+
                    return node.value
                elif isinstance(node, ast.BinOp):
                    return ops[type(node.op)](eval_expr(node.left), eval_expr(node.right))
                elif isinstance(node, ast.UnaryOp):
                    return ops[type(node.op)](eval_expr(node.operand))
                else:
                    raise TypeError(f"Unsupported operation: {node}")
            
            # å¼ã‚’è§£æã—ã¦è¨ˆç®—
            result = eval_expr(ast.parse(expression, mode='eval').body)
            return f"è¨ˆç®—çµæœ: {result}"
            
        except Exception as e:
            return f"è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def _execute_analyzer(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ"""
        
        try:
            word_count = len(text.split())
            char_count = len(text)
            sentence_count = len([s for s in text.split('.') if s.strip()])
            
            # ç°¡å˜ãªæ„Ÿæƒ…åˆ†æ
            positive_words = ['è‰¯ã„', 'ç´ æ™´ã‚‰ã—ã„', 'å„ªç§€', 'æˆåŠŸ', 'åŠ¹æœçš„', 'good', 'great', 'excellent', 'æ­£ã—ã„', 'é©åˆ‡']
            negative_words = ['æ‚ªã„', 'å•é¡Œ', 'å¤±æ•—', 'å›°é›£', 'å±é™º', 'bad', 'problem', 'fail', 'é–“é•ã„', 'ä¸é©åˆ‡']
            
            positive_count = sum(1 for word in positive_words if word in text.lower())
            negative_count = sum(1 for word in negative_words if word in text.lower())
            
            sentiment = "ä¸­æ€§"
            if positive_count > negative_count:
                sentiment = "ãƒã‚¸ãƒ†ã‚£ãƒ–"
            elif negative_count > positive_count:
                sentiment = "ãƒã‚¬ãƒ†ã‚£ãƒ–"
            
            return f"åˆ†æçµæœ: æ–‡å­—æ•°{char_count}, å˜èªæ•°{word_count}, æ–‡æ•°{sentence_count}, æ„Ÿæƒ…{sentiment}"
            
        except Exception as e:
            return f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def _execute_knowledge_search(self, query: str) -> str:
        """çŸ¥è­˜æ¤œç´¢ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ"""
        
        # ç°¡æ˜“çš„ãªçŸ¥è­˜ãƒ™ãƒ¼ã‚¹
        knowledge_base = {
            "python": "Pythonã¯é«˜æ°´æº–ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã€èª­ã¿ã‚„ã™ãæ›¸ãã‚„ã™ã„æ§‹æ–‡ãŒç‰¹å¾´ã§ã™ã€‚",
            "ai": "äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ã¯ã€äººé–“ã®çŸ¥èƒ½ã‚’æ¨¡å€£ã™ã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚",
            "machine learning": "æ©Ÿæ¢°å­¦ç¿’ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¦ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹AIã®æ‰‹æ³•ã§ã™ã€‚",
            "deep learning": "æ·±å±¤å­¦ç¿’ã¯ã€å¤šå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ã®æ‰‹æ³•ã§ã™ã€‚",
            "langchain": "LangChainã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã®ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚",
            "ollama": "Ollamaã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚",
            "streamlit": "Streamlitã¯ã€Pythonã§ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç°¡å˜ã«ä½œæˆã§ãã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚"
        }
        
        query_lower = query.lower()
        for key, value in knowledge_base.items():
            if key in query_lower:
                return f"çŸ¥è­˜: {value}"
        
        return f"'{query}'ã«é–¢ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    def _build_cot_response(self, reasoning_steps: List[str], original_question: str) -> str:
        """Chain-of-Thought ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰"""
        
        try:
            # æœ€çµ‚å›ç­”ã‚’æŠ½å‡º
            final_answer = ""
            for step in reversed(reasoning_steps):
                if "Final Answer:" in step:
                    final_answer_match = re.search(r"Final Answer:\s*(.+?)(?=\n|$)", step, re.DOTALL)
                    if final_answer_match:
                        final_answer = final_answer_match.group(1).strip()
                        break
            
            # æ¨è«–éç¨‹ã‚’è¦ç´„
            thought_steps = []
            for i, step in enumerate(reasoning_steps, 1):
                if "Thought:" in step:
                    thought_match = re.search(r"Thought:\s*(.+?)(?=Action:|Final Answer:|$)", step, re.DOTALL)
                    if thought_match:
                        thought = thought_match.group(1).strip()
                        if thought and len(thought) > 10:  # æ„å‘³ã®ã‚ã‚‹æ€è€ƒã®ã¿
                            thought_steps.append(f"ã‚¹ãƒ†ãƒƒãƒ—{i}: {thought}")
            
            # æœ€çµ‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰
            if final_answer:
                response = f"{final_answer}\n\n"
                if thought_steps:
                    response += "ã€æ¨è«–éç¨‹ã€‘\n" + "\n".join(thought_steps[:5])  # æœ€å¤§5ã‚¹ãƒ†ãƒƒãƒ—
                return response
            else:
                # Final AnswerãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä½¿ç”¨
                return reasoning_steps[-1] if reasoning_steps else "æ¨è«–ã‚’å®Œäº†ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
                
        except Exception as e:
            logger.error(f"CoTãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
            return "æ¨è«–çµæœã®æ§‹ç¯‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
    
    def _call_fastapi_endpoint(self, user_input: str) -> Optional[Dict[str, Any]]:
        """FastAPI ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå‘¼ã³å‡ºã—"""
        
        try:
            url = f"{self.api_base_url}/v1/chat/completions"
            
            payload = {
                "model": st.session_state.settings["model"],
                "messages": [
                    {"role": "user", "content": user_input}
                ],
                "temperature": st.session_state.settings["temperature"],
                "max_tokens": st.session_state.settings["max_tokens"]
            }
            
            response = requests.post(url, json=payload, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                return {
                    "response": data["choices"][0]["message"]["content"],
                    "processing_time": 1.5,
                    "confidence_score": 0.85
                }
            else:
                logger.warning(f"FastAPI ã‚¨ãƒ©ãƒ¼: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"FastAPIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _call_ollama_fallback(self, user_input: str) -> Optional[str]:
        """è»½é‡ãƒ¢ãƒ‡ãƒ«ã§ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        
        try:
            import ollama
            
            # è»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ
            fallback_models = ["qwen2:7b-instruct", "qwen2:1.5b-instruct-q4_k_m", "qwen2.5:7b-instruct-q4_k_m"]
            
            for model in fallback_models:
                try:
                    response = ollama.chat(
                        model=model,
                        messages=[
                            {"role": "user", "content": user_input}
                        ],
                        options={
                            "temperature": 0.7,
                            "num_predict": 200  # è»½é‡åŒ–ã®ãŸã‚çŸ­ã
                        }
                    )
                    
                    return f"[è»½é‡ãƒ¢ãƒ‡ãƒ« {model} ã«ã‚ˆã‚‹å›ç­”]\n{response['message']['content']}"
                    
                except Exception as model_error:
                    logger.warning(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ« {model} ã‚¨ãƒ©ãƒ¼: {model_error}")
                    continue
            
            return None
            
        except ImportError:
            return None
        except Exception as e:
            logger.error(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _extract_reasoning_steps(self, response: str) -> List[str]:
        """æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã®æŠ½å‡º - æ”¹è‰¯ç‰ˆ"""
        
        try:
            steps = []
            
            # Chain-of-Thought ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
            cot_patterns = [
                r"ã‚¹ãƒ†ãƒƒãƒ—\d+:\s*(.+?)(?=ã‚¹ãƒ†ãƒƒãƒ—\d+:|$)",
                r"Thought:\s*(.+?)(?=Action:|Observation:|Final Answer:|$)",
                r"Action:\s*(.+?)(?=Action Input:|Thought:|$)",
                r"Observation:\s*(.+?)(?=Thought:|Action:|$)",
                r"\d+\.\s*(.+?)(?=\d+\.|$)",
                r"ã€æ¨è«–éç¨‹ã€‘\n(.+?)(?=ã€|$)"
            ]
            
            for pattern in cot_patterns:
                matches = re.finditer(pattern, response, re.DOTALL | re.IGNORECASE)
                for match in matches:
                    content = match.group(1).strip()
                    if content and len(content) > 10:  # æ„å‘³ã®ã‚ã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿
                        # æ”¹è¡Œã‚’é™¤å»ã—ã¦æ•´å½¢
                        content = re.sub(r'\n+', ' ', content).strip()
                        if content not in steps:  # é‡è¤‡é™¤å»
                            steps.append(content)
            
            # ç•ªå·ä»˜ããƒªã‚¹ãƒˆã‚„æ®µéšçš„æ€è€ƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚æ¤œå‡º
            lines = response.split('\n')
            for line in lines:
                line = line.strip()
                
                # ç•ªå·ä»˜ããƒªã‚¹ãƒˆæ¤œå‡º
                if re.match(r'^\d+\.\s+.{10,}', line):
                    clean_line = re.sub(r'^\d+\.\s+', '', line)
                    if clean_line not in steps:
                        steps.append(clean_line)
                
                # æ®µéšçš„æ€è€ƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
                elif any(keyword in line for keyword in ['ç†è§£', 'æ•´ç†', 'æ¤œè¨', 'å›ç­”', 'ã‚¹ãƒ†ãƒƒãƒ—', 'æ®µéš', 'åˆ†æ', 'è€ƒå¯Ÿ']):
                    if len(line) > 15 and line not in steps:
                        steps.append(line)
            
            # é‡è¤‡é™¤å»ã¨é•·ã•åˆ¶é™
            unique_steps = []
            for step in steps:
                if step not in unique_steps and len(step) > 10:
                    unique_steps.append(step[:200])  # å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’200æ–‡å­—ä»¥å†…ã«åˆ¶é™
            
            return unique_steps[:8]  # æœ€å¤§8ã‚¹ãƒ†ãƒƒãƒ—ã¾ã§
            
        except Exception as e:
            logger.error(f"æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _render_monitoring_dashboard(self):
        """ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰"""
        
        st.markdown("### ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–")
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆ
        col1, col2, col3, col4 = st.columns(4)
        
        system_stats = self._get_system_stats_sync()
        gpu_stats = self._get_gpu_stats_sync()
        
        with col1:
            st.metric(
                "CPU ä½¿ç”¨ç‡",
                f"{system_stats.get('cpu_percent', 0):.1f}%",
                delta=None
            )
        
        with col2:
            st.metric(
                "ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡",
                f"{system_stats.get('memory_percent', 0):.1f}%",
                delta=None
            )
        
        with col3:
            st.metric(
                "GPU ãƒ¡ãƒ¢ãƒª",
                f"{gpu_stats.get('memory_percent', 0):.1f}%",
                delta=None
            )
        
        with col4:
            st.metric(
                "GPU æ¸©åº¦",
                f"{gpu_stats.get('temperature', 0):.1f}Â°C",
                delta=None
            )
        
        # ã‚°ãƒ©ãƒ•è¡¨ç¤º
        self._render_performance_charts()
    
    def _render_performance_charts(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ£ãƒ¼ãƒˆ - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å±¥æ­´ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–"""
        
        # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if st.session_state.system_stats_history:
            # å®Ÿéš›ã®å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ™‚ç³»åˆ—ãƒãƒ£ãƒ¼ãƒˆä½œæˆ
            history_df = pd.DataFrame(st.session_state.system_stats_history)
            
            fig = make_subplots(
                rows=2, cols=2,
                subplot_titles=('CPU ä½¿ç”¨ç‡', 'ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡', 'GPU ãƒ¡ãƒ¢ãƒª', 'GPU æ¸©åº¦'),
                specs=[[{"secondary_y": False}, {"secondary_y": False}],
                       [{"secondary_y": False}, {"secondary_y": False}]]
            )
            
            fig.add_trace(
                go.Scatter(
                    x=history_df["timestamp"], 
                    y=history_df["cpu_percent"], 
                    name="CPU", 
                    line=dict(color="blue"),
                    mode='lines+markers'
                ),
                row=1, col=1
            )
            
            fig.add_trace(
                go.Scatter(
                    x=history_df["timestamp"], 
                    y=history_df["memory_percent"], 
                    name="ãƒ¡ãƒ¢ãƒª", 
                    line=dict(color="green"),
                    mode='lines+markers'
                ),
                row=1, col=2
            )
            
            fig.add_trace(
                go.Scatter(
                    x=history_df["timestamp"], 
                    y=history_df["gpu_memory_percent"], 
                    name="GPU ãƒ¡ãƒ¢ãƒª", 
                    line=dict(color="red"),
                    mode='lines+markers'
                ),
                row=2, col=1
            )
            
            fig.add_trace(
                go.Scatter(
                    x=history_df["timestamp"], 
                    y=history_df["gpu_temperature"], 
                    name="GPU æ¸©åº¦", 
                    line=dict(color="orange"),
                    mode='lines+markers'
                ),
                row=2, col=2
            )
            
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            import numpy as np
            
            timestamps = pd.date_range(
                start=datetime.now() - timedelta(minutes=30),
                end=datetime.now(),
                freq='1min'
            )
            
            fig = make_subplots(
                rows=2, cols=2,
                subplot_titles=('CPU ä½¿ç”¨ç‡', 'ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡', 'GPU ãƒ¡ãƒ¢ãƒª', 'GPU æ¸©åº¦'),
                specs=[[{"secondary_y": False}, {"secondary_y": False}],
                       [{"secondary_y": False}, {"secondary_y": False}]]
            )
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
            cpu_data = np.random.normal(50, 10, len(timestamps))
            memory_data = np.random.normal(60, 8, len(timestamps))
            gpu_memory_data = np.random.normal(70, 12, len(timestamps))
            gpu_temp_data = np.random.normal(75, 5, len(timestamps))
            
            fig.add_trace(
                go.Scatter(x=timestamps, y=cpu_data, name="CPU", line=dict(color="blue")),
                row=1, col=1
            )
            
            fig.add_trace(
                go.Scatter(x=timestamps, y=memory_data, name="ãƒ¡ãƒ¢ãƒª", line=dict(color="green")),
                row=1, col=2
            )
            
            fig.add_trace(
                go.Scatter(x=timestamps, y=gpu_memory_data, name="GPU ãƒ¡ãƒ¢ãƒª", line=dict(color="red")),
                row=2, col=1
            )
            
            fig.add_trace(
                go.Scatter(x=timestamps, y=gpu_temp_data, name="GPU æ¸©åº¦", line=dict(color="orange")),
                row=2, col=2
            )
        
        # è­¦å‘Šãƒ©ã‚¤ãƒ³è¿½åŠ 
        fig.add_hline(y=80, line_dash="dash", line_color="orange", row=1, col=1, annotation_text="è­¦å‘Š")
        fig.add_hline(y=90, line_dash="dash", line_color="red", row=1, col=1, annotation_text="å±é™º")
        
        fig.add_hline(y=80, line_dash="dash", line_color="orange", row=1, col=2, annotation_text="è­¦å‘Š")
        fig.add_hline(y=90, line_dash="dash", line_color="red", row=1, col=2, annotation_text="å±é™º")
        
        fig.add_hline(y=85, line_dash="dash", line_color="orange", row=2, col=1, annotation_text="è­¦å‘Š")
        fig.add_hline(y=95, line_dash="dash", line_color="red", row=2, col=1, annotation_text="å±é™º")
        
        fig.add_hline(y=80, line_dash="dash", line_color="orange", row=2, col=2, annotation_text="è­¦å‘Š")
        fig.add_hline(y=90, line_dash="dash", line_color="red", row=2, col=2, annotation_text="å±é™º")
        
        fig.update_layout(height=600, showlegend=False)
        fig.update_xaxes(title_text="æ™‚åˆ»")
        fig.update_yaxes(title_text="ä½¿ç”¨ç‡ (%)", row=1, col=1)
        fig.update_yaxes(title_text="ä½¿ç”¨ç‡ (%)", row=1, col=2)
        fig.update_yaxes(title_text="ä½¿ç”¨ç‡ (%)", row=2, col=1)
        fig.update_yaxes(title_text="æ¸©åº¦ (Â°C)", row=2, col=2)
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ãƒœã‚¿ãƒ³
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ”„ ãƒ‡ãƒ¼ã‚¿æ›´æ–°"):
                self._update_system_stats_history()
                st.rerun()
        
        with col2:
            if st.button("ğŸ“Š çµ±è¨ˆãƒªã‚»ãƒƒãƒˆ"):
                st.session_state.system_stats_history = []
                st.success("çµ±è¨ˆå±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
                st.rerun()
        
        with col3:
            if st.button("ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"):
                self._export_performance_data()
    
    def _render_memory_search(self):
        """è¨˜æ†¶æ¤œç´¢ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
        
        st.markdown("### ğŸ” è¨˜æ†¶æ¤œç´¢")
        
        # æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ 
        with st.form("memory_search_form"):
            search_query = st.text_input("æ¤œç´¢ã‚¯ã‚¨ãƒª", placeholder="æ¤œç´¢ã—ãŸã„å†…å®¹ã‚’å…¥åŠ›...")
            
            col1, col2 = st.columns(2)
            with col1:
                max_results = st.slider("æœ€å¤§çµæœæ•°", 1, 20, 5)
            with col2:
                similarity_threshold = st.slider("é¡ä¼¼åº¦é–¾å€¤", 0.0, 1.0, 0.7, 0.1)
            
            search_button = st.form_submit_button("ğŸ” æ¤œç´¢")
        
        if search_button and search_query.strip():
            with st.spinner("è¨˜æ†¶ã‚’æ¤œç´¢ä¸­..."):
                results = self._search_memories(search_query, max_results, similarity_threshold)
                
                if results["total_found"] > 0:
                    st.success(f"{results['total_found']} ä»¶ã®è¨˜æ†¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                    
                    for i, result in enumerate(results["results"], 1):
                        with st.expander(f"è¨˜æ†¶ {i}: {result.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')}"):
                            st.markdown(f"**å†…å®¹:** {result.get('content', '')}")
                            st.markdown(f"**é¡ä¼¼åº¦:** {result.get('similarity', 0):.3f}")
                            st.markdown(f"**ä½œæˆæ—¥:** {result.get('created_at', 'N/A')}")
                else:
                    st.info("è©²å½“ã™ã‚‹è¨˜æ†¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    def _search_memories(self, query: str, max_results: int, similarity_threshold: float) -> Dict[str, Any]:
        """è¨˜æ†¶æ¤œç´¢å®Ÿè¡Œ"""
        
        try:
            # API å‘¼ã³å‡ºã—ï¼ˆç°¡ç•¥åŒ–ï¼‰
            url = f"{self.api_base_url}/v1/memory/search"
            
            payload = {
                "query": query,
                "max_results": max_results,
                "similarity_threshold": similarity_threshold
            }
            
            response = requests.post(url, json=payload, timeout=10)
            
            if response.status_code == 200:
                return response.json()
            else:
                return {"results": [], "total_found": 0}
                
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿
            return {
                "results": [
                    {
                        "title": "ã‚µãƒ³ãƒ—ãƒ«è¨˜æ†¶",
                        "content": f"'{query}' ã«é–¢é€£ã™ã‚‹è¨˜æ†¶å†…å®¹ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚",
                        "similarity": 0.85,
                        "created_at": datetime.now().isoformat()
                    }
                ],
                "total_found": 1
            }
    
    def _render_admin_panel(self):
        """ç®¡ç†ãƒ‘ãƒãƒ« - Pydantic + Streamlit è¨­å®šç®¡ç†çµ±åˆ"""
        
        st.markdown("### âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†")
        
        # è¨­å®šç®¡ç†UIçµ±åˆ
        try:
            from .settings_manager import get_settings_ui
            settings_ui = get_settings_ui()
            settings_ui.render_settings_panel()
            
        except ImportError as e:
            logger.warning(f"è¨­å®šç®¡ç†UIã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬ç®¡ç†ãƒ‘ãƒãƒ«
            self._render_basic_admin_panel()
    
    def _render_basic_admin_panel(self):
        """åŸºæœ¬ç®¡ç†ãƒ‘ãƒãƒ«ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        
        # ã‚·ã‚¹ãƒ†ãƒ åˆ¶å¾¡
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ åˆ¶å¾¡")
            
            if st.button("ğŸ”„ ã‚·ã‚¹ãƒ†ãƒ å†èµ·å‹•"):
                st.warning("ã‚·ã‚¹ãƒ†ãƒ å†èµ·å‹•æ©Ÿèƒ½ã¯å®Ÿè£…ä¸­ã§ã™")
            
            if st.button("ğŸ§¹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"):
                st.success("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
            
            if st.button("ğŸ’¾ è¨­å®šä¿å­˜"):
                self._save_settings()
        
        with col2:
            st.markdown("#### ğŸ“Š çµ±è¨ˆæƒ…å ±")
            
            stats = {
                "ç·ä¼šè©±æ•°": len(st.session_state.messages),
                "ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“": str(datetime.now() - st.session_state.get("session_start_time", datetime.now())),
                "API å‘¼ã³å‡ºã—æ•°": st.session_state.get("api_calls", 0),
                "ã‚¨ãƒ©ãƒ¼æ•°": st.session_state.get("error_count", 0)
            }
            
            for key, value in stats.items():
                st.metric(key, value)
        
        # ãƒ­ã‚°è¡¨ç¤º
        st.markdown("#### ğŸ“ ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°")
        
        if st.button("ãƒ­ã‚°æ›´æ–°"):
            st.text_area(
                "æœ€æ–°ãƒ­ã‚°",
                value="[INFO] ã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸å‹•ä½œä¸­\n[INFO] GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: 70%\n[INFO] æ¨è«–å®Œäº†: 1.2ç§’",
                height=150
            )
    
    def _save_session(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜ - Streamlit ã®æ—¢å­˜ã‚»ãƒƒã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã«ã‚ˆã‚‹ æ°¸ç¶šåŒ–"""
        try:
            session_data = {
                "session_id": st.session_state.current_session_id,
                "messages": st.session_state.messages,
                "settings": st.session_state.settings,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "message_count": len(st.session_state.messages),
                "start_time": st.session_state.get("session_start_time", datetime.now()).isoformat()
            }
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯æ°¸ç¶šåŒ–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ä¿å­˜ï¼‰
            if "saved_sessions" not in st.session_state:
                st.session_state.saved_sessions = {}
            
            st.session_state.saved_sessions[st.session_state.current_session_id] = session_data
            
            st.success(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ {st.session_state.current_session_id[:8]}... ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
            
        except Exception as e:
            st.error(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _save_settings(self):
        """è¨­å®šä¿å­˜"""
        try:
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            st.success("è¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸ")
            
        except Exception as e:
            st.error(f"è¨­å®šä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _export_performance_data(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        
        try:
            if st.session_state.system_stats_history:
                # DataFrame ã«å¤‰æ›
                df = pd.DataFrame(st.session_state.system_stats_history)
                
                # CSV å½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                csv_data = df.to_csv(index=False)
                
                st.download_button(
                    label="ğŸ“¥ CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv_data,
                    file_name=f"performance_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )
                
                st.success("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
            else:
                st.warning("ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                
        except Exception as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def _render_realtime_chat_status(self):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒãƒ£ãƒƒãƒˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º"""
        
        # Ollamaæ¥ç¶šçŠ¶æ³ãƒã‚§ãƒƒã‚¯
        ollama_status = self._check_ollama_connection()
        
        # æ¥ç¶šçŠ¶æ³è¡¨ç¤º
        if ollama_status["connected"]:
            status_color = "#d4edda"
            status_icon = "âœ…"
            status_text = f"Ollamaæ¥ç¶šä¸­ ({ollama_status['model']})"
        else:
            status_color = "#f8d7da"
            status_icon = "âŒ"
            status_text = f"Ollamaæœªæ¥ç¶š - {ollama_status['error']}"
        
        st.markdown(f"""
        <div style="display: flex; align-items: center; padding: 8px; background-color: {status_color}; border-radius: 5px; margin: 5px 0; font-size: 0.9rem;">
            <div style="margin-right: 8px;">{status_icon}</div>
            <div>{status_text}</div>
        </div>
        """, unsafe_allow_html=True)
        
        # å‡¦ç†ä¸­ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼
        if st.session_state.get("processing", False):
            st.markdown("""
            <div style="display: flex; align-items: center; padding: 10px; background-color: #fff3cd; border-radius: 5px; margin: 10px 0;">
                <div style="margin-right: 10px;">â³</div>
                <div>AI ãŒå¿œç­”ã‚’ç”Ÿæˆä¸­...</div>
            </div>
            """, unsafe_allow_html=True)
        
        # æœ€å¾Œã®å¿œç­”æ™‚é–“ã¨ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
        if st.session_state.messages:
            last_message = st.session_state.messages[-1]
            if last_message.get("role") == "assistant":
                processing_time = last_message.get("processing_time", 0)
                model_used = last_message.get("model_used", "primary")
                confidence = last_message.get("confidence_score", 0)
                
                model_text = " (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)" if model_used == "fallback" else ""
                
                st.markdown(f"""
                <div style="text-align: right; color: #6c757d; font-size: 0.8rem; margin: 5px 0;">
                    æœ€å¾Œã®å¿œç­”æ™‚é–“: {processing_time:.2f}ç§’{model_text} | ä¿¡é ¼åº¦: {confidence:.2f}
                </div>
                """, unsafe_allow_html=True)
    
    def _check_ollama_connection(self) -> Dict[str, Any]:
        """Ollamaæ¥ç¶šçŠ¶æ³ãƒã‚§ãƒƒã‚¯"""
        
        try:
            import ollama
            
            # ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã§æ¥ç¶šãƒ†ã‚¹ãƒˆ
            model = st.session_state.settings["model"]
            
            # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—ã§æ¥ç¶šç¢ºèª
            models = ollama.list()
            
            # æŒ‡å®šãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
            available_models = [m["name"] for m in models["models"]]
            
            if model in available_models:
                return {
                    "connected": True,
                    "model": model,
                    "available_models": available_models
                }
            else:
                return {
                    "connected": False,
                    "error": f"ãƒ¢ãƒ‡ãƒ« '{model}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                    "available_models": available_models
                }
                
        except ImportError:
            return {
                "connected": False,
                "error": "ollama ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
            }
        except Exception as e:
            return {
                "connected": False,
                "error": f"æ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)}"
            }
    
    def _auto_refresh(self):
        """è‡ªå‹•ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ - Streamlit ã®æ—¢å­˜å¿œç­”æ€§æ©Ÿèƒ½ã«ã‚ˆã‚‹ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°"""
        
        current_time = datetime.now()
        time_diff = (current_time - st.session_state.last_refresh).total_seconds()
        
        if time_diff >= st.session_state.settings["refresh_interval"]:
            st.session_state.last_refresh = current_time
            
            # ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆã‚’å±¥æ­´ã«è¿½åŠ 
            self._update_system_stats_history()
            
            # å¿…è¦ã«å¿œã˜ã¦çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
            st.rerun()
    
    def _update_system_stats_history(self):
        """ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆå±¥æ­´æ›´æ–° - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ãƒ‡ãƒ¼ã‚¿è“„ç©"""
        
        try:
            current_stats = {
                "timestamp": datetime.now(),
                "cpu_percent": self._get_system_stats_sync().get("cpu_percent", 0),
                "memory_percent": self._get_system_stats_sync().get("memory_percent", 0),
                "gpu_memory_percent": self._get_gpu_stats_sync().get("memory_percent", 0),
                "gpu_temperature": self._get_gpu_stats_sync().get("temperature", 0),
                "gpu_utilization": self._get_gpu_stats_sync().get("utilization_percent", 0)
            }
            
            # å±¥æ­´ã«è¿½åŠ ï¼ˆæœ€å¤§100ä»¶ã¾ã§ä¿æŒï¼‰
            st.session_state.system_stats_history.append(current_stats)
            
            if len(st.session_state.system_stats_history) > 100:
                st.session_state.system_stats_history.pop(0)
                
        except Exception as e:
            logger.warning(f"ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆå±¥æ­´æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _render_realtime_progress_indicator(self):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ãƒ»VRAMè¡¨ç¤º - Streamlit ã®æ—¢å­˜å¯è¦–åŒ–æ©Ÿèƒ½ã«ã‚ˆã‚‹ é€²æ—ãƒ»VRAM è¡¨ç¤º"""
        
        # é€²æ—ãƒãƒ¼è¡¨ç¤ºã‚¨ãƒªã‚¢
        progress_container = st.container()
        
        with progress_container:
            col1, col2, col3 = st.columns(3)
            
            # GPU VRAM ä½¿ç”¨ç‡
            with col1:
                gpu_stats = self._get_gpu_stats_sync()
                vram_percent = gpu_stats.get("memory_percent", 0)
                
                st.markdown("**ğŸ® GPU VRAM**")
                vram_progress = st.progress(vram_percent / 100)
                
                # è‰²åˆ†ã‘è¡¨ç¤º
                if vram_percent >= 90:
                    st.error(f"VRAM: {vram_percent:.1f}% (å±é™º)")
                elif vram_percent >= 75:
                    st.warning(f"VRAM: {vram_percent:.1f}% (æ³¨æ„)")
                else:
                    st.success(f"VRAM: {vram_percent:.1f}% (æ­£å¸¸)")
            
            # CPU ä½¿ç”¨ç‡
            with col2:
                system_stats = self._get_system_stats_sync()
                cpu_percent = system_stats.get("cpu_percent", 0)
                
                st.markdown("**ğŸ’» CPU**")
                cpu_progress = st.progress(cpu_percent / 100)
                
                if cpu_percent >= 90:
                    st.error(f"CPU: {cpu_percent:.1f}% (é«˜è² è·)")
                elif cpu_percent >= 70:
                    st.warning(f"CPU: {cpu_percent:.1f}% (ä¸­è² è·)")
                else:
                    st.success(f"CPU: {cpu_percent:.1f}% (æ­£å¸¸)")
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
            with col3:
                memory_percent = system_stats.get("memory_percent", 0)
                
                st.markdown("**ğŸ§  ãƒ¡ãƒ¢ãƒª**")
                memory_progress = st.progress(memory_percent / 100)
                
                if memory_percent >= 90:
                    st.error(f"ãƒ¡ãƒ¢ãƒª: {memory_percent:.1f}% (ä¸è¶³)")
                elif memory_percent >= 75:
                    st.warning(f"ãƒ¡ãƒ¢ãƒª: {memory_percent:.1f}% (æ³¨æ„)")
                else:
                    st.success(f"ãƒ¡ãƒ¢ãƒª: {memory_percent:.1f}% (æ­£å¸¸)")
        
        return progress_container
    
    def _generate_user_friendly_error_message(self, error_info) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ"""
        
        try:
            error_type = error_info.error_type.value
            severity = error_info.severity.value
            
            base_messages = {
                "connection_error": "ğŸ”Œ AI ãƒ¢ãƒ‡ãƒ«ã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
                "model_error": "ğŸ¤– AI ãƒ¢ãƒ‡ãƒ«ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                "timeout_error": "â±ï¸ AI ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚",
                "memory_error": "ğŸ’¾ ãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚å‡¦ç†ã‚’å®Œäº†ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
                "unknown_error": "â“ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
            }
            
            base_message = base_messages.get(error_type, base_messages["unknown_error"])
            
            # å¾©æ—§ææ¡ˆã‚’è¿½åŠ 
            suggestions = error_info.recovery_suggestions
            if suggestions:
                suggestion_text = "\n\nã€ğŸ’¡ å¯¾å‡¦æ–¹æ³•ã€‘\n" + "\n".join(f"â€¢ {s}" for s in suggestions[:3])
                return base_message + suggestion_text
            
        except Exception as e:
            logger.warning(f"ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å¯¾å‡¦æ–¹æ³•
        default_suggestions = [
            "Ollama ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„",
            "ãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ (ollama list)",
            "ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„",
            "åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ã¿ã¦ãã ã•ã„"
        ]
        
        suggestion_text = "\n\nã€ğŸ’¡ å¯¾å‡¦æ–¹æ³•ã€‘\n" + "\n".join(f"â€¢ {s}" for s in default_suggestions)
        return "ğŸ”Œ AI ãƒ¢ãƒ‡ãƒ«ã¸ã®æ¥ç¶šã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚" + suggestion_text


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # Streamlit UI ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    ui = StreamlitUI()
    
    # UI å®Ÿè¡Œ
    ui.run()


if __name__ == "__main__":
    main()