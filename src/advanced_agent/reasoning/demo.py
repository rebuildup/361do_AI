"""
LangChain + Ollama åŸºæœ¬æŽ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³çµ±åˆãƒ‡ãƒ¢
PromptTemplate ã¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯çµ±åˆãƒ†ã‚¹ãƒˆ
"""

import asyncio
import sys
from pathlib import Path
import time

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from src.advanced_agent.reasoning.base_engine import (
    BasicReasoningEngine, MemoryAwareReasoningEngine, ReasoningRequest, 
    ReasoningContext, create_reasoning_engine, quick_reasoning
)
from src.advanced_agent.reasoning.prompt_manager import (
    PromptManager, PromptConfig, PromptType, PromptExample, get_prompt_manager
)
from src.advanced_agent.reasoning.callbacks import (
    PerformanceCallbackHandler, create_performance_callback, get_performance_statistics
)
from src.advanced_agent.inference.ollama_client import create_ollama_client
from src.advanced_agent.core.config import get_config
from src.advanced_agent.core.logger import setup_logging, get_logger


async def demo_basic_reasoning_engine():
    """åŸºæœ¬æŽ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ãƒ‡ãƒ¢"""
    print("\n" + "="*70)
    print("BASIC REASONING ENGINE DEMO")
    print("="*70)
    
    logger = get_logger()
    
    try:
        # Ollama ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
        ollama_client = await create_ollama_client()
        
        # æŽ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ
        engine = await create_reasoning_engine(ollama_client, "basic")
        
        print("\n1. General Reasoning Test")
        print("-" * 35)
        
        # ä¸€èˆ¬çš„ãªæŽ¨è«–ãƒ†ã‚¹ãƒˆ
        context = ReasoningContext(
            session_id="demo_session_1",
            system_context="AIæŠ€è¡“ã®ç™ºå±•ã«ã¤ã„ã¦è­°è«–ä¸­",
            domain_context="äººå·¥çŸ¥èƒ½ã¨æ©Ÿæ¢°å­¦ç¿’",
            constraints=["å®¢è¦³çš„ãªè¦–ç‚¹ã‚’ä¿ã¤", "å…·ä½“ä¾‹ã‚’å«ã‚ã‚‹"]
        )
        
        request = ReasoningRequest(
            prompt="äººå·¥çŸ¥èƒ½ãŒç¤¾ä¼šã«ä¸Žãˆã‚‹å½±éŸ¿ã«ã¤ã„ã¦åˆ†æžã—ã¦ãã ã•ã„",
            context=context,
            reasoning_type="general",
            temperature=0.1
        )
        
        print(f"Question: {request.prompt}")
        print(f"Reasoning Type: {request.reasoning_type}")
        print("Processing...")
        
        result = await engine.reason(request)
        
        print(f"\nðŸ“Š Results:")
        print(f"   Processing Time: {result.processing_time:.2f}s")
        print(f"   Confidence Score: {result.confidence_score:.2f}")
        print(f"   Model Used: {result.model_used}")
        print(f"   Tokens Used: {result.tokens_used}")
        print(f"   Memory Usage: {result.memory_usage_mb:.2f}MB")
        print(f"\nðŸ“ Answer:")
        print(f"   {result.final_answer[:300]}...")
        
        print("\n2. Analytical Reasoning Test")
        print("-" * 35)
        
        # åˆ†æžçš„æŽ¨è«–ãƒ†ã‚¹ãƒˆ
        analytical_context = ReasoningContext(
            session_id="demo_session_2",
            domain_context="ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°æ•™è‚²",
            constraints=["åˆå¿ƒè€…å‘ã‘ã«èª¬æ˜Ž", "å®Ÿç”¨çš„ãªè¦³ç‚¹ã‚’é‡è¦–"]
        )
        
        analytical_request = ReasoningRequest(
            prompt="Pythonãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªžã®å­¦ç¿’æ–¹æ³•ã‚’ä½“ç³»çš„ã«åˆ†æžã—ã¦ãã ã•ã„",
            context=analytical_context,
            reasoning_type="analytical"
        )
        
        print(f"Question: {analytical_request.prompt}")
        print("Processing...")
        
        analytical_result = await engine.reason(analytical_request)
        
        print(f"\nðŸ“Š Results:")
        print(f"   Processing Time: {analytical_result.processing_time:.2f}s")
        print(f"   Confidence Score: {analytical_result.confidence_score:.2f}")
        print(f"\nðŸ“ Analysis:")
        print(f"   {analytical_result.final_answer[:300]}...")
        
        print("\n3. Creative Reasoning Test")
        print("-" * 30)
        
        # å‰µé€ çš„æŽ¨è«–ãƒ†ã‚¹ãƒˆ
        creative_context = ReasoningContext(
            session_id="demo_session_3",
            domain_context="ç’°å¢ƒå•é¡Œè§£æ±º",
            constraints=["å®Ÿç¾å¯èƒ½æ€§ã‚’è€ƒæ…®", "é©æ–°çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"]
        )
        
        creative_request = ReasoningRequest(
            prompt="æ°—å€™å¤‰å‹•å•é¡Œã‚’è§£æ±ºã™ã‚‹é©æ–°çš„ãªãƒ†ã‚¯ãƒŽãƒ­ã‚¸ãƒ¼ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆã—ã¦ãã ã•ã„",
            context=creative_context,
            reasoning_type="creative"
        )
        
        print(f"Question: {creative_request.prompt}")
        print("Processing...")
        
        creative_result = await engine.reason(creative_request)
        
        print(f"\nðŸ“Š Results:")
        print(f"   Processing Time: {creative_result.processing_time:.2f}s")
        print(f"   Confidence Score: {creative_result.confidence_score:.2f}")
        print(f"\nðŸ’¡ Creative Ideas:")
        print(f"   {creative_result.final_answer[:300]}...")
        
        await ollama_client.shutdown()
        return True
        
    except Exception as e:
        print(f"âŒ Basic reasoning engine demo failed: {e}")
        return False


async def demo_prompt_management():
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ãƒ‡ãƒ¢"""
    print("\n" + "="*70)
    print("PROMPT MANAGEMENT DEMO")
    print("="*70)
    
    logger = get_logger()
    
    try:
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒžãƒãƒ¼ã‚¸ãƒ£ãƒ¼å–å¾—
        manager = get_prompt_manager()
        
        print("\n1. Available Prompts")
        print("-" * 25)
        
        prompts = manager.list_prompts()
        print(f"Total prompts: {len(prompts)}")
        
        for prompt_type in PromptType:
            type_prompts = manager.list_prompts(prompt_type)
            if type_prompts:
                print(f"   {prompt_type.value}: {len(type_prompts)} prompts")
                for prompt_name in type_prompts[:2]:  # æœ€åˆã®2ã¤ã ã‘è¡¨ç¤º
                    print(f"     - {prompt_name}")
        
        print("\n2. Prompt Template Usage")
        print("-" * 30)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½¿ç”¨ä¾‹
        reasoning_template = manager.get_prompt_template("general_reasoning")
        if reasoning_template:
            formatted_prompt = reasoning_template.format(
                context="æ©Ÿæ¢°å­¦ç¿’ã®åŸºç¤Žã«ã¤ã„ã¦å­¦ç¿’ä¸­",
                question="ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ä»•çµ„ã¿ã‚’èª¬æ˜Žã—ã¦ãã ã•ã„"
            )
            
            print("Template: general_reasoning")
            print("Formatted prompt:")
            print(f"   {formatted_prompt[:200]}...")
        
        print("\n3. Chat Prompt Creation")
        print("-" * 30)
        
        # ãƒãƒ£ãƒƒãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        chat_prompt = manager.create_chat_prompt(
            "analytical_thinking",
            subject="ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹",
            context="ãƒ“ã‚¸ãƒã‚¹åˆ†æžã§ã®æ´»ç”¨",
            aspects="åŠ¹çŽ‡æ€§ã€æ­£ç¢ºæ€§ã€å®Ÿç”¨æ€§"
        )
        
        print("Chat prompt created successfully")
        print(f"Messages count: {len(chat_prompt.messages)}")
        
        print("\n4. Prompt Optimization")
        print("-" * 25)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
        test_prompts = [
            ("ä½•ã‹ã„ã„æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„", "specificity"),
            ("ã‚ã„ã¾ã„ãªèª¬æ˜Žã‚’ã—ã¦ãã ã•ã„", "clarity"),
            ("ã„ãã¤ã‹ã®ä¾‹ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚è©³ç´°ã«èª¬æ˜Žã—ã¦ãã ã•ã„ã€‚", "structure")
        ]
        
        for original_prompt, optimization_type in test_prompts:
            result = manager.optimize_prompt(original_prompt, optimization_type)
            
            print(f"\nOptimization: {optimization_type}")
            print(f"   Original: {result.original_prompt}")
            print(f"   Optimized: {result.optimized_prompt}")
            print(f"   Score: {result.improvement_score:.2f}")
            print(f"   Reasoning: {result.reasoning}")
        
        print("\n5. Few-Shot Prompt Creation")
        print("-" * 35)
        
        # Few-shot ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        examples = [
            PromptExample(
                input="æ—¥æœ¬ã®é¦–éƒ½ã¯ï¼Ÿ",
                output="æ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã™ã€‚"
            ),
            PromptExample(
                input="ãƒ•ãƒ©ãƒ³ã‚¹ã®é¦–éƒ½ã¯ï¼Ÿ",
                output="ãƒ•ãƒ©ãƒ³ã‚¹ã®é¦–éƒ½ã¯ãƒ‘ãƒªã§ã™ã€‚"
            )
        ]
        
        few_shot_config = manager.create_few_shot_prompt(
            name="geography_qa",
            base_template="åœ°ç†ã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n\nè³ªå•: {question}\n\nå›žç­”:",
            examples=examples,
            input_variables=["question"]
        )
        
        print(f"Few-shot prompt created: {few_shot_config.name}")
        print(f"Examples count: {len(few_shot_config.examples)}")
        
        # çµ±è¨ˆæƒ…å ±
        stats = manager.get_prompt_statistics()
        print(f"\nðŸ“Š Prompt Statistics:")
        print(f"   Total prompts: {stats['total_prompts']}")
        print(f"   Total examples: {stats['total_examples']}")
        print(f"   Average examples per prompt: {stats['average_examples_per_prompt']:.1f}")
        print(f"   Prompts by type: {stats['prompts_by_type']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Prompt management demo failed: {e}")
        return False


async def demo_performance_callbacks():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒ¢"""
    print("\n" + "="*70)
    print("PERFORMANCE CALLBACKS DEMO")
    print("="*70)
    
    logger = get_logger()
    
    try:
        print("\n1. Performance Callback Creation")
        print("-" * 40)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½œæˆ
        callback = create_performance_callback("demo_request_001", enable_gpu=True)
        
        print(f"Callback created for request: {callback.request_id}")
        print(f"GPU monitoring enabled: {callback.enable_gpu_monitoring}")
        
        print("\n2. Simulated LLM Processing")
        print("-" * 35)
        
        # æ¨¡æ“¬çš„ãªLLMå‡¦ç†
        print("Starting LLM simulation...")
        
        # LLMé–‹å§‹
        callback.on_llm_start(
            {"name": "deepseek-r1:7b"},
            ["ã“ã‚Œã¯æŽ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã§ã™ã€‚è¤‡é›‘ãªè³ªå•ã«å¯¾ã—ã¦è©³ç´°ãªå›žç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"]
        )
        
        # å‡¦ç†æ™‚é–“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        print("Processing tokens...")
        for i in range(100):
            callback.on_llm_new_token(f"token_{i}")
            await asyncio.sleep(0.005)  # 5msé–“éš”
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        from langchain_core.agents import AgentAction
        action = AgentAction(
            tool="reasoning_tool",
            tool_input="analyze the problem",
            log="Thinking about the problem..."
        )
        callback.on_agent_action(action)
        
        # LLMçµ‚äº†
        from langchain_core.outputs import LLMResult, Generation
        result = LLMResult(generations=[[Generation(
            text="ã“ã‚Œã¯è©³ç´°ãªå›žç­”ã§ã™ã€‚æŽ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã‚’çµŒã¦ç”Ÿæˆã•ã‚ŒãŸçµæžœã¨ãªã‚Šã¾ã™ã€‚"
        )]])
        callback.on_llm_end(result)
        
        print("LLM processing completed")
        
        print("\n3. Performance Metrics")
        print("-" * 25)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
        metrics = callback.get_metrics()
        
        print(f"ðŸ“Š Performance Metrics:")
        print(f"   Request ID: {metrics.request_id}")
        print(f"   Model: {metrics.model_name}")
        print(f"   Duration: {metrics.duration:.3f}s")
        print(f"   Input Tokens: {metrics.input_tokens}")
        print(f"   Output Tokens: {metrics.output_tokens}")
        print(f"   Total Tokens: {metrics.total_tokens}")
        print(f"   Memory Delta: {metrics.memory_delta_mb:.2f}MB")
        print(f"   Memory Peak: {metrics.memory_peak_mb:.2f}MB")
        print(f"   Reasoning Steps: {metrics.reasoning_steps}")
        print(f"   Tool Calls: {metrics.tool_calls}")
        print(f"   Errors: {metrics.errors}")
        
        if metrics.gpu_utilization_avg > 0:
            print(f"   GPU Utilization: {metrics.gpu_utilization_avg:.1f}%")
            print(f"   GPU Memory Peak: {metrics.gpu_memory_peak_mb:.2f}MB")
        
        print("\n4. Event Timeline")
        print("-" * 20)
        
        events = callback.get_events()
        print(f"Total events: {len(events)}")
        
        for event in events:
            elapsed = event.timestamp - callback.metrics.start_time
            print(f"   {elapsed:.3f}s: {event.event_type}")
        
        print("\n5. Performance Summary")
        print("-" * 25)
        
        summary = callback.get_summary()
        print(f"Summary: {summary}")
        
        print("\n6. Aggregated Statistics")
        print("-" * 30)
        
        # é›†ç´„çµ±è¨ˆã«è¿½åŠ 
        from src.advanced_agent.reasoning.callbacks import get_aggregated_handler
        aggregated = get_aggregated_handler()
        aggregated.add_performance_record(metrics)
        
        stats = get_performance_statistics()
        print(f"ðŸ“ˆ Aggregated Statistics:")
        print(f"   Total Requests: {stats['total_requests']}")
        print(f"   Average Duration: {stats['average_duration']:.3f}s")
        print(f"   Average Tokens: {stats['average_tokens']:.0f}")
        print(f"   Error Rate: {stats['error_rate']:.2%}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Performance callbacks demo failed: {e}")
        return False


async def demo_integrated_reasoning():
    """çµ±åˆæŽ¨è«–ãƒ‡ãƒ¢"""
    print("\n" + "="*70)
    print("INTEGRATED REASONING DEMO")
    print("="*70)
    
    logger = get_logger()
    
    try:
        print("\n1. Quick Reasoning Test")
        print("-" * 30)
        
        # ç°¡æ˜“æŽ¨è«–ãƒ†ã‚¹ãƒˆ
        quick_questions = [
            ("é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã®åŸºæœ¬åŽŸç†ã‚’èª¬æ˜Žã—ã¦ãã ã•ã„", "factual"),
            ("æ–°ã—ã„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªžã‚’å­¦ã¶åŠ¹çŽ‡çš„ãªæ–¹æ³•ã‚’åˆ†æžã—ã¦ãã ã•ã„", "analytical"),
            ("æŒç¶šå¯èƒ½ãªéƒ½å¸‚è¨­è¨ˆã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ææ¡ˆã—ã¦ãã ã•ã„", "creative")
        ]
        
        for question, reasoning_type in quick_questions:
            print(f"\nQuestion ({reasoning_type}): {question}")
            print("Processing...")
            
            start_time = time.time()
            answer = await quick_reasoning(question, reasoning_type)
            processing_time = time.time() - start_time
            
            print(f"Time: {processing_time:.2f}s")
            print(f"Answer: {answer[:150]}...")
        
        print("\n2. Complex Reasoning with Context")
        print("-" * 40)
        
        # è¤‡é›‘ãªæŽ¨è«–ãƒ†ã‚¹ãƒˆ
        ollama_client = await create_ollama_client()
        engine = await create_reasoning_engine(ollama_client, "basic")
        
        # ä¼šè©±å±¥æ­´ã‚’å«ã‚€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        complex_context = ReasoningContext(
            session_id="complex_demo",
            system_context="æŠ€è¡“ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³",
            domain_context="ä¼æ¥­ã®ãƒ‡ã‚¸ã‚¿ãƒ«å¤‰é©",
            conversation_history=[
                {"role": "user", "content": "æˆ‘ãŒç¤¾ã¯AIå°Žå…¥ã‚’æ¤œè¨Žã—ã¦ã„ã¾ã™"},
                {"role": "assistant", "content": "AIå°Žå…¥ã«ã¯æ®µéšŽçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒé‡è¦ã§ã™"},
                {"role": "user", "content": "å…·ä½“çš„ã«ã¯ã©ã®ã‚ˆã†ãªæ‰‹é †ã§é€²ã‚ã‚‹ã¹ãã§ã—ã‚‡ã†ã‹"}
            ],
            constraints=[
                "å®Ÿç”¨çš„ã§å®Ÿç¾å¯èƒ½ãªææ¡ˆ",
                "ã‚³ã‚¹ãƒˆåŠ¹çŽ‡ã‚’è€ƒæ…®",
                "æ®µéšŽçš„ãªå®Ÿè£…ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"
            ],
            metadata={
                "industry": "manufacturing",
                "company_size": "medium",
                "urgency": "high"
            }
        )
        
        complex_request = ReasoningRequest(
            prompt="è£½é€ æ¥­ã®ä¸­è¦æ¨¡ä¼æ¥­ã«ãŠã‘ã‚‹AIå°Žå…¥æˆ¦ç•¥ã‚’è©³ç´°ã«åˆ†æžã—ã€å…·ä½“çš„ãªå®Ÿè£…è¨ˆç”»ã‚’ææ¡ˆã—ã¦ãã ã•ã„",
            context=complex_context,
            reasoning_type="analytical",
            use_memory=True
        )
        
        print("Processing complex reasoning request...")
        print(f"Context includes: {len(complex_context.conversation_history)} conversation turns")
        print(f"Constraints: {len(complex_context.constraints)} items")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãã§å®Ÿè¡Œ
        callback = create_performance_callback("complex_demo_001")
        
        complex_result = await engine.reason(complex_request)
        
        print(f"\nðŸ“Š Complex Reasoning Results:")
        print(f"   Processing Time: {complex_result.processing_time:.2f}s")
        print(f"   Confidence Score: {complex_result.confidence_score:.2f}")
        print(f"   Reasoning Steps: {len(complex_result.reasoning_steps)}")
        print(f"   Context Used: {complex_result.context_used.session_id}")
        
        print(f"\nðŸ“ Strategic Analysis:")
        print(f"   {complex_result.final_answer[:400]}...")
        
        await ollama_client.shutdown()
        return True
        
    except Exception as e:
        print(f"âŒ Integrated reasoning demo failed: {e}")
        return False


async def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    print("LANGCHAIN + OLLAMA BASIC REASONING ENGINE DEMO")
    print("=" * 70)
    print("RTX 4050 6GB VRAM æœ€é©åŒ–æŽ¨è«–ã‚·ã‚¹ãƒ†ãƒ ")
    print("PromptTemplate + Callbacks + Performance Monitoring")
    
    # ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    logger = setup_logging("INFO")
    
    results = []
    
    try:
        # 1. åŸºæœ¬æŽ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ãƒ‡ãƒ¢
        reasoning_result = await demo_basic_reasoning_engine()
        results.append(("Basic Reasoning Engine", reasoning_result))
        
        # 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ãƒ‡ãƒ¢
        prompt_result = await demo_prompt_management()
        results.append(("Prompt Management", prompt_result))
        
        # 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒ¢
        callback_result = await demo_performance_callbacks()
        results.append(("Performance Callbacks", callback_result))
        
        # 4. çµ±åˆæŽ¨è«–ãƒ‡ãƒ¢
        integrated_result = await demo_integrated_reasoning()
        results.append(("Integrated Reasoning", integrated_result))
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Demo interrupted by user")
        return
    except Exception as e:
        print(f"\n\nâŒ Demo failed with error: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # çµæžœã‚µãƒžãƒªãƒ¼
    print("\n" + "="*70)
    print("DEMO RESULTS SUMMARY")
    print("="*70)
    
    success_count = sum(1 for _, result in results if result)
    total_count = len(results)
    
    print(f"\nðŸ“Š Overall Results: {success_count}/{total_count} demos successful")
    
    for demo_name, result in results:
        status = "âœ… SUCCESS" if result else "âŒ FAILED"
        print(f"   {status} {demo_name}")
    
    if success_count == total_count:
        print(f"\nðŸŽ‰ All demos completed successfully!")
        print(f"   â€¢ Basic reasoning engine working")
        print(f"   â€¢ Prompt management system working")
        print(f"   â€¢ Performance callbacks working")
        print(f"   â€¢ Integrated reasoning working")
    else:
        print(f"\nâš ï¸  {total_count - success_count} demo(s) failed")
        print(f"   Please check the error messages above")
    
    print(f"\nðŸ’¡ Next Steps:")
    print(f"   â€¢ Ensure Ollama server is running for full functionality")
    print(f"   â€¢ Test with different reasoning types and contexts")
    print(f"   â€¢ Monitor performance metrics for optimization")
    print(f"   â€¢ Integrate with memory system for enhanced reasoning")
    
    # æœ€çµ‚ãƒ­ã‚°
    logger.log_shutdown(
        component="basic_reasoning_demo",
        uptime_seconds=0,  # ãƒ‡ãƒ¢ãªã®ã§0
        final_stats={
            "demos_run": total_count,
            "demos_successful": success_count,
            "success_rate": success_count / total_count if total_count > 0 else 0
        }
    )


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nDemo interrupted by user")
    except Exception as e:
        print(f"\nDemo failed: {e}")
        import traceback
        traceback.print_exc()