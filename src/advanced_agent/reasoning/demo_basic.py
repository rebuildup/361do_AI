"""
åŸºæœ¬æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
LangChain + Ollama åŸºæœ¬æ¨è«–æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ
"""

import asyncio
import sys
from pathlib import Path
import time
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from src.advanced_agent.reasoning.basic_engine import (
    BasicReasoningEngine, create_basic_reasoning_engine, ReasoningState
)
from src.advanced_agent.inference.ollama_client import create_ollama_client
from src.advanced_agent.core.config import get_config
from src.advanced_agent.core.logger import setup_logging, get_logger


async def demo_basic_reasoning():
    """åŸºæœ¬æ¨è«–æ©Ÿèƒ½ãƒ‡ãƒ¢"""
    print("\n" + "="*70)
    print("BASIC REASONING ENGINE DEMO")
    print("="*70)
    
    logger = get_logger()
    
    try:
        # ä¾å­˜é–¢ä¿‚ä½œæˆ
        print("\n1. Initializing Dependencies")
        print("-" * 35)
        
        print("   Creating Ollama client...")
        ollama_client = await create_ollama_client()
        
        print("   Creating basic reasoning engine...")
        engine = await create_basic_reasoning_engine(ollama_client)
        
        print("   âœ… All dependencies initialized")
        
        print("\n2. Template Management Test")
        print("-" * 35)
        
        # åˆ©ç”¨å¯èƒ½ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¡¨ç¤º
        templates = engine.list_templates()
        print(f"   Available Prompt Templates: {len(templates['prompt_templates'])}")
        for template_name in templates['prompt_templates']:
            print(f"      - {template_name}")
        
        print(f"   Available Chat Templates: {len(templates['chat_templates'])}")
        for template_name in templates['chat_templates']:
            print(f"      - {template_name}")
        
        print("\n3. Basic Q&A Test")
        print("-" * 25)
        
        question = "Pythonã§ãƒªã‚¹ãƒˆã‚’é€†é †ã«ã™ã‚‹æ–¹æ³•ã‚’3ã¤æ•™ãˆã¦ãã ã•ã„"
        print(f"Question: {question}")
        
        response = await engine.reason(
            prompt=question,
            template_name="basic_qa"
        )
        
        print(f"\nğŸ“Š Results:")
        print(f"   Response: {response.response_text[:200]}...")
        print(f"   Processing Time: {response.processing_time:.2f}s")
        print(f"   Token Count: {response.token_count}")
        print(f"   Model Used: {response.model_used}")
        print(f"   Template Used: {response.template_used}")
        
        print("\n4. Code Analysis Test")
        print("-" * 30)
        
        code_sample = """
def bubble_sort(arr):
    n = len(arr)
    for i in range(n):
        for j in range(0, n-i-1):
            if arr[j] > arr[j+1]:
                arr[j], arr[j+1] = arr[j+1], arr[j]
    return arr
"""
        
        print("Analyzing bubble sort implementation...")
        
        code_response = await engine.reason(
            prompt=code_sample,
            template_name="code_analysis",
            template_variables={
                "code": code_sample,
                "language": "Python"
            }
        )
        
        print(f"\nğŸ“Š Code Analysis Results:")
        print(f"   Analysis: {code_response.response_text[:300]}...")
        print(f"   Processing Time: {code_response.processing_time:.2f}s")
        
        print("\n5. Problem Solving Test")
        print("-" * 30)
        
        problem = "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªãŒé…ã„"
        context = "ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ãŒ1ä¸‡äººã‚’è¶…ãˆã€è¤‡é›‘ãªJOINã‚¯ã‚¨ãƒªãŒå¤šç”¨ã•ã‚Œã¦ã„ã‚‹"
        
        print(f"Problem: {problem}")
        print(f"Context: {context}")
        
        solution_response = await engine.reason(
            prompt=problem,
            template_name="problem_solving",
            template_variables={
                "problem": problem,
                "context": context
            }
        )
        
        print(f"\nğŸ“Š Problem Solving Results:")
        print(f"   Solution: {solution_response.response_text[:300]}...")
        print(f"   Processing Time: {solution_response.processing_time:.2f}s")
        
        print("\n6. Summarization Test")
        print("-" * 25)
        
        long_text = """
æ©Ÿæ¢°å­¦ç¿’ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡ã§ã‚ã‚Šã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ã€
æ˜ç¤ºçš„ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã•ã‚Œã‚‹ã“ã¨ãªãäºˆæ¸¬ã‚„æ±ºå®šã‚’è¡Œã†èƒ½åŠ›ã‚’æŒ‡ã—ã¾ã™ã€‚
æ©Ÿæ¢°å­¦ç¿’ã«ã¯æ•™å¸«ã‚ã‚Šå­¦ç¿’ã€æ•™å¸«ãªã—å­¦ç¿’ã€å¼·åŒ–å­¦ç¿’ã®3ã¤ã®ä¸»è¦ãªã‚¿ã‚¤ãƒ—ãŒã‚ã‚Šã¾ã™ã€‚
æ•™å¸«ã‚ã‚Šå­¦ç¿’ã§ã¯ã€å…¥åŠ›ã¨æ­£è§£ã®ãƒšã‚¢ã‹ã‚‰ãªã‚‹è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ã¾ã™ã€‚
æ•™å¸«ãªã—å­¦ç¿’ã§ã¯ã€æ­£è§£ã®ãªã„ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„æ§‹é€ ã‚’ç™ºè¦‹ã—ã¾ã™ã€‚
å¼·åŒ–å­¦ç¿’ã§ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦æœ€é©ãªè¡Œå‹•ã‚’å­¦ç¿’ã—ã¾ã™ã€‚
è¿‘å¹´ã€æ·±å±¤å­¦ç¿’ã®ç™ºå±•ã«ã‚ˆã‚Šã€ç”»åƒèªè­˜ã€è‡ªç„¶è¨€èªå‡¦ç†ã€éŸ³å£°èªè­˜ãªã©ã®åˆ†é‡ã§
å¤§ããªé€²æ­©ãŒè¦‹ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚
"""
        
        print("Summarizing machine learning text...")
        
        summary_response = await engine.reason(
            prompt=long_text,
            template_name="summarization",
            template_variables={
                "text": long_text,
                "summary_length": "3è¡Œç¨‹åº¦"
            }
        )
        
        print(f"\nğŸ“Š Summarization Results:")
        print(f"   Summary: {summary_response.response_text}")
        print(f"   Processing Time: {summary_response.processing_time:.2f}s")
        
        print("\n7. Batch Processing Test")
        print("-" * 30)
        
        batch_requests = [
            {
                "prompt": "Pythonã®åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿å‹ã‚’æ•™ãˆã¦ãã ã•ã„",
                "template_name": "basic_qa"
            },
            {
                "prompt": "JavaScriptã¨Pythonã®é•ã„ã¯ä½•ã§ã™ã‹ï¼Ÿ",
                "template_name": "basic_qa"
            },
            {
                "prompt": "æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µ",
                "template_name": "analysis",
                "template_variables": {
                    "content": "æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µ",
                    "analysis_type": "æ¦‚è¦èª¬æ˜"
                }
            }
        ]
        
        print(f"Processing {len(batch_requests)} requests in batch...")
        
        batch_start_time = time.time()
        batch_responses = await engine.batch_reason(batch_requests)
        batch_total_time = time.time() - batch_start_time
        
        print(f"\nğŸ“Š Batch Processing Results:")
        print(f"   Total Batch Time: {batch_total_time:.2f}s")
        print(f"   Average Time per Request: {batch_total_time / len(batch_requests):.2f}s")
        
        for i, resp in enumerate(batch_responses, 1):
            print(f"   Request {i}:")
            print(f"      State: {resp.state.value}")
            print(f"      Response: {resp.response_text[:100]}...")
            print(f"      Processing Time: {resp.processing_time:.2f}s")
        
        print("\n8. Performance Statistics")
        print("-" * 30)
        
        stats = engine.get_performance_stats()
        print(f"   Total Requests: {stats['total_requests']}")
        print(f"   Successful Requests: {stats['successful_requests']}")
        print(f"   Failed Requests: {stats['failed_requests']}")
        print(f"   Success Rate: {stats['success_rate']:.2%}")
        print(f"   Average Processing Time: {stats['average_processing_time']:.2f}s")
        print(f"   Total Tokens Processed: {stats['total_tokens_processed']}")
        
        if 'recent_average_time' in stats:
            print(f"   Recent Average Time: {stats['recent_average_time']:.2f}s")
            print(f"   Recent Min Time: {stats['recent_min_time']:.2f}s")
            print(f"   Recent Max Time: {stats['recent_max_time']:.2f}s")
        
        print("\n9. Reasoning History Analysis")
        print("-" * 35)
        
        # å…¨å±¥æ­´
        all_history = engine.get_reasoning_history()
        print(f"   Total History Entries: {len(all_history)}")
        
        # æˆåŠŸã—ãŸæ¨è«–ã®ã¿
        successful_history = engine.get_reasoning_history(
            state_filter=ReasoningState.COMPLETED
        )
        print(f"   Successful Reasoning: {len(successful_history)}")
        
        # æœ€æ–°5ä»¶
        recent_history = engine.get_reasoning_history(limit=5)
        print(f"   Recent 5 Entries:")
        for i, entry in enumerate(recent_history, 1):
            print(f"      {i}. {entry.request_id}: {entry.state.value} ({entry.processing_time:.2f}s)")
        
        await engine.shutdown()
        await ollama_client.shutdown()
        
        return True
        
    except Exception as e:
        print(f"âŒ Basic reasoning demo failed: {e}")
        return False


async def demo_custom_templates():
    """ã‚«ã‚¹ã‚¿ãƒ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¢"""
    print("\n" + "="*70)
    print("CUSTOM TEMPLATES DEMO")
    print("="*70)
    
    try:
        # ä¾å­˜é–¢ä¿‚ä½œæˆ
        ollama_client = await create_ollama_client()
        engine = await create_basic_reasoning_engine(ollama_client)
        
        print("\n1. Creating Custom Templates")
        print("-" * 35)
        
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
        
        # æŠ€è¡“è¨˜äº‹ä½œæˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        tech_article_template = PromptTemplate(
            input_variables=["topic", "target_audience", "length"],
            template="""
æŠ€è¡“è¨˜äº‹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ãƒˆãƒ”ãƒƒã‚¯: {topic}
å¯¾è±¡èª­è€…: {target_audience}
è¨˜äº‹ã®é•·ã•: {length}

ä»¥ä¸‹ã®æ§‹æˆã§è¨˜äº‹ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
1. å°å…¥
2. åŸºæœ¬æ¦‚å¿µã®èª¬æ˜
3. å®Ÿè·µçš„ãªä¾‹
4. ã¾ã¨ã‚

è¨˜äº‹:"""
        )
        
        # ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        review_template = PromptTemplate(
            input_variables=["content", "review_type"],
            template="""
ä»¥ä¸‹ã®å†…å®¹ã«ã¤ã„ã¦{review_type}ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡:
{content}

ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦³ç‚¹:
- æ­£ç¢ºæ€§
- å®Œå…¨æ€§
- æ˜ç¢ºæ€§
- æ”¹å–„ææ¡ˆ

ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœ:"""
        )
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç™»éŒ²
        engine.register_template("tech_article", tech_article_template)
        engine.register_template("review", review_template)
        
        print("   âœ… Custom templates registered")
        
        print("\n2. Using Tech Article Template")
        print("-" * 35)
        
        article_response = await engine.reason(
            prompt="Python async/await",
            template_name="tech_article",
            template_variables={
                "topic": "Python async/await",
                "target_audience": "ä¸­ç´šãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ¼",
                "length": "1000æ–‡å­—ç¨‹åº¦"
            }
        )
        
        print(f"   Article: {article_response.response_text[:300]}...")
        print(f"   Processing Time: {article_response.processing_time:.2f}s")
        
        print("\n3. Using Review Template")
        print("-" * 30)
        
        code_to_review = """
def calculate_average(numbers):
    total = 0
    for num in numbers:
        total += num
    return total / len(numbers)
"""
        
        review_response = await engine.reason(
            prompt=code_to_review,
            template_name="review",
            template_variables={
                "content": code_to_review,
                "review_type": "ã‚³ãƒ¼ãƒ‰å“è³ª"
            }
        )
        
        print(f"   Review: {review_response.response_text[:300]}...")
        print(f"   Processing Time: {review_response.processing_time:.2f}s")
        
        print("\n4. Template List After Custom Addition")
        print("-" * 45)
        
        templates = engine.list_templates()
        print(f"   Total Prompt Templates: {len(templates['prompt_templates'])}")
        for template_name in templates['prompt_templates']:
            print(f"      - {template_name}\")")
        
        await engine.shutdown()
        await ollama_client.shutdown()
        
        return True
        
    except Exception as e:
        print(f"âŒ Custom templates demo failed: {e}")
        return False


async def demo_error_handling():
    """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ‡ãƒ¢"""
    print("\n" + "="*70)
    print("ERROR HANDLING DEMO")
    print("="*70)
    
    try:
        # ä¾å­˜é–¢ä¿‚ä½œæˆ
        ollama_client = await create_ollama_client()
        engine = await create_basic_reasoning_engine(ollama_client)
        
        print("\n1. Invalid Template Test")
        print("-" * 30)
        
        # å­˜åœ¨ã—ãªã„ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½¿ç”¨
        invalid_response = await engine.reason(
            prompt="Test prompt",
            template_name="nonexistent_template"
        )
        
        print(f"   State: {invalid_response.state.value}")
        print(f"   Error: {invalid_response.error_message}")
        
        print("\n2. Template Variable Missing Test")
        print("-" * 40)
        
        # å¿…è¦ãªå¤‰æ•°ãŒä¸è¶³
        missing_var_response = await engine.reason(
            prompt="Test analysis",
            template_name="analysis",
            template_variables={"content": "Test content"}  # analysis_type ãŒä¸è¶³
        )
        
        print(f"   State: {missing_var_response.state.value}")
        if missing_var_response.state == ReasoningState.ERROR:
            print(f"   Error: {missing_var_response.error_message}")
        else:
            print(f"   Response: {missing_var_response.response_text[:100]}...")
        
        print("\n3. Batch Error Handling Test")
        print("-" * 35)
        
        # ä¸€éƒ¨ã‚¨ãƒ©ãƒ¼ã‚’å«ã‚€ãƒãƒƒãƒ
        mixed_batch = [
            {"prompt": "Normal question", "template_name": "basic_qa"},
            {"prompt": "Error question", "template_name": "nonexistent"},
            {"prompt": "Another normal question", "template_name": "basic_qa"}
        ]
        
        batch_responses = await engine.batch_reason(mixed_batch)
        
        print(f"   Batch Results:")
        for i, resp in enumerate(batch_responses, 1):
            print(f"      Request {i}: {resp.state.value}")
            if resp.state == ReasoningState.ERROR:
                print(f"         Error: {resp.error_message}")
        
        print("\n4. Performance Stats with Errors")
        print("-" * 40)
        
        stats = engine.get_performance_stats()
        print(f"   Total Requests: {stats['total_requests']}")
        print(f"   Successful: {stats['successful_requests']}")
        print(f"   Failed: {stats['failed_requests']}")
        print(f"   Success Rate: {stats['success_rate']:.2%}")
        
        await engine.shutdown()
        await ollama_client.shutdown()
        
        return True
        
    except Exception as e:
        print(f"âŒ Error handling demo failed: {e}")
        return False


async def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    print("ğŸš€ Starting Basic Reasoning Engine Demonstrations")
    
    # ãƒ­ã‚°è¨­å®š
    setup_logging()
    
    demos = [
        ("Basic Reasoning", demo_basic_reasoning),
        ("Custom Templates", demo_custom_templates),
        ("Error Handling", demo_error_handling)
    ]
    
    results = {}
    
    for demo_name, demo_func in demos:
        print(f"\nğŸ¯ Running {demo_name} Demo...")
        try:
            result = await demo_func()
            results[demo_name] = "âœ… Success" if result else "âŒ Failed"
        except Exception as e:
            results[demo_name] = f"âŒ Error: {e}"
    
    print("\n" + "="*70)
    print("DEMO RESULTS SUMMARY")
    print("="*70)
    
    for demo_name, result in results.items():
        print(f"   {demo_name}: {result}")
    
    success_count = sum(1 for result in results.values() if "âœ…" in result)
    total_count = len(results)
    
    print(f"\nğŸ“Š Overall Success Rate: {success_count}/{total_count} ({success_count/total_count*100:.1f}%)")
    
    if success_count == total_count:
        print("ğŸ‰ All demos completed successfully!")
    else:
        print("âš ï¸  Some demos failed. Check the logs for details.")


if __name__ == "__main__":
    asyncio.run(main())