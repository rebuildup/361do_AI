"""
Codex Compatible Error Handling
Codexã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’Pythonã§å®Ÿè£…
"""

from typing import Optional, Dict, Any
from enum import Enum


class CodexErrorType(Enum):
    """ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ— (Codex CodexErrç›¸å½“)"""
    CONNECTION_ERROR = "connection_error"
    MODEL_ERROR = "model_error"
    CONFIGURATION_ERROR = "configuration_error"
    VALIDATION_ERROR = "validation_error"
    TIMEOUT_ERROR = "timeout_error"
    AUTHENTICATION_ERROR = "authentication_error"
    RATE_LIMIT_ERROR = "rate_limit_error"
    UNKNOWN_ERROR = "unknown_error"


class CodexError(Exception):
    """
    CodexåŸºåº•ã‚¨ãƒ©ãƒ¼ã‚¯ãƒ©ã‚¹ (Codex CodexErrç›¸å½“)
    """
    
    def __init__(
        self,
        message: str,
        error_type: CodexErrorType = CodexErrorType.UNKNOWN_ERROR,
        details: Optional[Dict[str, Any]] = None,
        original_error: Optional[Exception] = None
    ):
        super().__init__(message)
        self.message = message
        self.error_type = error_type
        self.details = details or {}
        self.original_error = original_error
    
    def to_dict(self) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¾æ›¸å½¢å¼ã§å–å¾—"""
        return {
            "type": self.error_type.value,
            "message": self.message,
            "details": self.details,
            "original_error": str(self.original_error) if self.original_error else None
        }
    
    def __str__(self) -> str:
        return f"[{self.error_type.value}] {self.message}"


class OllamaConnectionError(CodexError):
    """OLLAMAæ¥ç¶šã‚¨ãƒ©ãƒ¼ (Codex OLLAMA_CONNECTION_ERRORç›¸å½“)"""
    
    def __init__(self, message: Optional[str] = None, original_error: Optional[Exception] = None):
        default_message = (
            "No running Ollama server detected. "
            "Start it with: `ollama serve` (after installing). "
            "Install instructions: https://github.com/ollama/ollama"
        )
        super().__init__(
            message or default_message,
            CodexErrorType.CONNECTION_ERROR,
            {"service": "ollama"},
            original_error
        )


class ModelNotFoundError(CodexError):
    """ãƒ¢ãƒ‡ãƒ«æœªç™ºè¦‹ã‚¨ãƒ©ãƒ¼"""
    
    def __init__(self, model_name: str, available_models: Optional[list] = None):
        message = f"Model '{model_name}' not found"
        if available_models:
            message += f". Available models: {', '.join(available_models)}"
        
        super().__init__(
            message,
            CodexErrorType.MODEL_ERROR,
            {"model": model_name, "available_models": available_models}
        )


class ConfigurationError(CodexError):
    """è¨­å®šã‚¨ãƒ©ãƒ¼"""
    
    def __init__(self, message: str, config_key: Optional[str] = None):
        super().__init__(
            message,
            CodexErrorType.CONFIGURATION_ERROR,
            {"config_key": config_key}
        )


class ValidationError(CodexError):
    """ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼"""
    
    def __init__(self, message: str, field: Optional[str] = None, value: Any = None):
        super().__init__(
            message,
            CodexErrorType.VALIDATION_ERROR,
            {"field": field, "value": value}
        )


class TimeoutError(CodexError):
    """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼"""
    
    def __init__(self, operation: str, timeout_seconds: int):
        message = f"Operation '{operation}' timed out after {timeout_seconds} seconds"
        super().__init__(
            message,
            CodexErrorType.TIMEOUT_ERROR,
            {"operation": operation, "timeout": timeout_seconds}
        )


class RateLimitError(CodexError):
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼"""
    
    def __init__(self, message: str, retry_after: Optional[int] = None):
        super().__init__(
            message,
            CodexErrorType.RATE_LIMIT_ERROR,
            {"retry_after": retry_after}
        )


def handle_ollama_error(error: Exception) -> CodexError:
    """
    OLLAMAé–¢é€£ã‚¨ãƒ©ãƒ¼ã®å¤‰æ› (Codex error handlingç›¸å½“)
    """
    import aiohttp
    
    if isinstance(error, aiohttp.ClientConnectorError):
        return OllamaConnectionError(original_error=error)
    
    elif isinstance(error, aiohttp.ClientTimeout):
        return TimeoutError("OLLAMA request", 30)
    
    elif isinstance(error, aiohttp.ClientResponseError):
        if error.status == 404:
            return ModelNotFoundError("unknown", [])
        elif error.status == 429:
            return RateLimitError("Too many requests to OLLAMA server")
        else:
            return CodexError(
                f"OLLAMA HTTP error: {error.status}",
                CodexErrorType.CONNECTION_ERROR,
                {"status_code": error.status},
                error
            )
    
    elif isinstance(error, ConnectionError):
        return OllamaConnectionError(str(error), error)
    
    else:
        return CodexError(
            f"Unexpected OLLAMA error: {str(error)}",
            CodexErrorType.UNKNOWN_ERROR,
            original_error=error
        )


def get_error_message_ui(error: CodexError) -> str:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ç”Ÿæˆ (Codex get_error_message_uiç›¸å½“)
    """
    if error.error_type == CodexErrorType.CONNECTION_ERROR:
        if "ollama" in error.details.get("service", ""):
            return (
                "âŒ OLLAMA server connection failed\n"
                "ğŸ’¡ Make sure OLLAMA is running: `ollama serve`\n"
                "ğŸ“– Installation guide: https://github.com/ollama/ollama"
            )
        else:
            return f"âŒ Connection error: {error.message}"
    
    elif error.error_type == CodexErrorType.MODEL_ERROR:
        if "not found" in error.message.lower():
            available = error.details.get("available_models", [])
            if available:
                return (
                    f"âŒ Model not found: {error.details.get('model', 'unknown')}\n"
                    f"ğŸ’¡ Available models: {', '.join(available[:5])}"
                )
            else:
                return f"âŒ {error.message}\nğŸ’¡ Try: `ollama list` to see available models"
        else:
            return f"âŒ Model error: {error.message}"
    
    elif error.error_type == CodexErrorType.CONFIGURATION_ERROR:
        return f"âš™ï¸ Configuration error: {error.message}"
    
    elif error.error_type == CodexErrorType.TIMEOUT_ERROR:
        return f"â±ï¸ Timeout: {error.message}"
    
    elif error.error_type == CodexErrorType.RATE_LIMIT_ERROR:
        retry_after = error.details.get("retry_after")
        if retry_after:
            return f"ğŸš¦ Rate limited: {error.message} (retry after {retry_after}s)"
        else:
            return f"ğŸš¦ Rate limited: {error.message}"
    
    else:
        return f"âŒ Error: {error.message}"


class ErrorReporter:
    """
    ã‚¨ãƒ©ãƒ¼å ±å‘Šã‚·ã‚¹ãƒ†ãƒ  (Codex error reportingç›¸å½“)
    """
    
    def __init__(self, verbose: bool = False):
        self.verbose = verbose
    
    def report_error(self, error: CodexError) -> str:
        """ã‚¨ãƒ©ãƒ¼ã‚’å ±å‘Šã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™"""
        from loguru import logger
        
        # ãƒ­ã‚°ã«è©³ç´°ã‚’è¨˜éŒ²
        if self.verbose:
            logger.error(f"CodexError: {error.to_dict()}")
        else:
            logger.error(f"Error: {error.message}")
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ
        return get_error_message_ui(error)
    
    def handle_exception(self, exc: Exception) -> str:
        """ä¾‹å¤–ã‚’CodexErrorã«å¤‰æ›ã—ã¦å ±å‘Š"""
        if isinstance(exc, CodexError):
            return self.report_error(exc)
        else:
            # ä¸€èˆ¬çš„ãªä¾‹å¤–ã‚’CodexErrorã«å¤‰æ›
            codex_error = handle_ollama_error(exc)
            return self.report_error(codex_error)