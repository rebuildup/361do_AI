# API ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹

## æ¦‚è¦

Advanced Self-Learning AI Agent ã¯ã€RESTful APIã€Python ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã€WebSocket æ¥ç¶šã‚’é€šã˜ã¦åˆ©ç”¨ã§ãã¾ã™ã€‚ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€ã™ã¹ã¦ã®åˆ©ç”¨å¯èƒ½ãª API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨ãã®ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¾ã™ã€‚

## ğŸŒ REST API

### ãƒ™ãƒ¼ã‚¹ URL

```
http://localhost:8000
```

### èªè¨¼

ç¾åœ¨ã€é–‹ç™ºç’°å¢ƒã§ã¯èªè¨¼ã¯ä¸è¦ã§ã™ã€‚æœ¬ç•ªç’°å¢ƒã§ã¯ API ã‚­ãƒ¼ã¾ãŸã¯ JWT ãƒˆãƒ¼ã‚¯ãƒ³ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚

```bash
# æœ¬ç•ªç’°å¢ƒã§ã®èªè¨¼ä¾‹
curl -H "Authorization: Bearer YOUR_API_KEY" \
     -H "Content-Type: application/json" \
     http://localhost:8000/api/v1/chat
```

## ğŸ“¡ ãƒãƒ£ãƒƒãƒˆ API

### POST /chat

åŸºæœ¬çš„ãªãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

#### ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

```json
{
  "message": "ã“ã‚“ã«ã¡ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
  "use_memory": true,
  "use_cot": true,
  "model": "deepseek-r1:7b",
  "session_id": "optional_session_id",
  "temperature": 0.7,
  "max_tokens": 2048
}
```

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "response": "æ©Ÿæ¢°å­¦ç¿’ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒæ˜ç¤ºçš„ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã•ã‚Œã‚‹ã“ã¨ãªã...",
  "reasoning_steps": [
    "ã‚¹ãƒ†ãƒƒãƒ—1: æ©Ÿæ¢°å­¦ç¿’ã®å®šç¾©ã‚’è€ƒãˆã‚‹",
    "ã‚¹ãƒ†ãƒƒãƒ—2: ä¸»è¦ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ•´ç†ã™ã‚‹",
    "ã‚¹ãƒ†ãƒƒãƒ—3: å®Ÿç”¨ä¾‹ã‚’æŒ™ã’ã‚‹"
  ],
  "session_id": "session_123",
  "model_used": "deepseek-r1:7b",
  "response_time": 1.23,
  "memory_used": true,
  "metadata": {
    "vram_usage": "4.2GB",
    "gpu_utilization": "85%",
    "inference_time": 1.15
  }
}
```

#### cURL ä¾‹

```bash
curl -X POST "http://localhost:8000/chat" \
     -H "Content-Type: application/json" \
     -d '{
       "message": "Pythonã§ã‚½ãƒ¼ãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„",
       "use_memory": true,
       "use_cot": true
     }'
```

#### Python ä¾‹

```python
import requests

response = requests.post(
    "http://localhost:8000/chat",
    json={
        "message": "æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã‚’èª¬æ˜ã—ã¦ãã ã•ã„",
        "use_memory": True,
        "use_cot": True,
        "temperature": 0.7
    }
)

result = response.json()
print(f"å¿œç­”: {result['response']}")
print(f"æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—: {result['reasoning_steps']}")
```

### POST /chat/stream

ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

#### ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

```json
{
  "message": "é•·ã„èª¬æ˜ãŒå¿…è¦ãªè¤‡é›‘ãªè³ªå•",
  "use_memory": true,
  "use_cot": true,
  "stream": true
}
```

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆServer-Sent Eventsï¼‰

```
data: {"type": "start", "session_id": "session_123"}

data: {"type": "reasoning_step", "step": 1, "content": "ã¾ãšåŸºæœ¬æ¦‚å¿µã‚’æ•´ç†ã—ã¾ã™"}

data: {"type": "content", "content": "æ©Ÿæ¢°å­¦ç¿’ã¯"}

data: {"type": "content", "content": "ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰"}

data: {"type": "end", "metadata": {"response_time": 2.34}}
```

#### Python ä¾‹ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰

```python
import requests
import json

response = requests.post(
    "http://localhost:8000/chat/stream",
    json={
        "message": "è©³ç´°ãªèª¬æ˜ã‚’ãŠé¡˜ã„ã—ã¾ã™",
        "stream": True
    },
    stream=True
)

for line in response.iter_lines():
    if line:
        data = json.loads(line.decode('utf-8').replace('data: ', ''))
        if data['type'] == 'content':
            print(data['content'], end='', flush=True)
```

## ğŸ§  æ¨è«– API

### POST /reasoning/cot

Chain-of-Thought æ¨è«–ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

#### ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

```json
{
  "query": "2x + 5 = 15 ã‚’è§£ã„ã¦ãã ã•ã„",
  "steps": 5,
  "model": "deepseek-r1:7b"
}
```

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "query": "2x + 5 = 15 ã‚’è§£ã„ã¦ãã ã•ã„",
  "reasoning_steps": [
    {
      "step": 1,
      "thought": "æ–¹ç¨‹å¼ 2x + 5 = 15 ã‚’è§£ãå¿…è¦ãŒã‚ã‚‹",
      "action": "ä¸¡è¾ºã‹ã‚‰5ã‚’å¼•ã"
    },
    {
      "step": 2,
      "thought": "2x = 15 - 5 = 10",
      "action": "ä¸¡è¾ºã‚’2ã§å‰²ã‚‹"
    },
    {
      "step": 3,
      "thought": "x = 10 / 2 = 5",
      "action": "è§£ã‚’ç¢ºèªã™ã‚‹"
    }
  ],
  "final_answer": "x = 5",
  "confidence": 0.95
}
```

### POST /reasoning/batch

è¤‡æ•°ã®æ¨è«–ã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã¾ã™ã€‚

#### ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

```json
{
  "queries": ["è³ªå•1: ...", "è³ªå•2: ...", "è³ªå•3: ..."],
  "use_cot": true,
  "parallel": true
}
```

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "results": [
    {
      "query": "è³ªå•1: ...",
      "response": "å›ç­”1...",
      "reasoning_steps": ["..."]
    },
    {
      "query": "è³ªå•2: ...",
      "response": "å›ç­”2...",
      "reasoning_steps": ["..."]
    }
  ],
  "total_time": 3.45,
  "parallel_efficiency": 0.87
}
```

## ğŸ’¾ è¨˜æ†¶ API

### GET /memory/sessions

ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ã€‚

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "sessions": [
    {
      "session_id": "session_123",
      "created_at": "2024-01-15T10:30:00Z",
      "last_activity": "2024-01-15T11:45:00Z",
      "message_count": 15,
      "summary": "æ©Ÿæ¢°å­¦ç¿’ã«ã¤ã„ã¦ã®è­°è«–"
    }
  ],
  "total_sessions": 1
}
```

### GET /memory/sessions/{session_id}

ç‰¹å®šã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å±¥æ­´ã‚’å–å¾—ã—ã¾ã™ã€‚

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "session_id": "session_123",
  "messages": [
    {
      "id": 1,
      "role": "user",
      "content": "ã“ã‚“ã«ã¡ã¯",
      "timestamp": "2024-01-15T10:30:00Z"
    },
    {
      "id": 2,
      "role": "assistant",
      "content": "ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
      "timestamp": "2024-01-15T10:30:05Z"
    }
  ],
  "metadata": {
    "total_messages": 2,
    "session_duration": "01:15:00"
  }
}
```

### POST /memory/search

è¨˜æ†¶ã‚’æ¤œç´¢ã—ã¾ã™ã€‚

#### ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

```json
{
  "query": "æ©Ÿæ¢°å­¦ç¿’",
  "limit": 10,
  "importance_threshold": 0.5,
  "memory_types": ["conversation", "knowledge", "preference"]
}
```

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "memories": [
    {
      "id": "mem_123",
      "content": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯æ©Ÿæ¢°å­¦ç¿’ã«èˆˆå‘³ãŒã‚ã‚‹",
      "importance": 0.8,
      "memory_type": "preference",
      "created_at": "2024-01-15T10:30:00Z",
      "relevance_score": 0.92
    }
  ],
  "total_found": 1
}
```

### POST /memory/store

æ–°ã—ã„è¨˜æ†¶ã‚’ä¿å­˜ã—ã¾ã™ã€‚

#### ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

```json
{
  "content": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯Pythonãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãŒå¾—æ„",
  "importance": 0.7,
  "memory_type": "user_skill",
  "tags": ["programming", "python", "skill"]
}
```

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "memory_id": "mem_456",
  "status": "stored",
  "importance": 0.7
}
```

## ğŸ§¬ å­¦ç¿’ API

### GET /learning/status

å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®çŠ¶æ…‹ã‚’å–å¾—ã—ã¾ã™ã€‚

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "evolutionary_learning": {
    "enabled": true,
    "current_generation": 5,
    "population_size": 10,
    "best_fitness": 0.87,
    "training_progress": 0.65
  },
  "adapters": {
    "total_adapters": 15,
    "active_adapters": 3,
    "best_adapter": {
      "id": "adapter_123",
      "fitness": 0.87,
      "config": {
        "r": 16,
        "lora_alpha": 32
      }
    }
  }
}
```

### POST /learning/evolve

é€²åŒ–çš„å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ã€‚

#### ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

```json
{
  "training_data": [
    { "input": "è³ªå•1", "output": "å›ç­”1" },
    { "input": "è³ªå•2", "output": "å›ç­”2" }
  ],
  "generations": 5,
  "population_size": 10,
  "mutation_rate": 0.1
}
```

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "evolution_id": "evo_123",
  "status": "started",
  "estimated_duration": "00:30:00"
}
```

### GET /learning/evolution/{evolution_id}

é€²åŒ–çš„å­¦ç¿’ã®é€²æ—ã‚’å–å¾—ã—ã¾ã™ã€‚

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "evolution_id": "evo_123",
  "status": "running",
  "current_generation": 3,
  "total_generations": 5,
  "progress": 0.6,
  "best_fitness": 0.82,
  "estimated_remaining": "00:12:00"
}
```

## ğŸ“Š ç›£è¦– API

### GET /monitoring/status

ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã‚’å–å¾—ã—ã¾ã™ã€‚

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "system": {
    "status": "healthy",
    "uptime": "02:15:30",
    "version": "1.0.0"
  },
  "gpu": {
    "utilization": 75,
    "memory_used": 4.2,
    "memory_total": 6.0,
    "temperature": 68,
    "power_usage": 150
  },
  "cpu": {
    "utilization": 45,
    "cores": 16,
    "frequency": 3.2
  },
  "memory": {
    "used": 18.5,
    "total": 32.0,
    "swap_used": 2.1
  },
  "performance": {
    "avg_response_time": 1.23,
    "requests_per_minute": 45,
    "error_rate": 0.02
  }
}
```

### GET /monitoring/metrics

è©³ç´°ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ã—ã¾ã™ã€‚

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "timestamp": "2024-01-15T12:00:00Z",
  "metrics": {
    "inference_latency_ms": [1200, 1150, 1300, 1100],
    "gpu_utilization_percent": [75, 78, 72, 80],
    "vram_usage_gb": [4.2, 4.3, 4.1, 4.4],
    "cpu_utilization_percent": [45, 48, 42, 50],
    "memory_usage_gb": [18.5, 18.7, 18.3, 18.9]
  },
  "aggregated": {
    "avg_inference_latency": 1187.5,
    "max_gpu_utilization": 80,
    "avg_vram_usage": 4.25
  }
}
```

### GET /monitoring/alerts

ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¢ãƒ©ãƒ¼ãƒˆã‚’å–å¾—ã—ã¾ã™ã€‚

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "alerts": [
    {
      "id": "alert_123",
      "type": "warning",
      "message": "GPUæ¸©åº¦ãŒé–¾å€¤ã‚’è¶…ãˆã¦ã„ã¾ã™",
      "metric": "gpu_temperature",
      "value": 82,
      "threshold": 80,
      "timestamp": "2024-01-15T12:00:00Z",
      "resolved": false
    }
  ],
  "total_alerts": 1,
  "critical_count": 0,
  "warning_count": 1
}
```

## ğŸ”§ è¨­å®š API

### GET /config

ç¾åœ¨ã®è¨­å®šã‚’å–å¾—ã—ã¾ã™ã€‚

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "gpu": {
    "max_vram_gb": 5.0,
    "quantization_levels": [8, 4, 3],
    "temperature_threshold": 80
  },
  "models": {
    "primary": "deepseek-r1:7b",
    "fallback": "qwen2.5:7b-instruct-q4_k_m"
  },
  "learning": {
    "adapter_pool_size": 10,
    "mutation_rate": 0.1
  }
}
```

### PUT /config

è¨­å®šã‚’æ›´æ–°ã—ã¾ã™ã€‚

#### ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

```json
{
  "gpu": {
    "max_vram_gb": 4.5,
    "temperature_threshold": 75
  }
}
```

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "status": "updated",
  "changes": [
    "gpu.max_vram_gb: 5.0 -> 4.5",
    "gpu.temperature_threshold: 80 -> 75"
  ],
  "restart_required": false
}
```

## ğŸ Python ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```python
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
from src.advanced_agent.client import AdvancedAgentClient
```

### åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•

```python
import asyncio
from src.advanced_agent.client import AdvancedAgentClient

async def main():
    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
    client = AdvancedAgentClient("http://localhost:8000")

    # åŸºæœ¬çš„ãªãƒãƒ£ãƒƒãƒˆ
    response = await client.chat(
        message="æ©Ÿæ¢°å­¦ç¿’ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        use_memory=True,
        use_cot=True
    )

    print(f"å¿œç­”: {response.content}")
    print(f"æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—: {response.reasoning_steps}")

    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆ
    async for chunk in client.chat_stream(
        message="è©³ç´°ãªèª¬æ˜ã‚’ãŠé¡˜ã„ã—ã¾ã™"
    ):
        print(chunk.content, end='', flush=True)

    # è¨˜æ†¶ã®æ¤œç´¢
    memories = await client.search_memories(
        query="ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°",
        limit=5
    )

    for memory in memories:
        print(f"è¨˜æ†¶: {memory.content}")

    # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®ç¢ºèª
    status = await client.get_system_status()
    print(f"GPUä½¿ç”¨ç‡: {status.gpu.utilization}%")
    print(f"VRAMä½¿ç”¨é‡: {status.gpu.memory_used}GB")

# å®Ÿè¡Œ
asyncio.run(main())
```

### é«˜åº¦ãªä½¿ç”¨æ–¹æ³•

```python
# ãƒãƒƒãƒæ¨è«–
results = await client.batch_reasoning([
    "è³ªå•1",
    "è³ªå•2",
    "è³ªå•3"
])

# é€²åŒ–çš„å­¦ç¿’ã®é–‹å§‹
evolution_id = await client.start_evolution(
    training_data=training_data,
    generations=5
)

# å­¦ç¿’é€²æ—ã®ç›£è¦–
while True:
    progress = await client.get_evolution_progress(evolution_id)
    print(f"é€²æ—: {progress.progress * 100:.1f}%")

    if progress.status == "completed":
        break

    await asyncio.sleep(10)

# è¨­å®šã®æ›´æ–°
await client.update_config({
    "gpu": {"max_vram_gb": 4.5}
})
```

## ğŸ”Œ WebSocket API

### æ¥ç¶š

```javascript
const ws = new WebSocket("ws://localhost:8000/ws");

ws.onopen = function (event) {
  console.log("WebSocketæ¥ç¶šãŒç¢ºç«‹ã•ã‚Œã¾ã—ãŸ");
};

ws.onmessage = function (event) {
  const data = JSON.parse(event.data);
  console.log("å—ä¿¡:", data);
};
```

### ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼

#### ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

```json
{
  "type": "chat",
  "data": {
    "message": "ã“ã‚“ã«ã¡ã¯",
    "use_memory": true,
    "use_cot": true
  }
}
```

#### ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã®è³¼èª­

```json
{
  "type": "subscribe",
  "data": {
    "events": ["system_status", "alerts", "performance"]
  }
}
```

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "type": "chat_response",
  "data": {
    "response": "ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
    "session_id": "session_123"
  }
}
```

## ğŸ“ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼

```json
{
  "error": {
    "code": "VRAM_INSUFFICIENT",
    "message": "VRAMä¸è¶³ã®ãŸã‚æ¨è«–ã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“",
    "details": {
      "required_vram": "6.0GB",
      "available_vram": "4.2GB",
      "suggestion": "è»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€é‡å­åŒ–ãƒ¬ãƒ™ãƒ«ã‚’ä¸Šã’ã¦ãã ã•ã„"
    }
  },
  "timestamp": "2024-01-15T12:00:00Z",
  "request_id": "req_123"
}
```

### ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰

| ã‚³ãƒ¼ãƒ‰              | èª¬æ˜                   | å¯¾å‡¦æ³•                                 |
| ------------------- | ---------------------- | -------------------------------------- |
| `VRAM_INSUFFICIENT` | VRAM ä¸è¶³              | è»½é‡ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã€é‡å­åŒ–ãƒ¬ãƒ™ãƒ«ã®èª¿æ•´   |
| `MODEL_NOT_FOUND`   | ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„   | Ollama ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰          |
| `INFERENCE_TIMEOUT` | æ¨è«–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ       | ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå€¤ã®èª¿æ•´ã€è»½é‡ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ |
| `MEMORY_FULL`       | è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã®å®¹é‡ä¸è¶³ | å¤ã„è¨˜æ†¶ã®å‰Šé™¤ã€å®¹é‡åˆ¶é™ã®èª¿æ•´         |
| `CONFIG_INVALID`    | è¨­å®šãŒç„¡åŠ¹             | è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèªã¨ä¿®æ­£               |

### Python ã§ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
from src.advanced_agent.client import AdvancedAgentClient, AgentError

async def handle_errors():
    client = AdvancedAgentClient("http://localhost:8000")

    try:
        response = await client.chat("è¤‡é›‘ãªè³ªå•")
    except AgentError as e:
        if e.code == "VRAM_INSUFFICIENT":
            print("VRAMä¸è¶³ã§ã™ã€‚è»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¾ã™...")
            response = await client.chat(
                "è¤‡é›‘ãªè³ªå•",
                model="qwen2:1.5b-instruct-q4_k_m"
            )
        else:
            print(f"ã‚¨ãƒ©ãƒ¼: {e.message}")
            raise
```

## ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£

### API ã‚­ãƒ¼èªè¨¼ï¼ˆæœ¬ç•ªç’°å¢ƒï¼‰

```python
# ç’°å¢ƒå¤‰æ•°ã§APIã‚­ãƒ¼ã‚’è¨­å®š
import os
os.environ['AGENT_API_KEY'] = 'your-secret-api-key'

# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§APIã‚­ãƒ¼ã‚’ä½¿ç”¨
client = AdvancedAgentClient(
    "http://localhost:8000",
    api_key="your-secret-api-key"
)
```

### ãƒ¬ãƒ¼ãƒˆåˆ¶é™

```json
{
  "error": {
    "code": "RATE_LIMIT_EXCEEDED",
    "message": "ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è¶…ãˆã¾ã—ãŸ",
    "details": {
      "limit": "60 requests per minute",
      "reset_time": "2024-01-15T12:01:00Z"
    }
  }
}
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### ãƒãƒƒãƒãƒªã‚¯ã‚¨ã‚¹ãƒˆ

```python
# è¤‡æ•°ã®æ¨è«–ã‚’åŠ¹ç‡çš„ã«å®Ÿè¡Œ
responses = await client.batch_chat([
    "è³ªå•1",
    "è³ªå•2",
    "è³ªå•3"
], parallel=True)
```

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ´»ç”¨

```python
# è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã‚’æ´»ç”¨ã—ãŸåŠ¹ç‡çš„ãªæ¨è«–
response = await client.chat(
    "ä»¥å‰è©±ã—ãŸæ©Ÿæ¢°å­¦ç¿’ã«ã¤ã„ã¦è©³ã—ãæ•™ãˆã¦",
    use_memory=True,  # é–¢é€£ã™ã‚‹éå»ã®ä¼šè©±ã‚’è‡ªå‹•çš„ã«å‚ç…§
    cache_ttl=3600    # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
)
```

### æ¥ç¶šãƒ—ãƒ¼ãƒ«ã®ä½¿ç”¨

```python
# è¤‡æ•°ã®åŒæ™‚æ¥ç¶šã‚’åŠ¹ç‡çš„ã«ç®¡ç†
client = AdvancedAgentClient(
    "http://localhost:8000",
    max_connections=10,
    timeout=30
)
```

---

**ğŸ“¡ ã“ã® API ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã‚’å‚è€ƒã«ã€Advanced Self-Learning AI Agent ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã—ã¦ãã ã•ã„ï¼**
