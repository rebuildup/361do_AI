# ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰

## æ¦‚è¦

Advanced Self-Learning AI Agent ã®ä½¿ç”¨ä¸­ã«ç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹å•é¡Œã¨ã€ãã®è§£æ±ºæ–¹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚å•é¡Œã®ç¨®é¡åˆ¥ã«æ•´ç†ã—ã€æ®µéšçš„ãªè§£æ±ºæ‰‹é †ã‚’æä¾›ã—ã¾ã™ã€‚

## ğŸš¨ ç·Šæ€¥æ™‚ã®å¯¾å‡¦æ³•

### ã‚·ã‚¹ãƒ†ãƒ ãŒå¿œç­”ã—ãªã„å ´åˆ

```bash
# 1. ãƒ—ãƒ­ã‚»ã‚¹ã®ç¢ºèª
ps aux | grep python | grep advanced_agent

# 2. å¼·åˆ¶çµ‚äº†
pkill -f "advanced_agent"

# 3. GPU ãƒ—ãƒ­ã‚»ã‚¹ã®ç¢ºèªãƒ»çµ‚äº†
nvidia-smi
sudo kill -9 <GPU_PROCESS_ID>

# 4. å®‰å…¨ãƒ¢ãƒ¼ãƒ‰ã§ã®å†èµ·å‹•
AGENT_MAX_VRAM_GB=3.0 python -m src.advanced_agent.interfaces.streamlit_app
```

### VRAM ä¸è¶³ã«ã‚ˆã‚‹ç·Šæ€¥åœæ­¢

```bash
# ç·Šæ€¥æ™‚ã®è»½é‡ãƒ¢ãƒ¼ãƒ‰èµ·å‹•
export AGENT_OLLAMA_PRIMARY_MODEL="qwen2:1.5b-instruct-q4_k_m"
export AGENT_MAX_VRAM_GB=2.5
export AGENT_QUANTIZATION_LEVELS="3"

python -m src.advanced_agent.interfaces.streamlit_app
```

## ğŸ’¾ VRAMãƒ»ãƒ¡ãƒ¢ãƒªé–¢é€£ã®å•é¡Œ

### å•é¡Œ 1: "CUDA out of memory" ã‚¨ãƒ©ãƒ¼

#### ç—‡çŠ¶

```
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 6.00 GiB total capacity; 4.50 GiB already allocated)
```

#### åŸå› 

- VRAM ä½¿ç”¨é‡ãŒ 6GB åˆ¶é™ã‚’è¶…é
- è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ãŒåŒæ™‚ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹
- é‡å­åŒ–è¨­å®šãŒä¸é©åˆ‡

#### è§£æ±ºæ‰‹é †

**ã‚¹ãƒ†ãƒƒãƒ— 1: å³åº§ã®å¯¾å‡¦**

```bash
# GPU ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
python -c "
import torch
torch.cuda.empty_cache()
print('GPU cache cleared')
"
```

**ã‚¹ãƒ†ãƒƒãƒ— 2: è¨­å®šèª¿æ•´**

```yaml
# config/system.yaml
gpu:
  max_vram_gb: 4.0 # 5.0ã‹ã‚‰4.0ã«å‰Šæ¸›
  quantization_levels: [4, 3] # ã‚ˆã‚Šç©æ¥µçš„ãªé‡å­åŒ–
  memory_reserve_gb: 1.0 # äºˆç´„ãƒ¡ãƒ¢ãƒªã‚’å¢—åŠ 
```

**ã‚¹ãƒ†ãƒƒãƒ— 3: ãƒ¢ãƒ‡ãƒ«å¤‰æ›´**

```yaml
# config/system.yaml
models:
  primary: "qwen2.5:7b-instruct-q4_k_m" # ã‚ˆã‚Šè»½é‡ãªãƒ¢ãƒ‡ãƒ«
  fallback: "qwen2:1.5b-instruct-q4_k_m"
```

**ã‚¹ãƒ†ãƒƒãƒ— 4: å‹•çš„æœ€é©åŒ–ã®æœ‰åŠ¹åŒ–**

```python
# è‡ªå‹•æœ€é©åŒ–ã®å®Ÿè¡Œ
from src.advanced_agent.optimization.auto_optimizer import AutoOptimizer

async def fix_vram_issue():
    optimizer = AutoOptimizer()
    result = await optimizer.optimize_vram_usage()
    print(f"æœ€é©åŒ–çµæœ: {result.status}")
    print(f"VRAMç¯€ç´„: {result.vram_saved_gb:.1f}GB")

import asyncio
asyncio.run(fix_vram_issue())
```

### å•é¡Œ 2: ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã«ã‚ˆã‚‹æ€§èƒ½ä½ä¸‹

#### ç—‡çŠ¶

- æ™‚é–“çµŒéã¨ã¨ã‚‚ã«å¿œç­”é€Ÿåº¦ãŒä½ä¸‹
- VRAM ä½¿ç”¨é‡ãŒå¾ã€…ã«å¢—åŠ 
- ã‚·ã‚¹ãƒ†ãƒ ãŒä¸å®‰å®šã«ãªã‚‹

#### è¨ºæ–­æ–¹æ³•

```python
# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–
from src.advanced_agent.monitoring.system_monitor import SystemMonitor

async def diagnose_memory_leak():
    monitor = SystemMonitor()

    for i in range(10):
        status = await monitor.get_system_status()
        print(f"æ™‚åˆ» {i}: VRAMä½¿ç”¨é‡ {status.gpu.memory_used:.2f}GB")
        await asyncio.sleep(60)  # 1åˆ†é–“éš”ã§ç›£è¦–

asyncio.run(diagnose_memory_leak())
```

#### è§£æ±ºæ–¹æ³•

```python
# å®šæœŸçš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
class MemoryManager:
    def __init__(self):
        self.cleanup_interval = 300  # 5åˆ†é–“éš”

    async def start_cleanup_scheduler(self):
        while True:
            await self.cleanup_memory()
            await asyncio.sleep(self.cleanup_interval)

    async def cleanup_memory(self):
        # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        torch.cuda.empty_cache()

        # Python ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        import gc
        gc.collect()

        # æœªä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
        await self._unload_unused_models()
```

### å•é¡Œ 3: ã‚¹ãƒ¯ãƒƒãƒ—ãƒ¡ãƒ¢ãƒªã®éåº¦ãªä½¿ç”¨

#### ç—‡çŠ¶

- ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ãŒé‡ããªã‚‹
- ãƒ‡ã‚£ã‚¹ã‚¯ã‚¢ã‚¯ã‚»ã‚¹ãŒé »ç¹ã«ç™ºç”Ÿ
- å¿œç­”æ™‚é–“ãŒè‘—ã—ãé…ã„

#### è§£æ±ºæ–¹æ³•

```yaml
# config/system.yaml
memory:
  system_ram_gb: 16 # ä½¿ç”¨å¯èƒ½RAMé‡ã‚’æ­£ç¢ºã«è¨­å®š
  swap_limit_gb: 4 # ã‚¹ãƒ¯ãƒƒãƒ—ä½¿ç”¨é‡ã‚’åˆ¶é™
  cache_size_mb: 256 # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºã‚’å‰Šæ¸›

cpu:
  offload_threshold: 0.9 # CPUã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã‚’é…ã‚‰ã›ã‚‹
```

## ğŸ¤– Ollama é–¢é€£ã®å•é¡Œ

### å•é¡Œ 1: Ollama ã‚µãƒ¼ãƒ“ã‚¹ã«æ¥ç¶šã§ããªã„

#### ç—‡çŠ¶

```
ConnectionError: Cannot connect to Ollama service at http://localhost:11434
```

#### è§£æ±ºæ‰‹é †

**ã‚¹ãƒ†ãƒƒãƒ— 1: ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ç¢ºèª**

```bash
# Ollama ã‚µãƒ¼ãƒ“ã‚¹ã®çŠ¶æ…‹ç¢ºèª
ollama list
ollama ps

# ã‚µãƒ¼ãƒ“ã‚¹ãŒå‹•ä½œã—ã¦ã„ãªã„å ´åˆ
ollama serve
```

**ã‚¹ãƒ†ãƒƒãƒ— 2: ãƒãƒ¼ãƒˆç¢ºèª**

```bash
# ãƒãƒ¼ãƒˆ11434ã®ä½¿ç”¨çŠ¶æ³ç¢ºèª
netstat -tulpn | grep 11434
lsof -i :11434
```

**ã‚¹ãƒ†ãƒƒãƒ— 3: è¨­å®šç¢ºèª**

```bash
# ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª
echo $OLLAMA_HOST
echo $OLLAMA_ORIGINS

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
cat config/.env | grep OLLAMA
```

**ã‚¹ãƒ†ãƒƒãƒ— 4: æ‰‹å‹•èµ·å‹•**

```bash
# Ollama ã‚’æ‰‹å‹•ã§èµ·å‹•
OLLAMA_HOST=0.0.0.0:11434 ollama serve

# åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ãƒ¢ãƒ‡ãƒ«ç¢ºèª
ollama list
```

### å•é¡Œ 2: ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—

#### ç—‡çŠ¶

```
Error: failed to pull model 'deepseek-r1:7b': network error
```

#### è§£æ±ºæ–¹æ³•

**ã‚¹ãƒ†ãƒƒãƒ— 1: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç¢ºèª**

```bash
# ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šç¢ºèª
ping -c 4 ollama.ai
curl -I https://ollama.ai

# ãƒ—ãƒ­ã‚­ã‚·è¨­å®šç¢ºèª
echo $HTTP_PROXY
echo $HTTPS_PROXY
```

**ã‚¹ãƒ†ãƒƒãƒ— 2: æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**

```bash
# æ®µéšçš„ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
ollama pull qwen2:1.5b-instruct-q4_k_m  # è»½é‡ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰
ollama pull qwen2.5:7b-instruct-q4_k_m  # ä¸­ç¨‹åº¦ãƒ¢ãƒ‡ãƒ«
ollama pull deepseek-r1:7b               # é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«
```

**ã‚¹ãƒ†ãƒƒãƒ— 3: ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨**

```yaml
# config/system.yaml - åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ã¿ä½¿ç”¨
models:
  primary: "qwen2.5:7b-instruct-q4_k_m"
  fallback: "qwen2:1.5b-instruct-q4_k_m"
  emergency: "qwen2:1.5b-instruct-q4_k_m"
```

### å•é¡Œ 3: ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ãŒé…ã„

#### ç—‡çŠ¶

- å¿œç­”æ™‚é–“ãŒ 10 ç§’ä»¥ä¸Š
- GPU ä½¿ç”¨ç‡ãŒä½ã„
- CPU ãŒé«˜è² è·

#### è¨ºæ–­ãƒ»è§£æ±ºæ–¹æ³•

**ã‚¹ãƒ†ãƒƒãƒ— 1: æ€§èƒ½æ¸¬å®š**

```python
# æ¨è«–æ€§èƒ½ã®æ¸¬å®š
from src.advanced_agent.monitoring.performance_analyzer import PerformanceAnalyzer

async def measure_inference_performance():
    analyzer = PerformanceAnalyzer()

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ
    result = await analyzer.benchmark_inference(
        queries=["ç°¡å˜ãªè³ªå•", "è¤‡é›‘ãªè³ªå•", "é•·ã„è³ªå•"],
        iterations=5
    )

    print(f"å¹³å‡å¿œç­”æ™‚é–“: {result.avg_response_time:.2f}ç§’")
    print(f"GPUä½¿ç”¨ç‡: {result.avg_gpu_utilization:.1f}%")
    print(f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {result.throughput:.1f} queries/sec")

asyncio.run(measure_inference_performance())
```

**ã‚¹ãƒ†ãƒƒãƒ— 2: æœ€é©åŒ–é©ç”¨**

```python
# è‡ªå‹•æœ€é©åŒ–ã®å®Ÿè¡Œ
from src.advanced_agent.optimization.prometheus_optimizer import PrometheusOptimizer

async def optimize_inference_speed():
    optimizer = PrometheusOptimizer()

    # ç¾åœ¨ã®æ€§èƒ½æ¸¬å®š
    baseline = await optimizer.measure_baseline_performance()

    # æœ€é©åŒ–å®Ÿè¡Œ
    optimizations = await optimizer.optimize_inference_pipeline()

    # æœ€é©åŒ–å¾Œã®æ€§èƒ½æ¸¬å®š
    optimized = await optimizer.measure_optimized_performance()

    improvement = ((optimized.response_time - baseline.response_time) / baseline.response_time) * 100
    print(f"å¿œç­”æ™‚é–“æ”¹å–„: {improvement:.1f}%")

asyncio.run(optimize_inference_speed())
```

## ğŸ§  æ¨è«–ãƒ»å­¦ç¿’é–¢é€£ã®å•é¡Œ

### å•é¡Œ 1: Chain-of-Thought æ¨è«–ãŒæ©Ÿèƒ½ã—ãªã„

#### ç—‡çŠ¶

- æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ãŒè¡¨ç¤ºã•ã‚Œãªã„
- è«–ç†çš„ã§ãªã„å›ç­”
- ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã‚‹

#### è§£æ±ºæ–¹æ³•

**ã‚¹ãƒ†ãƒƒãƒ— 1: è¨­å®šç¢ºèª**

```python
# CoTè¨­å®šã®ç¢ºèª
from src.advanced_agent.core.config import get_config

config = get_config()
print(f"CoTæœ‰åŠ¹: {config.reasoning.enable_cot}")
print(f"æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°: {config.reasoning.max_steps}")
print(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {config.models.primary}")
```

**ã‚¹ãƒ†ãƒƒãƒ— 2: æ‰‹å‹•ãƒ†ã‚¹ãƒˆ**

```python
# CoTã‚¨ãƒ³ã‚¸ãƒ³ã®ç›´æ¥ãƒ†ã‚¹ãƒˆ
from src.advanced_agent.reasoning.chain_of_thought import ChainOfThoughtEngine

async def test_cot_engine():
    engine = ChainOfThoughtEngine()

    try:
        result = await engine.reason_step_by_step(
            "2 + 2 = ? ã‚’æ®µéšçš„ã«è¨ˆç®—ã—ã¦ãã ã•ã„"
        )

        print("æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—:")
        for i, step in enumerate(result.reasoning_steps, 1):
            print(f"  {i}. {step}")

        print(f"æœ€çµ‚å›ç­”: {result.final_answer}")

    except Exception as e:
        print(f"CoTã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

asyncio.run(test_cot_engine())
```

**ã‚¹ãƒ†ãƒƒãƒ— 3: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª¿æ•´**

```python
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ç¢ºèªãƒ»ä¿®æ­£
from src.advanced_agent.reasoning.prompt_manager import PromptManager

prompt_manager = PromptManager()

# ã‚«ã‚¹ã‚¿ãƒ CoTãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¨­å®š
custom_cot_prompt = """
ã‚ãªãŸã¯æ®µéšçš„ã«è€ƒãˆã‚‹ AI ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã—ã¦ã€æ€è€ƒéç¨‹ã‚’æ˜ç¢ºã«ç¤ºã—ãªãŒã‚‰å›ç­”ã—ã¦ãã ã•ã„ã€‚

è³ªå•: {query}

æ€è€ƒéç¨‹:
1. ã¾ãšå•é¡Œã‚’ç†è§£ã—ã¾ã™
2. è§£æ±ºã«å¿…è¦ãªæƒ…å ±ã‚’æ•´ç†ã—ã¾ã™
3. æ®µéšçš„ã«è§£æ±ºç­–ã‚’è€ƒãˆã¾ã™
4. æœ€çµ‚çš„ãªå›ç­”ã‚’å°ãã¾ã™

å›ç­”:
"""

prompt_manager.update_template("cot_reasoning", custom_cot_prompt)
```

### å•é¡Œ 2: é€²åŒ–çš„å­¦ç¿’ãŒåæŸã—ãªã„

#### ç—‡çŠ¶

- ä¸–ä»£ã‚’é‡ã­ã¦ã‚‚æ€§èƒ½ãŒå‘ä¸Šã—ãªã„
- ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹ã‚¹ã‚³ã‚¢ãŒä¸å®‰å®š
- å­¦ç¿’ãŒé€”ä¸­ã§åœæ­¢ã™ã‚‹

#### è¨ºæ–­æ–¹æ³•

```python
# å­¦ç¿’é€²æ—ã®è©³ç´°ç›£è¦–
from src.advanced_agent.evolution.evolutionary_system import EvolutionaryLearningSystem

async def diagnose_evolution_issues():
    evolution_system = EvolutionaryLearningSystem()

    # ç¾åœ¨ã®å­¦ç¿’çŠ¶æ…‹ã‚’ç¢ºèª
    status = await evolution_system.get_learning_status()

    print(f"ç¾åœ¨ã®ä¸–ä»£: {status.current_generation}")
    print(f"å€‹ä½“ç¾¤ã‚µã‚¤ã‚º: {status.population_size}")
    print(f"æœ€é«˜ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹: {status.best_fitness}")
    print(f"å¹³å‡ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹: {status.avg_fitness}")
    print(f"ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹åˆ†æ•£: {status.fitness_variance}")

    # å€‹ä½“ç¾¤ã®å¤šæ§˜æ€§ç¢ºèª
    diversity = await evolution_system.calculate_population_diversity()
    print(f"å€‹ä½“ç¾¤å¤šæ§˜æ€§: {diversity}")

    if diversity < 0.1:
        print("âš ï¸ å€‹ä½“ç¾¤ã®å¤šæ§˜æ€§ãŒä½ã™ãã¾ã™ã€‚å¤‰ç•°ç‡ã‚’ä¸Šã’ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")

asyncio.run(diagnose_evolution_issues())
```

#### è§£æ±ºæ–¹æ³•

```yaml
# config/system.yaml - å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´
learning:
  adapter_pool_size: 15 # å€‹ä½“ç¾¤ã‚µã‚¤ã‚ºã‚’å¢—åŠ 
  generation_size: 8 # ä¸–ä»£ã‚µã‚¤ã‚ºã‚’å¢—åŠ 
  mutation_rate: 0.15 # å¤‰ç•°ç‡ã‚’ä¸Šã’ã‚‹
  crossover_rate: 0.8 # äº¤é…ç‡ã‚’ä¸Šã’ã‚‹
  max_training_epochs: 5 # è¨“ç·´ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å‰Šæ¸›
  learning_rate: 0.0005 # å­¦ç¿’ç‡ã‚’ä¸Šã’ã‚‹
  diversity_threshold: 0.1 # å¤šæ§˜æ€§é–¾å€¤ã‚’è¨­å®š
```

### å•é¡Œ 3: LoRA ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®è¨“ç·´ã«å¤±æ•—

#### ç—‡çŠ¶

```
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
```

#### è§£æ±ºæ–¹æ³•

**ã‚¹ãƒ†ãƒƒãƒ— 1: ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã®ç¢ºèª**

```python
# ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã®è¨ºæ–­
import torch

print(f"CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")
print(f"CUDA ãƒ‡ãƒã‚¤ã‚¹æ•°: {torch.cuda.device_count()}")
print(f"ç¾åœ¨ã®ãƒ‡ãƒã‚¤ã‚¹: {torch.cuda.current_device()}")
print(f"ãƒ‡ãƒã‚¤ã‚¹å: {torch.cuda.get_device_name(0)}")

# ãƒ¡ãƒ¢ãƒªæƒ…å ±
if torch.cuda.is_available():
    print(f"ç·VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    print(f"ä½¿ç”¨VRAM: {torch.cuda.memory_allocated(0) / 1e9:.1f}GB")
```

**ã‚¹ãƒ†ãƒƒãƒ— 2: è¨“ç·´è¨­å®šã®ä¿®æ­£**

```python
# æ­£ã—ã„ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã§ã®è¨“ç·´
from src.advanced_agent.adaptation.qlora_trainer import QLoRATrainer

async def fix_training_device_issues():
    trainer = QLoRATrainer()

    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
    trainer.device = "cuda:0" if torch.cuda.is_available() else "cpu"
    trainer.force_device_consistency = True

    # è¨“ç·´å®Ÿè¡Œ
    result = await trainer.train_adapter(
        adapter_config={
            "r": 8,  # ã‚ˆã‚Šå°ã•ãªãƒ©ãƒ³ã‚¯
            "lora_alpha": 16,
            "target_modules": ["q_proj", "v_proj"],  # å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é™å®š
            "lora_dropout": 0.1
        },
        training_data=training_data
    )

    return result
```

## ğŸ“Š ç›£è¦–ãƒ»æ€§èƒ½é–¢é€£ã®å•é¡Œ

### å•é¡Œ 1: Prometheus ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒåé›†ã•ã‚Œãªã„

#### ç—‡çŠ¶

- Grafana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«ãƒ‡ãƒ¼ã‚¿ãŒè¡¨ç¤ºã•ã‚Œãªã„
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„
- ç›£è¦–ã‚¢ãƒ©ãƒ¼ãƒˆãŒæ©Ÿèƒ½ã—ãªã„

#### è§£æ±ºæ‰‹é †

**ã‚¹ãƒ†ãƒƒãƒ— 1: Prometheus ã‚µãƒ¼ãƒ“ã‚¹ç¢ºèª**

```bash
# Prometheus ãƒ—ãƒ­ã‚»ã‚¹ç¢ºèª
ps aux | grep prometheus

# ãƒãƒ¼ãƒˆç¢ºèª
netstat -tulpn | grep 9090

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
cat config/prometheus.yml
```

**ã‚¹ãƒ†ãƒƒãƒ— 2: ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã®æ‰‹å‹•ãƒ†ã‚¹ãƒˆ**

```python
# ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã®ç›´æ¥ãƒ†ã‚¹ãƒˆ
from src.advanced_agent.monitoring.prometheus_collector import PrometheusCollector

async def test_metrics_collection():
    collector = PrometheusCollector()

    try:
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†é–‹å§‹
        await collector.start_collection()

        # 5ç§’é–“åé›†
        await asyncio.sleep(5)

        # åé›†ã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç¢ºèª
        metrics = await collector.get_current_metrics()

        print("åé›†ã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
        for metric_name, value in metrics.items():
            print(f"  {metric_name}: {value}")

    except Exception as e:
        print(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

asyncio.run(test_metrics_collection())
```

**ã‚¹ãƒ†ãƒƒãƒ— 3: è¨­å®šä¿®æ­£**

```yaml
# config/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: "advanced-agent"
    static_configs:
      - targets: ["localhost:8000"]
    scrape_interval: 5s
    metrics_path: "/metrics"
```

### å•é¡Œ 2: GPU ç›£è¦–ãŒæ©Ÿèƒ½ã—ãªã„

#### ç—‡çŠ¶

- GPU ä½¿ç”¨ç‡ãŒ 0%ã¨è¡¨ç¤ºã•ã‚Œã‚‹
- VRAM ä½¿ç”¨é‡ãŒå–å¾—ã§ããªã„
- GPU æ¸©åº¦ãŒè¡¨ç¤ºã•ã‚Œãªã„

#### è§£æ±ºæ–¹æ³•

**ã‚¹ãƒ†ãƒƒãƒ— 1: NVIDIA ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ç¢ºèª**

```bash
# NVIDIA ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ç¢ºèª
nvidia-smi
nvcc --version

# NVIDIA ML ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç¢ºèª
python -c "
try:
    import pynvml
    pynvml.nvmlInit()
    print('NVIDIA ML ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæ­£å¸¸')

    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    name = pynvml.nvmlDeviceGetName(handle)
    print(f'GPU: {name}')
except Exception as e:
    print(f'NVIDIA ML ã‚¨ãƒ©ãƒ¼: {e}')
"
```

**ã‚¹ãƒ†ãƒƒãƒ— 2: æ¨©é™ç¢ºèª**

```bash
# GPU ãƒ‡ãƒã‚¤ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¨©é™ç¢ºèª
ls -la /dev/nvidia*

# ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒdockerã‚°ãƒ«ãƒ¼ãƒ—ã«å±ã—ã¦ã„ã‚‹ã‹ç¢ºèª
groups $USER

# å¿…è¦ã«å¿œã˜ã¦ã‚°ãƒ«ãƒ¼ãƒ—ã«è¿½åŠ 
sudo usermod -a -G docker $USER
```

**ã‚¹ãƒ†ãƒƒãƒ— 3: ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®å†èµ·å‹•**

```python
# GPUç›£è¦–ã®æ‰‹å‹•åˆæœŸåŒ–
from src.advanced_agent.monitoring.system_monitor import SystemMonitor

async def reinitialize_gpu_monitoring():
    monitor = SystemMonitor()

    try:
        # GPUç›£è¦–ã®åˆæœŸåŒ–
        await monitor.initialize_gpu_monitoring()

        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        gpu_stats = await monitor.collect_gpu_metrics()

        print("GPUç›£è¦–æ­£å¸¸:")
        print(f"  ä½¿ç”¨ç‡: {gpu_stats.utilization}%")
        print(f"  VRAM: {gpu_stats.memory_used:.1f}GB / {gpu_stats.memory_total:.1f}GB")
        print(f"  æ¸©åº¦: {gpu_stats.temperature}Â°C")

    except Exception as e:
        print(f"GPUç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: CPUç›£è¦–ã®ã¿
        await monitor.fallback_to_cpu_only_monitoring()

asyncio.run(reinitialize_gpu_monitoring())
```

## ğŸŒ Web UIãƒ»API é–¢é€£ã®å•é¡Œ

### å•é¡Œ 1: Streamlit ã‚¢ãƒ—ãƒªãŒèµ·å‹•ã—ãªã„

#### ç—‡çŠ¶

```
ModuleNotFoundError: No module named 'streamlit'
```

#### è§£æ±ºæ–¹æ³•

**ã‚¹ãƒ†ãƒƒãƒ— 1: ä¾å­˜é–¢ä¿‚ã®ç¢ºèªãƒ»å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**

```bash
# ä»®æƒ³ç’°å¢ƒã®ç¢ºèª
which python
pip list | grep streamlit

# ä¾å­˜é–¢ä¿‚ã®å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# Streamlit ã®å€‹åˆ¥ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install streamlit>=1.28.0
```

**ã‚¹ãƒ†ãƒƒãƒ— 2: ãƒ‘ã‚¹è¨­å®šã®ç¢ºèª**

```python
# Python ãƒ‘ã‚¹ã®ç¢ºèª
import sys
print("Python ãƒ‘ã‚¹:")
for path in sys.path:
    print(f"  {path}")

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®è¿½åŠ 
import os
from pathlib import Path

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
print(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¿½åŠ : {project_root}")
```

**ã‚¹ãƒ†ãƒƒãƒ— 3: æ‰‹å‹•èµ·å‹•**

```bash
# ç›´æ¥èµ·å‹•
cd /path/to/advanced-self-learning-agent
python -m streamlit run src/advanced_agent/interfaces/streamlit_app.py

# ãƒãƒ¼ãƒˆæŒ‡å®šèµ·å‹•
streamlit run src/advanced_agent/interfaces/streamlit_app.py --server.port 8502
```

### å•é¡Œ 2: FastAPI ã‚µãƒ¼ãƒãƒ¼ãŒå¿œç­”ã—ãªã„

#### ç—‡çŠ¶

- API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„
- 502 Bad Gateway ã‚¨ãƒ©ãƒ¼
- ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼

#### è§£æ±ºæ–¹æ³•

**ã‚¹ãƒ†ãƒƒãƒ— 1: ã‚µãƒ¼ãƒãƒ¼çŠ¶æ…‹ç¢ºèª**

```bash
# ãƒ—ãƒ­ã‚»ã‚¹ç¢ºèª
ps aux | grep uvicorn
ps aux | grep fastapi

# ãƒãƒ¼ãƒˆç¢ºèª
netstat -tulpn | grep 8000
lsof -i :8000
```

**ã‚¹ãƒ†ãƒƒãƒ— 2: æ‰‹å‹•èµ·å‹•ãƒ»ãƒ†ã‚¹ãƒˆ**

```bash
# é–‹ç™ºãƒ¢ãƒ¼ãƒ‰ã§ã®èµ·å‹•
uvicorn src.advanced_agent.interfaces.fastapi_gateway:app --reload --host 0.0.0.0 --port 8000

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
curl http://localhost:8000/health

# API ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç¢ºèª
curl http://localhost:8000/docs
```

**ã‚¹ãƒ†ãƒƒãƒ— 3: ãƒ­ã‚°ç¢ºèªãƒ»ãƒ‡ãƒãƒƒã‚°**

```python
# FastAPI ã‚¢ãƒ—ãƒªã®ç›´æ¥ãƒ†ã‚¹ãƒˆ
from src.advanced_agent.interfaces.fastapi_gateway import FastAPIGateway

async def test_fastapi_app():
    gateway = FastAPIGateway()

    # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–ç¢ºèª
    print(f"ã‚¢ãƒ—ãƒªè¨­å®š: {gateway.app.title}")
    print(f"ãƒ«ãƒ¼ãƒˆæ•°: {len(gateway.app.routes)}")

    # å„ãƒ«ãƒ¼ãƒˆã®ç¢ºèª
    for route in gateway.app.routes:
        print(f"  {route.methods} {route.path}")

asyncio.run(test_fastapi_app())
```

### å•é¡Œ 3: WebSocket æ¥ç¶šãŒåˆ‡æ–­ã•ã‚Œã‚‹

#### ç—‡çŠ¶

- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ãŒåœæ­¢ã™ã‚‹
- æ¥ç¶šã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã‚‹
- ãƒšãƒ¼ã‚¸ã®å†èª­ã¿è¾¼ã¿ãŒå¿…è¦

#### è§£æ±ºæ–¹æ³•

**ã‚¹ãƒ†ãƒƒãƒ— 1: æ¥ç¶šè¨­å®šã®èª¿æ•´**

```javascript
// WebSocket æ¥ç¶šã®æ”¹å–„
const ws = new WebSocket("ws://localhost:8000/ws");

// å†æ¥ç¶šæ©Ÿèƒ½ã®è¿½åŠ 
let reconnectInterval = 1000;
const maxReconnectInterval = 30000;

function connect() {
  ws = new WebSocket("ws://localhost:8000/ws");

  ws.onopen = function (event) {
    console.log("WebSocketæ¥ç¶šæˆåŠŸ");
    reconnectInterval = 1000; // å†æ¥ç¶šé–“éš”ã‚’ãƒªã‚»ãƒƒãƒˆ
  };

  ws.onclose = function (event) {
    console.log("WebSocketæ¥ç¶šåˆ‡æ–­ã€å†æ¥ç¶šä¸­...");
    setTimeout(connect, reconnectInterval);
    reconnectInterval = Math.min(reconnectInterval * 2, maxReconnectInterval);
  };

  ws.onerror = function (error) {
    console.error("WebSocketã‚¨ãƒ©ãƒ¼:", error);
  };
}
```

**ã‚¹ãƒ†ãƒƒãƒ— 2: ã‚µãƒ¼ãƒãƒ¼å´ã®è¨­å®šèª¿æ•´**

```python
# WebSocket æ¥ç¶šã®å®‰å®šåŒ–
from fastapi import WebSocket, WebSocketDisconnect
import asyncio

class WebSocketManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        self.heartbeat_interval = 30  # 30ç§’é–“éš”ã§ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆ

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

        # ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆé–‹å§‹
        asyncio.create_task(self.heartbeat(websocket))

    async def heartbeat(self, websocket: WebSocket):
        try:
            while websocket in self.active_connections:
                await websocket.send_json({"type": "heartbeat"})
                await asyncio.sleep(self.heartbeat_interval)
        except WebSocketDisconnect:
            self.disconnect(websocket)

    def disconnect(self, websocket: WebSocket):
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)
```

## ğŸ”§ è¨­å®šãƒ»ç’°å¢ƒé–¢é€£ã®å•é¡Œ

### å•é¡Œ 1: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œãªã„

#### ç—‡çŠ¶

- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãŒä½¿ç”¨ã•ã‚Œã‚‹
- ç’°å¢ƒå¤‰æ•°ãŒåæ˜ ã•ã‚Œãªã„
- è¨­å®šå¤‰æ›´ãŒç„¡åŠ¹

#### è§£æ±ºæ–¹æ³•

**ã‚¹ãƒ†ãƒƒãƒ— 1: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª**

```bash
# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
ls -la config/
cat config/system.yaml
cat config/.env
```

**ã‚¹ãƒ†ãƒƒãƒ— 2: è¨­å®šèª­ã¿è¾¼ã¿ã®ãƒ†ã‚¹ãƒˆ**

```python
# è¨­å®šèª­ã¿è¾¼ã¿ã®ç›´æ¥ãƒ†ã‚¹ãƒˆ
from src.advanced_agent.core.config import get_config, load_config

try:
    config = get_config()
    print("è¨­å®šèª­ã¿è¾¼ã¿æˆåŠŸ:")
    print(f"  GPUæœ€å¤§VRAM: {config.gpu.max_vram_gb}GB")
    print(f"  ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ¢ãƒ‡ãƒ«: {config.models.primary}")
    print(f"  ç›£è¦–é–“éš”: {config.monitoring.gpu_monitoring.interval_seconds}ç§’")

except Exception as e:
    print(f"è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®ç”Ÿæˆ
    from src.advanced_agent.core.config import AdvancedAgentConfig
    default_config = AdvancedAgentConfig()
    print("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
```

**ã‚¹ãƒ†ãƒƒãƒ— 3: ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª**

```bash
# ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª
env | grep AGENT_
echo $AGENT_MAX_VRAM_GB
echo $AGENT_OLLAMA_PRIMARY_MODEL
```

### å•é¡Œ 2: æ¨©é™ã‚¨ãƒ©ãƒ¼

#### ç—‡çŠ¶

```
PermissionError: [Errno 13] Permission denied: '/path/to/logs/advanced_agent.log'
```

#### è§£æ±ºæ–¹æ³•

**ã‚¹ãƒ†ãƒƒãƒ— 1: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¨©é™ã®ç¢ºèªãƒ»ä¿®æ­£**

```bash
# ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ¨©é™ç¢ºèª
ls -la logs/
ls -la data/

# æ¨©é™ä¿®æ­£
chmod 755 logs/
chmod 755 data/
chmod 644 logs/*.log
chmod 644 data/*.db
```

**ã‚¹ãƒ†ãƒƒãƒ— 2: æ‰€æœ‰è€…ã®ç¢ºèªãƒ»å¤‰æ›´**

```bash
# æ‰€æœ‰è€…ç¢ºèª
ls -la logs/ data/

# æ‰€æœ‰è€…å¤‰æ›´ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
sudo chown -R $USER:$USER logs/
sudo chown -R $USER:$USER data/
```

**ã‚¹ãƒ†ãƒƒãƒ— 3: ä»£æ›¿ãƒ‘ã‚¹ã®ä½¿ç”¨**

```python
# æ¨©é™å•é¡Œã®å›é¿
import tempfile
import os

# ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½¿ç”¨
temp_dir = tempfile.mkdtemp(prefix="advanced_agent_")
os.environ['AGENT_LOG_DIR'] = temp_dir
os.environ['AGENT_DATA_DIR'] = temp_dir

print(f"ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨: {temp_dir}")
```

## ğŸ“‹ è¨ºæ–­ãƒ„ãƒ¼ãƒ«

### ç·åˆè¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
#!/usr/bin/env python3
"""
Advanced Self-Learning AI Agent ç·åˆè¨ºæ–­ãƒ„ãƒ¼ãƒ«
"""

import asyncio
import sys
import os
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

async def comprehensive_diagnosis():
    """ç·åˆè¨ºæ–­ã®å®Ÿè¡Œ"""

    print("ğŸ” Advanced Self-Learning AI Agent ç·åˆè¨ºæ–­")
    print("=" * 60)

    # 1. ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ç¢ºèª
    print("\n1. ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ç¢ºèª")
    print("-" * 30)

    try:
        from src.advanced_agent.core.environment import validate_environment_startup
        report = validate_environment_startup()
        print(f"âœ… ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶: {report.overall_status}")
    except Exception as e:
        print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")

    # 2. GPUç¢ºèª
    print("\n2. GPUç¢ºèª")
    print("-" * 30)

    try:
        import torch
        print(f"CUDAåˆ©ç”¨å¯èƒ½: {'âœ…' if torch.cuda.is_available() else 'âŒ'}")
        if torch.cuda.is_available():
            print(f"GPUå: {torch.cuda.get_device_name(0)}")
            print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    except Exception as e:
        print(f"âŒ GPUç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")

    # 3. Ollamaç¢ºèª
    print("\n3. Ollamaç¢ºèª")
    print("-" * 30)

    try:
        from src.advanced_agent.inference.ollama_client import OllamaClient
        client = OllamaClient()
        models = await client.list_models()
        print(f"âœ… Ollamaæ¥ç¶šæˆåŠŸ")
        print(f"åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«: {len(models)}å€‹")
        for model in models[:3]:  # æœ€åˆã®3å€‹ã‚’è¡¨ç¤º
            print(f"  - {model}")
    except Exception as e:
        print(f"âŒ Ollamaç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")

    # 4. è¨­å®šç¢ºèª
    print("\n4. è¨­å®šç¢ºèª")
    print("-" * 30)

    try:
        from src.advanced_agent.core.config import get_config
        config = get_config()
        print(f"âœ… è¨­å®šèª­ã¿è¾¼ã¿æˆåŠŸ")
        print(f"æœ€å¤§VRAM: {config.gpu.max_vram_gb}GB")
        print(f"ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ¢ãƒ‡ãƒ«: {config.models.primary}")
    except Exception as e:
        print(f"âŒ è¨­å®šç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")

    # 5. è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ç¢ºèª
    print("\n5. è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ç¢ºèª")
    print("-" * 30)

    try:
        from src.advanced_agent.memory.persistent_memory import PersistentMemoryManager
        memory = PersistentMemoryManager()
        await memory.initialize()
        print(f"âœ… è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸")
    except Exception as e:
        print(f"âŒ è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")

    # 6. æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ç¢ºèª
    print("\n6. æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ç¢ºèª")
    print("-" * 30)

    try:
        from src.advanced_agent.reasoning.basic_engine import BasicReasoningEngine
        engine = BasicReasoningEngine()
        result = await engine.reason("ãƒ†ã‚¹ãƒˆè³ªå•: 1+1ã¯ï¼Ÿ")
        print(f"âœ… æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³æ­£å¸¸")
        print(f"ãƒ†ã‚¹ãƒˆå¿œç­”: {result.content[:50]}...")
    except Exception as e:
        print(f"âŒ æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")

    print("\n" + "=" * 60)
    print("è¨ºæ–­å®Œäº†")

if __name__ == "__main__":
    asyncio.run(comprehensive_diagnosis())
```

### æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ„ãƒ¼ãƒ«

```python
#!/usr/bin/env python3
"""
æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ„ãƒ¼ãƒ«
"""

import asyncio
import time
from typing import List, Dict

async def performance_benchmark():
    """æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å®Ÿè¡Œ"""

    print("ğŸš€ æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
    print("=" * 50)

    # ãƒ†ã‚¹ãƒˆè³ªå•
    test_queries = [
        "ç°¡å˜ãªè³ªå•: ä»Šæ—¥ã®å¤©æ°—ã¯ï¼Ÿ",
        "ä¸­ç¨‹åº¦ã®è³ªå•: Pythonã§ã‚½ãƒ¼ãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„",
        "è¤‡é›‘ãªè³ªå•: æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã¨å¿œç”¨ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„"
    ]

    from src.advanced_agent.reasoning.basic_engine import BasicReasoningEngine
    engine = BasicReasoningEngine()

    results = []

    for i, query in enumerate(test_queries, 1):
        print(f"\nãƒ†ã‚¹ãƒˆ {i}: {query[:30]}...")

        start_time = time.time()

        try:
            result = await engine.reason(query)
            end_time = time.time()

            response_time = end_time - start_time
            results.append({
                "query": query,
                "response_time": response_time,
                "success": True,
                "response_length": len(result.content)
            })

            print(f"  å¿œç­”æ™‚é–“: {response_time:.2f}ç§’")
            print(f"  å¿œç­”é•·: {len(result.content)}æ–‡å­—")

        except Exception as e:
            end_time = time.time()
            results.append({
                "query": query,
                "response_time": end_time - start_time,
                "success": False,
                "error": str(e)
            })
            print(f"  ã‚¨ãƒ©ãƒ¼: {e}")

    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 50)
    print("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 50)

    successful_results = [r for r in results if r["success"]]

    if successful_results:
        avg_response_time = sum(r["response_time"] for r in successful_results) / len(successful_results)
        print(f"å¹³å‡å¿œç­”æ™‚é–“: {avg_response_time:.2f}ç§’")
        print(f"æˆåŠŸç‡: {len(successful_results)}/{len(results)} ({len(successful_results)/len(results)*100:.1f}%)")

        if avg_response_time <= 2.0:
            print("âœ… æ€§èƒ½ç›®æ¨™é”æˆï¼ˆ2ç§’ä»¥å†…ï¼‰")
        else:
            print("âš ï¸ æ€§èƒ½ç›®æ¨™æœªé”æˆï¼ˆ2ç§’è¶…éï¼‰")
    else:
        print("âŒ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    asyncio.run(performance_benchmark())
```

## ğŸ“ ã‚µãƒãƒ¼ãƒˆãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£

### å•é¡Œå ±å‘Šã®æ–¹æ³•

1. **GitHub Issues**: ãƒã‚°å ±å‘Šãƒ»æ©Ÿèƒ½è¦æœ›
2. **Discussions**: è³ªå•ãƒ»è­°è«–
3. **Wiki**: ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

### å•é¡Œå ±å‘Šæ™‚ã«å«ã‚ã‚‹æƒ…å ±

```bash
# ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®åé›†
echo "=== ã‚·ã‚¹ãƒ†ãƒ æƒ…å ± ==="
uname -a
python --version
pip list | grep -E "(torch|transformers|langchain|ollama)"

echo "=== GPUæƒ…å ± ==="
nvidia-smi

echo "=== è¨­å®šæƒ…å ± ==="
cat config/system.yaml | head -20

echo "=== ãƒ­ã‚°æƒ…å ± ==="
tail -50 logs/advanced_agent.log
```

---

**ğŸ”§ ã“ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰ã‚’å‚è€ƒã«ã€å•é¡Œã‚’åŠ¹ç‡çš„ã«è§£æ±ºã—ã¦ãã ã•ã„ï¼**
