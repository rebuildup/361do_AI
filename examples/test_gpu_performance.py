#!/usr/bin/env python3
"""
GPU Performance Test Script
GPUã‚’ä½¿ç”¨ã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
"""

import asyncio
import time
import os
import sys
from pathlib import Path

# Add the src directory to the Python path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from codex_agent.config import CodexConfig
from codex_agent.ollama_client import CodexOllamaClient


async def test_gpu_performance():
    """GPU ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    print("ğŸš€ GPU ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 50)
    
    # GPUè¨­å®šã‚’æœ‰åŠ¹åŒ–
    os.environ["OLLAMA_GPU_ENABLED"] = "true"
    os.environ["OLLAMA_GPU_MEMORY_FRACTION"] = "0.8"
    os.environ["OLLAMA_GPU_LAYERS"] = "32"
    os.environ["OLLAMA_PARALLEL_REQUESTS"] = "4"
    
    # è¨­å®šã‚’èª­ã¿è¾¼ã¿
    config = CodexConfig()
    print(f"ğŸ“Š è¨­å®šæƒ…å ±:")
    print(f"  ãƒ¢ãƒ‡ãƒ«: {config.model}")
    print(f"  GPUæœ‰åŠ¹: {config.gpu_enabled}")
    print(f"  GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {config.gpu_memory_fraction}")
    print(f"  GPUå±¤æ•°: {config.gpu_layers}")
    print(f"  ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {config.parallel_requests}")
    print()
    
    # OLLAMAã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    async with CodexOllamaClient(config) as client:
        print("âœ… OLLAMAã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
        model_info = client.get_model_info()
        print(f"ğŸ“‹ ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
        for key, value in model_info.items():
            print(f"  {key}: {value}")
        print()
        
        # ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        test_prompts = [
            "Pythonã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ç°¡å˜ãªã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚",
            "æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã‚’3ã¤èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åŸºæœ¬çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã®åŸºæœ¬åŸå‰‡ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
        ]
        
        print("ğŸ§ª ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        
        # å˜ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ†ã‚¹ãƒˆ
        print("\nğŸ“ˆ å˜ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ†ã‚¹ãƒˆ:")
        for i, prompt in enumerate(test_prompts[:2], 1):
            print(f"  ãƒ†ã‚¹ãƒˆ {i}: {prompt[:30]}...")
            
            start_time = time.time()
            response = await client.generate(prompt, max_tokens=200)
            end_time = time.time()
            
            response_time = end_time - start_time
            tokens_per_second = len(response.response.split()) / response_time if response_time > 0 else 0
            
            print(f"    å¿œç­”æ™‚é–“: {response_time:.2f}ç§’")
            print(f"    ãƒˆãƒ¼ã‚¯ãƒ³/ç§’: {tokens_per_second:.1f}")
            print(f"    å¿œç­”é•·: {len(response.response)}æ–‡å­—")
            print()
        
        # ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ†ã‚¹ãƒˆ
        print("ğŸ”„ ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ†ã‚¹ãƒˆ:")
        start_time = time.time()
        
        tasks = []
        for i, prompt in enumerate(test_prompts, 1):
            task = client.generate(f"ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„: {prompt}", max_tokens=100)
            tasks.append(task)
        
        responses = await asyncio.gather(*tasks)
        end_time = time.time()
        
        total_time = end_time - start_time
        avg_time_per_request = total_time / len(tasks)
        
        print(f"  ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {len(tasks)}")
        print(f"  ç·å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
        print(f"  å¹³å‡å¿œç­”æ™‚é–“: {avg_time_per_request:.2f}ç§’")
        print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {len(tasks)/total_time:.2f} req/sec")
        print()
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
        print("ğŸ“¡ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆ:")
        stream_prompt = "Pythonã®ç‰¹å¾´ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        
        start_time = time.time()
        first_token_time = None
        token_count = 0
        
        print(f"  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {stream_prompt}")
        print("  å¿œç­”: ", end="", flush=True)
        
        async for chunk in client.generate_stream(stream_prompt, max_tokens=300):
            if first_token_time is None:
                first_token_time = time.time()
            
            if chunk.response:
                print(chunk.response, end="", flush=True)
                token_count += len(chunk.response.split())
            
            if chunk.done:
                break
        
        end_time = time.time()
        
        if first_token_time:
            time_to_first_token = first_token_time - start_time
            total_stream_time = end_time - start_time
            tokens_per_second = token_count / total_stream_time if total_stream_time > 0 else 0
            
            print(f"\n  åˆå›ãƒˆãƒ¼ã‚¯ãƒ³æ™‚é–“: {time_to_first_token:.2f}ç§’")
            print(f"  ç·ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ™‚é–“: {total_stream_time:.2f}ç§’")
            print(f"  ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é€Ÿåº¦: {tokens_per_second:.1f} tokens/sec")
        
        print("\n" + "=" * 50)
        print("âœ… GPU ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº†")


async def compare_cpu_gpu_performance():
    """CPU vs GPU ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ"""
    print("âš–ï¸  CPU vs GPU ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ")
    print("=" * 50)
    
    test_prompt = "Pythonã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚"
    
    # CPUè¨­å®šã§ãƒ†ã‚¹ãƒˆ
    print("ğŸ–¥ï¸  CPU ãƒ¢ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ:")
    os.environ["OLLAMA_GPU_ENABLED"] = "false"
    config_cpu = CodexConfig()
    
    async with CodexOllamaClient(config_cpu) as client_cpu:
        start_time = time.time()
        response_cpu = await client_cpu.generate(test_prompt, max_tokens=200)
        cpu_time = time.time() - start_time
        
        print(f"  å¿œç­”æ™‚é–“: {cpu_time:.2f}ç§’")
        print(f"  å¿œç­”é•·: {len(response_cpu.response)}æ–‡å­—")
    
    # GPUè¨­å®šã§ãƒ†ã‚¹ãƒˆ
    print("\nğŸš€ GPU ãƒ¢ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ:")
    os.environ["OLLAMA_GPU_ENABLED"] = "true"
    config_gpu = CodexConfig()
    
    async with CodexOllamaClient(config_gpu) as client_gpu:
        start_time = time.time()
        response_gpu = await client_gpu.generate(test_prompt, max_tokens=200)
        gpu_time = time.time() - start_time
        
        print(f"  å¿œç­”æ™‚é–“: {gpu_time:.2f}ç§’")
        print(f"  å¿œç­”é•·: {len(response_gpu.response)}æ–‡å­—")
    
    # æ¯”è¼ƒçµæœ
    if cpu_time > 0 and gpu_time > 0:
        speedup = cpu_time / gpu_time
        print(f"\nğŸ“Š æ¯”è¼ƒçµæœ:")
        print(f"  GPUé«˜é€ŸåŒ–å€ç‡: {speedup:.2f}x")
        print(f"  æ™‚é–“çŸ­ç¸®: {((cpu_time - gpu_time) / cpu_time * 100):.1f}%")


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    try:
        await test_gpu_performance()
        print("\n" + "=" * 50)
        await compare_cpu_gpu_performance()
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())